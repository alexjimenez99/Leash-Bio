{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchsummary, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 torchsummary-1.5.1 transformers-4.42.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2024.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (10.2.0)\n",
      "Downloading rdkit-2024.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2024.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d2l\n",
      "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (1.0.0)\n",
      "Collecting numpy==1.23.5 (from d2l)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting matplotlib==3.7.2 (from d2l)\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (0.1.6)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (2.31.0)\n",
      "Collecting pandas==2.0.3 (from d2l)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy==1.10.1 (from d2l)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: notebook in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.1.2)\n",
      "Requirement already satisfied: qtconsole in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.16.3)\n",
      "Requirement already satisfied: ipykernel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.29.3)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (10.2.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->d2l)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (2.9.0)\n",
      "Requirement already satisfied: traitlets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib-inline==0.1.6->d2l) (5.14.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.22.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (6.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.42)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.17.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.13.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.25.4)\n",
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (4.1.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->d2l) (4.2.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.5.3)\n",
      "Requirement already satisfied: overrides in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
      "Requirement already satisfied: websocket-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.2.4)\n",
      "Requirement already satisfied: tomli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.9.24)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (4.21.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.18.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.4)\n",
      "Requirement already satisfied: uri-template in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.9.0.20240316)\n",
      "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, numpy, scipy, pandas, matplotlib, d2l\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.12.0\n",
      "    Uninstalling scipy-1.12.0:\n",
      "      Successfully uninstalled scipy-1.12.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.0.3 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed d2l-1.0.3 matplotlib-3.7.2 numpy-1.23.5 pandas-2.0.3 pyparsing-3.0.9 scipy-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.1.3)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (5.9.8)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: multidict, h5py, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 h5py-3.11.0 multidict-6.0.5 torch-geometric-2.5.3 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tape-proteins\n",
      "  Downloading tape_proteins-0.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (4.66.2)\n",
      "Collecting tensorboardX (from tape-proteins)\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (1.10.1)\n",
      "Collecting lmdb (from tape-proteins)\n",
      "  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (1.34.101)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (2.31.0)\n",
      "Collecting biopython (from tape-proteins)\n",
      "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (3.13.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from biopython->tape-proteins) (1.23.5)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.101 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3->tape-proteins) (1.34.101)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3->tape-proteins) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3->tape-proteins) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (2024.2.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX->tape-proteins) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX->tape-proteins) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.101->boto3->tape-proteins) (2.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->tensorboardX->tape-proteins) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.101->boto3->tape-proteins) (1.16.0)\n",
      "Downloading tape_proteins-0.5-py3-none-any.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lmdb, biopython, tensorboardX, tape-proteins\n",
      "Successfully installed biopython-1.84 lmdb-1.5.1 tape-proteins-0.5 tensorboardX-2.6.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tape-proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepchem\n",
      "  Downloading deepchem-2.8.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.4.1.post1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.12)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.10.1)\n",
      "Requirement already satisfied: rdkit in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (2024.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->deepchem) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit->deepchem) (10.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.4.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->deepchem) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
      "Downloading deepchem-2.8.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deepchem\n",
      "Successfully installed deepchem-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2024.7.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dask-ml\n",
      "  Downloading dask_ml-2024.4.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: click>=8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (21.3)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (6.0.1)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (6.11.0)\n",
      "Collecting dask-glm>=0.2.0 (from dask-ml)\n",
      "  Downloading dask_glm-0.3.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting distributed>=2.4.0 (from dask-ml)\n",
      "  Downloading distributed-2024.7.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting multipledispatch>=0.4.9 (from dask-ml)\n",
      "  Downloading multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (0.59.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (1.10.1)\n",
      "Collecting sparse>=0.7.0 (from dask-glm>=0.2.0->dask-ml)\n",
      "  Downloading sparse-0.15.4-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[array,dataframe]>=2.4.0->dask-ml)\n",
      "  Downloading dask_expr-1.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (3.1.3)\n",
      "Collecting locket>=1.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting msgpack>=1.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (5.9.8)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (6.4)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.2.1)\n",
      "Collecting zict>=3.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask) (3.17.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from numba>=0.51.0->dask-ml) (0.42.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->dask) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (3.4.0)\n",
      "Requirement already satisfied: pyarrow>=7.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-expr<1.2,>=1.1->dask[array,dataframe]>=2.4.0->dask-ml) (15.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->dask-ml) (1.16.0)\n",
      "Downloading dask-2024.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dask_ml-2024.4.4-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.8/149.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dask_glm-0.3.2-py2.py3-none-any.whl (13 kB)\n",
      "Downloading distributed-2024.7.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dask_expr-1.1.7-py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.1/385.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading sparse-0.15.4-py2.py3-none-any.whl (237 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.3/237.3 kB\u001b[0m \u001b[31m778.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m162.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, multipledispatch, zict, toolz, msgpack, locket, sparse, partd, dask, distributed, dask-expr, dask-glm, dask-ml\n",
      "Successfully installed dask-2024.7.0 dask-expr-1.1.7 dask-glm-0.3.2 dask-ml-2024.4.4 distributed-2024.7.0 locket-1.0.0 msgpack-1.0.8 multipledispatch-1.0.0 partd-1.4.2 sortedcontainers-2.4.0 sparse-0.15.4 toolz-0.12.1 zict-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.0-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.10.1)\n",
      "Downloading xgboost-2.1.0-py3-none-manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\n",
      "  Downloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)\n",
      "Downloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: duckdb\n",
      "Successfully installed duckdb-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (2.0.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: plotly in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (5.20.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (10.2.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from plotly->catboost) (8.2.3)\n",
      "Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m278.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.5 graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "--46iLsBsl_W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from   d2l import torch as d2l\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from   torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from   transformers import AutoTokenizer\n",
    "from   torch.nn.utils.rnn import pad_sequence\n",
    "from   transformers import AutoModel, AutoTokenizer\n",
    "from   torch import nn\n",
    "\n",
    "from torch_geometric.nn import GATConv, GCNConv, global_mean_pool\n",
    "from torch.nn import LazyLinear, Transformer, Conv1d, Dropout, BatchNorm1d\n",
    "import torch.nn.utils as utils\n",
    "from torch.nn import init\n",
    "\n",
    "from torch.nn import Embedding\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader  # Correct import\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "\n",
    "from tape import ProteinBertModel, TAPETokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "import h5py\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "import psutil\n",
    "\n",
    "# Self Created Code\n",
    "from JunctionTree.main    import Datapreprocess\n",
    "from leash_utils.utils    import create_protein_embeddings, open_train_test, create_graph_embeddings, get_memory_usage\n",
    "from leash_utils.utils    import load_embeddings, load_vocab_tokens, preprocess_build, _prepare_data\n",
    "from leash_utils.models   import Model1, Model2, MorganModel, ResModel, SimpleNN\n",
    "from leash_utils.s3_utils import load_s3_files\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import deepchem as dc\n",
    "from deepchem.feat import MolGraphConvFeaturizer\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "\n",
    "import dask_ml\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from xgboost.dask import DaskDMatrix, train, predict\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Test size is 300 mil, truncated version is loaded 18May2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zVJ7PakpNTnx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load with m5.12xlarge instance to load all the data into memory\n",
    "# dataframes = load_s3_files()\n",
    "\n",
    "# Multiple models available (facebook/esm2_t33_650M_UR50D, )\n",
    "DUCK_DB               = True\n",
    "model_name            = 'factorize'\n",
    "device                = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# embedded_sequences    = create_protein_embeddings(model_name = model_name)\n",
    "embedded_sequences    = {'sEH': 0, 'BRD4': 1, 'HSA': 2}\n",
    "\n",
    "if DUCK_DB:\n",
    "    class0_size   = 4000000\n",
    "    class1_size   = 1589906 # all binding data\n",
    "    test_data     = pickle.load(open('test_df.pckl', 'rb'))\n",
    "    train_path    = 'train.parquet'\n",
    "#     con           = duckdb.connect()\n",
    "\n",
    "#     train_data    = con.query(f\"\"\"(SELECT *\n",
    "#                             FROM parquet_scan('{train_path}')\n",
    "#                             WHERE binds = 0\n",
    "#                             ORDER BY random()\n",
    "#                             LIMIT {class0_size})\n",
    "#                             UNION ALL\n",
    "#                             (SELECT *\n",
    "#                             FROM parquet_scan('{train_path}')\n",
    "#                             WHERE binds = 1\n",
    "#                             ORDER BY random()\n",
    "#                             LIMIT {class1_size})\"\"\").df()\n",
    "\n",
    "#     con.close()\n",
    "    print('test_loaded')\n",
    "    train_data = pd.read_csv('5.5million_train.csv')\n",
    "#     train_data = dd.read_csv('5.5million_train.csv')\n",
    "else:\n",
    "    test_data, train_data = open_train_test(class_0_size = 3000000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_metadata():\n",
    "    '''Taken from https://www.uniprot.org/uniprotkb/P02768/entry'''\n",
    "    \n",
    "    \n",
    "    protein_metadata = {'sEH': {'Binding Site Mg2+': [9, 11, 185], \n",
    "                                'Binding Site Phosphate': [123, 124], \n",
    "                                'Nucleophile Active Site': [335],\n",
    "                                'Substrate Binding Site': [466], \n",
    "                                'Proton Donor': [466],\n",
    "                                 'Proton Acceptor': [524]}, \n",
    "                       'BRD4': {'Acetylated histone binding': [140, 433],\n",
    "                                'Breakpoint for translocation to form BDR4-NUTM1 fusion protein': [719, 720]\n",
    "                               },\n",
    "                        'HSA': {'Binding Site Cu Cation': [27],\n",
    "                                'Site Not Glycalated': [28, 44, 65, 88, 97, 117, 130, 160, 183, 198, 205, 214, 219, \n",
    "                                                        229, 236, 264, 286, 298, 310, 383, 396, 413, 426, 438, 456, \n",
    "                                                        460, 490, 499, 524, 543, 548, 562, 581, 584, 588, 598],\n",
    "                                'Ca+2 Binding Site': [30, 37, 268, 273, 276, 279, 283],\n",
    "                                'Zn+2 Binding Site': [91, 271, 273],\n",
    "                                'Aspirin-acetylated lysine': [223]\n",
    "                               }\n",
    "                       }\n",
    "\n",
    "    protein_metadata = pd.DataFrame(protein_metadata).transpose()\n",
    "    for index in protein_metadata.index:\n",
    "        protein_metadata.loc[index] = protein_metadata.loc[index].apply(lambda x: 1 if isinstance(x, list) else 0)\n",
    "        \n",
    "    return protein_metadata\n",
    "\n",
    "# protein_metadata = get_protein_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write code to check for exploding gradients** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):\n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, \n",
    "                 max_epochs,\n",
    "                 bypass_prepare = False,\n",
    "                 num_gpus=0, \n",
    "                 gradient_clip_val=0, \n",
    "                 batch_size = 32):\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.bypass_prepare     = bypass_prepare\n",
    "        self.max_epochs         = max_epochs\n",
    "        self.batch_size         = batch_size\n",
    "        # assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, train_data, val_data):\n",
    "        \n",
    "        # Convert MACCS Keys to tensor\n",
    "        if isinstance(train_data, list):\n",
    "            print(type(train_data[0]))\n",
    "            train_data[0]      = torch.from_numpy(train_data[0])\n",
    "            train_data         = TensorDataset(train_data[0], train_data[1])\n",
    "            \n",
    "        train_dataloader       = DataLoader(train_data, batch_size = self.batch_size)\n",
    "        if val_data is not None:\n",
    "            val_dataloader     = DataLoader(val_data, batch_size = self.batch_size)\n",
    "        else:\n",
    "            val_dataloader     = None\n",
    "            \n",
    "        self.num_train_batches = len(train_dataloader)\n",
    "        self.num_val_batches   = (len(val_dataloader)\n",
    "                                if val_dataloader is not None else 0)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "#     def prepare_model(self, model, optimizer, loss_fn):\n",
    "#         # model.trainer = self\n",
    "#         # model.board.xlim = [0, self.max_epochs]\n",
    "#         self.model     = model\n",
    "#         self.optimizer = optimizer\n",
    "#         self.loss_fn   = loss_fn\n",
    "\n",
    "    def fit(self, \n",
    "            model,\n",
    "            train_data, \n",
    "            other_data   = None,\n",
    "            val_data     = None, \n",
    "            optimizer    = None, \n",
    "            loss_fn      = None, \n",
    "            lr_scheduler = None):\n",
    "        \n",
    "        self.model           = model\n",
    "        self.optimizer       = optimizer\n",
    "        self.loss_fn         = loss_fn\n",
    "        self.lr_scheduler    = lr_scheduler\n",
    "        \n",
    "        # Bass other data (Placeholder for now 05May2024)\n",
    "        if other_data is not None:\n",
    "            self.supplement_data = other_data\n",
    "   \n",
    "        if not self.bypass_prepare:\n",
    "            self.train_dataloader, self.val_dataloader = self.prepare_data(train_data, val_data)\n",
    "            loader_type      = None\n",
    "            \n",
    "        else:\n",
    "            self.train_dataloader, self.val_dataloader = train_data, val_data\n",
    "            loader_type  = 'Graph'\n",
    "        \n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx   = 0\n",
    "        for self.epoch in tqdm(range(self.max_epochs), total = self.max_epochs):\n",
    "            self.fit_epoch(loader_type = loader_type, p_data = self.supplement_data)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                loss  = self.loss.to('cpu').item()\n",
    "                print(f'Epoch {self.epoch + 1} LOSS: {loss}')\n",
    "            else:\n",
    "                print(f'Epoch {self.epoch + 1} LOSS: {self.loss.detach().numpy()}')\n",
    "            \n",
    "    def fit_epoch(self, loader_type = 'Graph', p_data = None, gradients = True):\n",
    "        \n",
    "        if loader_type == 'Graph':\n",
    "            for index, data in enumerate(self.train_dataloader):\n",
    "                \n",
    "                p_batch     = p_data[index*self.batch_size:(index + 1)*self.batch_size]\n",
    "#                 p_batch     = protein_real_time(p_batch)\n",
    "                if isinstance(p_batch, list):\n",
    "                    p_batch = torch.concatenate([p for p in p_batch], axis = 0)\n",
    "                \n",
    "                print(data)\n",
    "                targets     = data.y.unsqueeze(1).float()\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    p_batch  = p_batch.to(device)\n",
    "                    data     = data.to(device)\n",
    "                    targets  = targets.to(device)\n",
    "                    \n",
    "                predictions = self.model.forward(data, p_batch)\n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                self.loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "#             if gradients:\n",
    "#                 for name, param in model.named_parameters():\n",
    "#                     if param.requires_grad:\n",
    "#                         print(f\"Gradients for {name}: {param.grad}\")       \n",
    "                \n",
    "        if loader_type != 'Graph':\n",
    "            for index, data in tqdm(enumerate(self.train_dataloader), total = 32753, desc=\"Training\"):\n",
    "                targets     = data[1].float()\n",
    "                features    = data[0].float()\n",
    "                \n",
    "                if len(list(targets.shape)) < 2:\n",
    "                    targets     = targets.unsqueeze(1).float()\n",
    "                p_batch     = p_data[index*self.batch_size:(index + 1)*self.batch_size]\n",
    "                \n",
    "#                 # Real time batch processing\n",
    "#                 print(p_batch)\n",
    "                p_batch     = torch.tensor(p_batch, dtype = torch.float32)\n",
    "    \n",
    "                # Code for protein batches for protein embeddings\n",
    "#                 p_batch     = protein_real_time(p_batch)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    p_batch  = p_batch.to(device)\n",
    "                    features = features.to(device)\n",
    "                    targets  = targets.to(device)\n",
    "                    \n",
    "                    \n",
    "                predictions = self.model.forward(features, p_batch) \n",
    "                \n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                self.loss.backward()\n",
    "                \n",
    "            \n",
    "                self.optimizer.step()\n",
    "#                 self.lr_scheduler.step()\n",
    "                # Left off here trying to fix the generalizatin of trainer 05May2024\n",
    "                if self.val_dataloader is not None:\n",
    "                  with torch.no_grad():\n",
    "                    for val_features, val_targets in self.val_dataloader:\n",
    "                      val_predictions = model.forward(val_features)\n",
    "                      val_loss        = self.loss_fn(val_predictions, val_targets)\n",
    "\n",
    "                      print(f'Epoch {self.epoch} w/ Loss: ', val_loss)\n",
    "                \n",
    "                # Make sure GPU does not get overloaded with unnecessary_memory\n",
    "                del p_batch, features, targets\n",
    "                \n",
    "                if index % 100 == 0:\n",
    "                    gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Gradients Are Vanishing Confirmed 18May2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loaded\n",
      "test_loaded\n"
     ]
    }
   ],
   "source": [
    "def load_processed_data(batch_data = False, batch_size = None):\n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - batch_data (bool):\n",
    "    - batches (int):\n",
    "    \n",
    "    Returns:\n",
    "    - final_train\n",
    "    - final_test\n",
    "    - x_train_p\n",
    "    - x_test_p\n",
    "    - y_train\n",
    "    - y_test\n",
    "    \n",
    "    '''\n",
    "#     global final_train, final_test, x_train_p, x_test_p, y_train, y_test\n",
    "    \n",
    "    class_0_size                                                  = 3000000\n",
    "    embedding_type                                                = 'morgan_fingerprints' # DeepChem, self-made, MACCS, morgan_fingerprints\n",
    "    protein_dtype                                                 = 'numpy'\n",
    "    if batch_data and batch_size is not None:\n",
    "        final_train = {}\n",
    "        final_test  = {}\n",
    "        x_train_p   = {}\n",
    "        x_test_p    = {}\n",
    "        y_train     = {}\n",
    "        y_test      = {}\n",
    "        batches     = math.ceil(len(train_data.index) / batch_size)\n",
    "        \n",
    "        for batch in tqdm(range(batches), total = batches):\n",
    "            try:\n",
    "                t_batch   = train_data[batch * batch_size:(batch + 1)*batch_size]\n",
    "\n",
    "                f_train, f_test, x_p_train, x_p_test, y_tr, y_te    = preprocess_build(t_batch, \n",
    "                                                                        embedded_sequences,\n",
    "                                                                        batch_size     = class_0_size, \n",
    "                                                                        batch_loading  = False,\n",
    "                                                                        embedding_type = embedding_type,\n",
    "                                                                        protein_dtype = protein_dtype)\n",
    "                final_train[batch] = f_train\n",
    "                final_test[batch]  = f_test\n",
    "                x_train_p[batch]   = x_p_train\n",
    "                x_test_p[batch]    = x_p_test\n",
    "                y_train[batch]     = y_tr\n",
    "                y_test[batch]      = y_te\n",
    "\n",
    "                del f_train, f_test, x_p_train, x_p_test, y_tr, y_te\n",
    "                gc.collect()\n",
    "                get_memory_usage()\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "    else:\n",
    "        final_train, final_test, x_train_p, x_test_p, y_train, y_test = preprocess_build(train_data, \n",
    "                                                                    embedded_sequences,\n",
    "                                                                    batch_size     = class_0_size, \n",
    "                                                                    batch_loading  = False,\n",
    "                                                                    embedding_type = embedding_type,\n",
    "                                                                    protein_dtype = protein_dtype)\n",
    "    \n",
    "    \n",
    "    return final_train, final_test, x_train_p, x_test_p, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# final_train, final_test, x_train_p, x_test_p, y_train, y_test = load_processed_data(batch_size = 500000, \n",
    "#                                                                                     batch_data = True)\n",
    "\n",
    "def load_processed_batches(test_only = False):\n",
    "    \n",
    "    \n",
    "    if not test_only:\n",
    "        final_train  = np.load('final_train.npz')\n",
    "        final_train  = {key: value for key, value in final_train.items()}\n",
    "        final_train  = np.concatenate([final_train[index] for index in final_train.keys()])\n",
    "        print('train_loaded')\n",
    "    else:\n",
    "        final_train  = None\n",
    "    \n",
    "    final_test   = np.load('final_test.npz')\n",
    "    final_test   = {key: value for key, value in final_test.items()}\n",
    "    final_test   = np.concatenate([final_test[index] for index in final_test.keys()])\n",
    "    print('test_loaded')\n",
    "    x_train_p    = pickle.load(open('x_train_p.pckl', 'rb'))\n",
    "    x_test_p     = pickle.load(open('x_test_p.pckl', 'rb'))\n",
    "    y_train      = pickle.load(open('y_train.pckl', 'rb'))\n",
    "    y_test       = pickle.load(open('y_test.pckl', 'rb'))\n",
    "    \n",
    "    y_train      = torch.concatenate([y_train[index] for index in y_train.keys()], axis = 0)\n",
    "    y_test       = torch.concatenate([y_test[index] for index in y_test.keys()], axis = 0)\n",
    "    \n",
    "    x_train_pcat = []\n",
    "    x_test_pcat  = []\n",
    "    for key in x_train_p.keys():\n",
    "        x_train_pcat += x_train_p[key]\n",
    "        x_test_pcat  += x_test_p[key]\n",
    "        \n",
    "    return final_train, final_test, x_train_pcat, x_test_pcat, y_train, y_test\n",
    "\n",
    "final_train, final_test, x_train_p, x_test_p, y_train, y_test = load_processed_batches(test_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0     = 500000\n",
    "train_1     = 3500000\n",
    "final_train = np.concatenate([final_train[:train_0], final_train[train_1:]], axis = 0)\n",
    "y_t         = pd.Series(y_train.squeeze(1))\n",
    "y_train     = pd.concat([y_t[y_t==0][:train_0], y_t[y_t==1]], axis = 0)\n",
    "y_train     = torch.tensor(list(y_train.reset_index().drop(columns = ['index'])[0])).unsqueeze(1)\n",
    "x_train_p   = x_train_p[:train_0] + x_train_p[train_1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_data(final_train, y_train):\n",
    "    \n",
    "    '''\n",
    "    Evenly spread out binding and non binding for model later\n",
    "    So that we don't exhaust memory indexing large arrays\n",
    "    \n",
    "    '''\n",
    "    class_0_data = final_train[:len(y_train[y_train==0])]\n",
    "    class_1_data = final_train[len(y_train[y_train==0]):]\n",
    "\n",
    "    # Determine the number of points to interleave\n",
    "    num_class_1 = len(class_1_data)\n",
    "    num_class_0 = len(class_0_data)\n",
    "    \n",
    "   \n",
    "    # Calculate the step size to interleave class 1 data\n",
    "    step_size = num_class_0 // num_class_1\n",
    "    \n",
    "\n",
    "    # Initialize arrays to hold the interleaved data\n",
    "    interleaved_data   = []\n",
    "    interleaved_labels = []\n",
    "\n",
    "    # Interleave class 1 data into class 0 data\n",
    "    j = 0\n",
    "    for i in range(num_class_0):\n",
    "        interleaved_data.append(class_0_data[i])\n",
    "        interleaved_labels.append(0)\n",
    "        if (i + 1) % step_size == 0 and j < num_class_1:\n",
    "            interleaved_data.append(class_1_data[j])\n",
    "            interleaved_labels.append(1)\n",
    "            j += 1\n",
    "\n",
    "    # Add any remaining class 1 data if there's any left\n",
    "    while j < num_class_1:\n",
    "        interleaved_data.append(class_1_data[j])\n",
    "        interleaved_labels.append(1)\n",
    "        j += 1\n",
    "\n",
    "    final_train = np.array(interleaved_data)\n",
    "    y_train     = torch.tensor(interleaved_labels).unsqueeze(1)\n",
    "\n",
    "    del interleaved_data, interleaved_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_train, y_train\n",
    "\n",
    "# final_train, y_train = stratify_data(final_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test, y_test   = stratify_data(final_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_real_time(protein_batch):\n",
    "    '''\n",
    "    Process In real time to prevent memory and performance drops\n",
    "    \n",
    "    Parameters:\n",
    "        protein_batch: \n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    '''\n",
    "    return torch.concatenate([p for p in protein_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_workflow(data, fingerprints):\n",
    "    \n",
    "    proteins    = data['protein_name']\n",
    "    proteins    = list(proteins.apply(lambda x: embedded_sequences[x]))\n",
    "    final_test  = TensorDataset(fingerprints)\n",
    "    final_test  = DataLoader(final_test, batch_size = 32)\n",
    "    \n",
    "    return proteins, final_test\n",
    "\n",
    "# proteins, final_test = test_workflow(test_data, fingerprints)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate becomes too low after one epoch for JunctionTree Embeddings**\n",
    "1. Might have to rethink how we want to go about this. We don't have computing power to deal with both the protein and chemical embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb(params):\n",
    "    return xgb.XGBClassifier(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features_vectorized(maccs_list, embedding_list, metadata = None, chunk_size = None):\n",
    "#     flattened_embeddings = [embedding.flatten() for embedding in embedding_list]\n",
    "#     combined_features = np.hstack((maccs_list, flattened_embeddings))\n",
    "    \n",
    "    # Pull Metadata for Protein\n",
    "    embedded_seq_flipped     = {value: key for key, value in embedded_sequences.items()}\n",
    "    \n",
    "    if metadata is not None and chunk_size is not None:\n",
    "        protein_numpy        = np.empty(shape = (chunk_size, metadata.shape[1]))\n",
    "        for index, protein in enumerate(embedding_list):\n",
    "            protein          = embedded_seq_flipped[protein]\n",
    "            p_meta           = metadata.loc[protein]\n",
    "            protein_numpy[index] = np.array(p_meta)\n",
    "            \n",
    "        \n",
    "    # If metadata is fed, add to numpy object\n",
    "    embedding_list        = np.expand_dims(np.array(embedding_list), axis = 1)\n",
    "    if metadata is None:\n",
    "        combined_features = np.hstack((maccs_list, embedding_list))\n",
    "    else:\n",
    "        \n",
    "        metadata          = np.array(metadata)\n",
    "        combined_features = np.hstack((maccs_list, embedding_list))\n",
    "    \n",
    "    del embedded_seq_flipped, embedding_list, metadata\n",
    "    gc.collect()\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "\n",
    "def get_available_memory():\n",
    "    mem = psutil.virtual_memory()\n",
    "    available_memory = mem.available / (1024 ** 2)  # Convert from bytes to MB\n",
    "    print(f\"Available Memory: {available_memory:.2f} MB\")\n",
    "    return available_memory\n",
    "\n",
    "def process_and_train_in_batches(maccs_list, embedding_list, y_list, chunk_size=1000, weight = {0:1, 1: 1}, metadata = None, dd_pandas = False):\n",
    "    \n",
    "    client = Client()\n",
    "    \n",
    "    if isinstance(maccs_list, dict):\n",
    "        maccs_list = da.concatenate([maccs_list[index] for index in maccs_list.keys()], axis = 0)\n",
    "        \n",
    "    if chunk_size is not None:\n",
    "        clf = Incremental(SGDClassifier(loss='log_loss', class_weight = weight, alpha = 0.001))\n",
    "        \n",
    "    else:\n",
    "        chunk_size = len(maccs_list)\n",
    "\n",
    "#         clf        = SVC()\n",
    "  \n",
    "#     for i in tqdm(range(0, len(maccs_list), chunk_size)):\n",
    "#         chunk_maccs = maccs_list[i:i + chunk_size]\n",
    "#         chunk_embs  = embedding_list[i:i + chunk_size]\n",
    "#         y_train     = y_list[i:i + chunk_size]\n",
    "        \n",
    "        # Combine features for the current chunk\n",
    "    if metadata is None:\n",
    "        combined_features       = combine_features_vectorized(maccs_list, embedding_list)\n",
    "    else:\n",
    "        combined_features       = combine_features_vectorized(maccs_list, embedding_list, metadata, chunk_size)\n",
    "\n",
    "        \n",
    "#         chunk_df     = np.concatenate([combined_features\n",
    "    if dd_pandas:\n",
    "        chunk_df            = pd.DataFrame(combined_features)\n",
    "        chunk_df['targets'] = y_train\n",
    "    else:\n",
    "        final_train         = combined_features\n",
    "        y_train             = np.array(y_list)\n",
    "    \n",
    "    \n",
    "\n",
    "    del maccs_list, embedding_list, y_list\n",
    "    print('hi')\n",
    "    get_memory_usage()\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to Dask DataFrame\n",
    "#     if isinstance(chunk_df, pd.DataFrame):\n",
    "#         chunk_df    = dd.from_pandas(chunk_df, npartitions=500)\n",
    "# #         final_train =  client.scatter(combined_features)\n",
    "#         final_train = chunk_df.drop(columns = ['targets']).to_dask_array(lengths = True)\n",
    "#         y_train     = chunk_df['targets'].to_dask_array(lengths=True)\n",
    "#         del chunk_df\n",
    "\n",
    "#     else:\n",
    "    final_train = da.from_array(final_train) #, chunks = (10000, final_train.shape[1]))\n",
    "    y_train     = da.from_array(y_train) #, chunks = (10000,1))\n",
    "    \n",
    "#     final_train =  final_train.persist()\n",
    "#     y_train     =  y_train.persist()\n",
    "#         y_train     = client.scatter(np.array(chunk_labels))\n",
    "\n",
    "    print('higher')\n",
    "    get_memory_usage()\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "    # Convert to DMatrix\n",
    "\n",
    "#     dtrain     = DaskDMatrix(client, final_train, label=y_train)\n",
    "#     print('dtren')\n",
    "#         final_train = DaskDMatrix(client, final_train, y_train)\n",
    "#     get_memory_usage()\n",
    "#     del y_train, final_train\n",
    "#     gc.collect()\n",
    "    # Train the model incrementally in batches\n",
    "#         for j in range(0, final_train.shape[0], 100):\n",
    "#             X_batch = final_train[j:j+100].compute()\n",
    "#             y_batch = y_train[j:j+100].compute()\n",
    "# #             clf.partial_fit(X_batch, y_batch, classes=[0, 1])\n",
    "#             clf.fit(X_batch, y_batch)\n",
    "\n",
    "\n",
    "    params = {\n",
    "    'objective': 'binary:logistic',  # Change based on your task\n",
    "    'eval_metric': 'map',\n",
    "    'seed': '42'\n",
    "\n",
    "#     'colsample_bytree': '1',\n",
    "#     'eta': '0.4',\n",
    "#     'max_depth': '6',\n",
    "#     'n_estimators': '493',\n",
    "#     'subsample': '0.6',\n",
    "#     'gamma': '1',\n",
    "#     'scale_pos_weight': '2'\n",
    "#     'subsample': 0.2,\n",
    "#     'sampling_method': 'gradient_based'\n",
    "    }\n",
    "\n",
    "  \n",
    "\n",
    "    chunk_size = final_train.shape[0]\n",
    "    clf        = None\n",
    "    num_chunks = math.ceil(final_train.shape[0] / chunk_size)\n",
    "    num_boost_round = 20\n",
    "    \n",
    "    \n",
    "    print('stratifieid')\n",
    "    for i in tqdm(range(num_chunks), total = num_chunks):\n",
    "\n",
    "        # Load chunk into memory\n",
    "        X_chunk = final_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        y_chunk = y_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        \n",
    "        # Create DMatrix for XGBoost\n",
    "        dtrain  = xgb.DMatrix(X_chunk, label=y_chunk)\n",
    "   \n",
    "        if clf is None:\n",
    "            clf = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "        else:\n",
    "            clf = xgb.train(params, dtrain, num_boost_round=num_boost_round, xgb_model=clf)\n",
    "\n",
    "        # Free up memory\n",
    "        del X_chunk, y_chunk, dtrain\n",
    "        gc.collect()\n",
    "\n",
    "    # Train the model\n",
    "#     clf = get_xgb(params = params)\n",
    "#     num_boost_round = 10  # previously 50\n",
    "#     output = train(client, params, dtrain, num_boost_round=num_boost_round)\n",
    "    \n",
    "#     # Get the trained model\n",
    "#     clf = output['booster']\n",
    "\n",
    "#     clf         = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Wrap the classifier with ParallelPostFit\n",
    "#     clf         = ParallelPostFit(estimator=clf)\n",
    "\n",
    "    # Train the model\n",
    "#     clf.fit(X_train, y_train)\n",
    "    \n",
    "#     clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "#     clf.fit(final_train, y_train)\n",
    "        \n",
    "        # Clear memory\n",
    "#         del chunk_df, dask_df, final_train, y_train, X_batch, y_batch\n",
    "    \n",
    "    print('you did it')\n",
    "       \n",
    "    gc.collect()\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_train_in_batches(maccs_list, embedding_list, y_list, chunk_size=1000, weight={0:1, 1:1}, metadata=None, dd_pandas=False):\n",
    "    client = Client()\n",
    "    \n",
    "    if isinstance(maccs_list, dict):\n",
    "        maccs_list = da.concatenate([maccs_list[index] for index in maccs_list.keys()], axis=0)\n",
    "        \n",
    "    if chunk_size is None:\n",
    "        chunk_size = len(maccs_list)\n",
    "\n",
    "    # Combine features for the current chunk\n",
    "    if metadata is None:\n",
    "        combined_features = combine_features_vectorized(maccs_list, embedding_list)\n",
    "    else:\n",
    "        combined_features = combine_features_vectorized(maccs_list, embedding_list, metadata, chunk_size)\n",
    "\n",
    "    if dd_pandas:\n",
    "        chunk_df = pd.DataFrame(combined_features)\n",
    "        chunk_df['targets'] = y_list\n",
    "    else:\n",
    "        final_train = combined_features\n",
    "        y_train = np.array(y_list)\n",
    "    \n",
    "    del maccs_list, embedding_list, y_list\n",
    "    get_memory_usage()\n",
    "    gc.collect()\n",
    "\n",
    "    final_train = da.from_array(final_train)\n",
    "    y_train = da.from_array(y_train)\n",
    "\n",
    "    get_memory_usage()\n",
    "    gc.collect()\n",
    "\n",
    "    params = {\n",
    "        'loss_function': 'Logloss',  # Use 'CrossEntropy' for multi-class problems\n",
    "        'eval_metric': 'AUC',\n",
    "        'iterations': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'depth': 6,\n",
    "        'verbose': 200,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "\n",
    "    num_chunks = math.ceil(final_train.shape[0] / chunk_size)\n",
    "    clf = CatBoostClassifier(**params)\n",
    "    \n",
    "    print('Stratified training in chunks')\n",
    "    for i in tqdm(range(num_chunks), total=num_chunks):\n",
    "\n",
    "        # Load chunk into memory\n",
    "        X_chunk = final_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        y_chunk = y_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        \n",
    "        # Create Pool for CatBoost\n",
    "        train_pool = Pool(X_chunk, y_chunk)\n",
    "        \n",
    "        # Train the model incrementally\n",
    "        if i == 0:\n",
    "            clf.fit(train_pool, init_model=None)\n",
    "        else:\n",
    "            clf.fit(train_pool, init_model=clf, use_best_model=False)\n",
    "\n",
    "        # Free up memory\n",
    "        del X_chunk, y_chunk, train_pool\n",
    "        gc.collect()\n",
    "        \n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible Solution To Memory Issue with XGBOOST**\n",
    "1. https://xgboost.readthedocs.io/en/latest/tutorials/external_memory.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 94.34 GB\n",
      "Available Memory: 94.33 GB\n",
      "Stratified training in chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 18.20 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/distributed/client.py:3164: UserWarning: Sending large graph of size 12.91 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "2024-07-06 01:41:19,067 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:33409', name: 5, status: running, memory: 0, processing: 0>\n",
      "2024-07-06 01:41:19,067 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:34347', name: 0, status: running, memory: 0, processing: 0>\n",
      "2024-07-06 01:41:19,068 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:34687', name: 3, status: running, memory: 1, processing: 0>\n",
      "2024-07-06 01:41:19,068 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:35559', name: 6, status: running, memory: 0, processing: 0>\n",
      "2024-07-06 01:41:19,068 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:37247', name: 4, status: running, memory: 0, processing: 0>\n",
      "2024-07-06 01:41:19,069 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:43923', name: 2, status: running, memory: 0, processing: 0>\n",
      "2024-07-06 01:41:19,069 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:43949', name: 1, status: running, memory: 0, processing: 0>\n",
      "2024-07-06 01:41:19,069 - distributed.scheduler - WARNING - Worker failed to heartbeat for 440s; attempting restart: <WorkerState 'tcp://127.0.0.1:45329', name: 7, status: running, memory: 0, processing: 0>\n",
      "2024-07-06 01:41:19,528 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:41:19,586 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:41:19,594 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:41:19,635 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:41:19,658 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:41:19,662 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:41:19,695 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:41:19,718 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 139ms\tremaining: 13.8s\n",
      "99:\ttotal: 8.7s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:28<00:00, 508.91s/it]\n",
      "2024-07-06 01:59:21,755 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:59:21,756 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:59:21,759 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:59:21,762 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:59:21,766 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:59:21,769 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:59:21,772 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-07-06 01:59:21,775 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "model_type                  = 'random_forest'\n",
    "\n",
    "# if train_data was not loaded, try tensor operations\n",
    "try:\n",
    "    class1                  = len(train_data[train_data['binds'] == 1].dropna())\n",
    "    class0                  = len(train_data[train_data['binds'] == 0].dropna()[:class_0_size])\n",
    "except NameError:\n",
    "    class1                  = len(y_test[y_test==1]) + len(y_train[y_train==1])\n",
    "    class0                  = len(y_test[y_test==0]) + len(y_train[y_train==0])\n",
    "    \n",
    "weight                      = class0 / class1\n",
    "try:\n",
    "    protein_size            = max(embedded_sequences['sEH'].size())\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "loss_function               = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weight))\n",
    "rf_features                 = []\n",
    "if model_type == 'nueral_network':\n",
    "# loss_function             = nn.BCELoss()\n",
    "#     model                 = Model4(input_dim   = 30, \n",
    "#                                 hidden_dim   = 30, \n",
    "#                                 protein_size = protein_size,\n",
    "#                                 kernel_size  = 10).to(device)\n",
    "#     model                   = ResModel().to(device)\n",
    "    clf        = SimpleNN().to(device)\n",
    "#     model.apply(initialize_weights_he)\n",
    "    \n",
    "    # model              = Model1(input_dim  = 1,\n",
    "    #                            kernel_size = 10,\n",
    "    #                            protein_size = protein_size)\n",
    "\n",
    "    # model               = MorganModel().to(device)\n",
    "\n",
    " \n",
    "    batch_size              = 256\n",
    "    epochs                  = 4\n",
    "    # For Molecular Graph NN\n",
    "#     trainer             = Trainer(max_epochs = epochs, bypass_prepare = True, batch_size = batch_size)\n",
    "    trainer                 = Trainer(max_epochs = epochs, bypass_prepare = False, batch_size = batch_size)\n",
    "    # trainer             = Trainer(max_epochs = 20, bypass_prepare = False, batch_size = batch_size)\n",
    "    optimizer               = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "    scheduler               = lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, factor=0.1)\n",
    "    \n",
    "    trainer.fit(model, [final_train, y_train], x_train_p, None, optimizer, loss_function, scheduler)\n",
    "#     trainer.fit(model, final_train, x_train_p, None, optimizer, loss_function, scheduler)\n",
    "    \n",
    "elif model_type == 'random_forest':\n",
    "#     chunk_size             = 50000\n",
    "    chunk_size             = None\n",
    "    protein_metadata       = None\n",
    "    class_weights          = {0: 1, 1: weight/8}\n",
    "    clf = process_and_train_in_batches(final_train, x_train_p, y_train, chunk_size=chunk_size, weight = class_weights, metadata = protein_metadata, dd_pandas = False) \n",
    "    \n",
    "#   Evaluate the model\n",
    "#   score = clf.score(X_test, y_test)\n",
    "#   print(f\"Test score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7fd5e0127610>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = xgb.XGBClassifier()\n",
    "clf = CatBoostClassifier()\n",
    "clf.load_model('5.5mil_catboost_24-07-06-01:59.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torch_model(model_path):\n",
    "    \n",
    "    model               = ResModel()\n",
    "    state_dict          = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model   =    load_torch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m x_tp   \u001b[38;5;241m=\u001b[39m x_test_p[batch\u001b[38;5;241m*\u001b[39mbatch_s:(batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_s]\n\u001b[1;32m     93\u001b[0m y_t    \u001b[38;5;241m=\u001b[39m y_test[batch\u001b[38;5;241m*\u001b[39mbatch_s:(batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_s]\n\u001b[0;32m---> 94\u001b[0m pred, prob, metrics_dictionary[batch] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(pred)\n\u001b[1;32m     96\u001b[0m probabilities\u001b[38;5;241m.\u001b[39mextend(prob)\n",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(clf, X_test_maccs, X_test_protein, y_test, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(clf, catboost\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mCatBoostClassifier):\n\u001b[1;32m     30\u001b[0m         X_test_prepared   \u001b[38;5;241m=\u001b[39m prepare_test_data(X_test_maccs, X_test_protein)\n\u001b[0;32m---> 31\u001b[0m         probabilities     \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_prepared\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m         predictions       \u001b[38;5;241m=\u001b[39m (probabilities \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# Predict using the prepared test data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#          if 'leash_utils' in str(type(clf)) or 'ResModel' in str(clf):\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/catboost/core.py:5276\u001b[0m, in \u001b[0;36mCatBoostClassifier.predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   5225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, prediction_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m, ntree_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ntree_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, thread_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   5226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5227\u001b[0m \u001b[38;5;124;03m    Predict with data.\u001b[39;00m\n\u001b[1;32m   5228\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5274\u001b[0m \u001b[38;5;124;03m              with log probability for every class for each object.\u001b[39;00m\n\u001b[1;32m   5275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/catboost/core.py:2605\u001b[0m, in \u001b[0;36mCatBoost._predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2604\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m data, data_is_single_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_predict_input_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_method_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_prediction_type(prediction_type)\n\u001b[1;32m   2608\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/catboost/core.py:2585\u001b[0m, in \u001b[0;36mCatBoost._process_predict_input_data\u001b[0;34m(self, data, parent_method_name, thread_count, label)\u001b[0m\n\u001b[1;32m   2583\u001b[0m is_single_object \u001b[38;5;241m=\u001b[39m _is_data_single_object(data)\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Pool):\n\u001b[0;32m-> 2585\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mPool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_single_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cat_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2590\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_embedding_feature_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFeaturesData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthread_count\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, is_single_object\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/catboost/core.py:848\u001b[0m, in \u001b[0;36mPool.__init__\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[1;32m    843\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[1;32m    844\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    846\u001b[0m             )\n\u001b[0;32m--> 848\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_can_be_none:\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/catboost/core.py:1481\u001b[0m, in \u001b[0;36mPool._init\u001b[0;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m     feature_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_transform_tags(feature_tags, feature_names)\n\u001b[0;32m-> 1481\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_tags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_test_data(X_test_maccs, X_test_protein):\n",
    "    \"\"\"\n",
    "    Combine and preprocess test data similarly to training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Flatten the protein embeddings and concatenate with maccs features\n",
    "    X_test_combined = combine_features_vectorized(X_test_maccs, X_test_protein)\n",
    "\n",
    "    return X_test_combined\n",
    "\n",
    "def evaluate_model(clf, X_test_maccs, X_test_protein, y_test, batch_size = 200000):\n",
    "    \"\"\"\n",
    "    Evaluate the model using the prepared test data.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Nueral network is a bad choice at least how I have it defined\n",
    "\n",
    "   \n",
    "        \n",
    "    gradient_boost =  isinstance(clf, xgb.core.Booster) or isinstance(clf, xgb.sklearn.XGBClassifier)\n",
    "    if gradient_boost:\n",
    "        X_test_prepared   = prepare_test_data(X_test_maccs, X_test_protein)\n",
    "        X_test_prepared   = dd.from_array(X_test_prepared)\n",
    "        X_test_prepared   = xgb.DMatrix(X_test_prepared)\n",
    "        probabilities     = clf.predict(X_test_prepared)\n",
    "        predictions       = (probabilities >= 0.5).astype(int)\n",
    "        \n",
    "    elif isinstance(clf, catboost.core.CatBoostClassifier):\n",
    "        X_test_prepared   = prepare_test_data(X_test_maccs, X_test_protein)\n",
    "        probabilities     = clf.predict(X_test_prepared)[:, 1]\n",
    "        predictions       = (probabilities >= 0.5).astype(int)\n",
    "        \n",
    "    else:\n",
    "        # Predict using the prepared test data\n",
    "#          if 'leash_utils' in str(type(clf)) or 'ResModel' in str(clf):\n",
    "        batches           = math.ceil(len(X_test_maccs) / batch_size)\n",
    "       \n",
    "        X_test_maccs      = torch.tensor(X_test_maccs, device = device)\n",
    "        X_test_protein    = torch.tensor(X_test_protein, dtype = torch.float32, device = device)\n",
    "        predictions       = []\n",
    "        for batch in tqdm(range(batches), total = batches):\n",
    "            subset_maccs  = X_test_maccs[batch*batch_size:(batch + 1)*batch_size]\n",
    "            subset_prot   = X_test_protein[batch*batch_size:(batch + 1)*batch_size]\n",
    "            subset_pred   = clf(subset_maccs, subset_prot).cpu().detach().numpy()\n",
    "            predictions.extend(subset_pred)\n",
    "            \n",
    "            \n",
    "            del subset_maccs, subset_prot, subset_pred\n",
    "            gc.collect()\n",
    "            \n",
    "        probabilities     = np.array(predictions) \n",
    "        predictions       = (probabilities >= 0.5).astype(int)\n",
    "        del X_test_maccs, X_test_protein\n",
    "        gc.collect()\n",
    "        \n",
    "       \n",
    "    \n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(predictions, y_test)\n",
    "    recall    = recall_score(predictions, y_test)\n",
    "    f1        = f1_score(predictions, y_test)\n",
    "    accuracy  = accuracy_score(predictions, y_test)\n",
    "    mae       = mean_absolute_error(predictions, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    metrics_dictionary = {'Precision': precision,\n",
    "                         'Recall': recall,\n",
    "                         'F1 Score:': f1,\n",
    "                         'Accuracy': accuracy,\n",
    "                         'MAE': mae}\n",
    "\n",
    "    return predictions, probabilities, metrics_dictionary\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Assuming X_test_maccs and X_test_protein are defined similarly to how training data was defined\n",
    "# These should be appropriately reshaped or transformed to match the training data setup\n",
    "\n",
    "batch_s    = math.ceil(final_test.shape[0] / 5)\n",
    "\n",
    "predictions        = [] \n",
    "probabilities      = []\n",
    "metrics_dictionary = {}\n",
    "for batch in range(5):\n",
    "    f_test = final_test[batch*batch_s:(batch+1)*batch_s]\n",
    "    x_tp   = x_test_p[batch*batch_s:(batch+1)*batch_s]\n",
    "    y_t    = y_test[batch*batch_s:(batch+1)*batch_s]\n",
    "    pred, prob, metrics_dictionary[batch] = evaluate_model(clf, f_test, x_tp, y_t)\n",
    "    predictions.extend(pred)\n",
    "    probabilities.extend(prob)\n",
    "\n",
    "# predictions = evaluate_model(clf, final_test, x_test_p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xgboost_model(clf, model_name, metrics_dict = None):\n",
    "    \n",
    "    clf.save_model(model_name)\n",
    "    \n",
    "    return\n",
    "\n",
    "# timestamp  = datetime.now().strftime('%y-%m-%d-%H:%M')\n",
    "# model_name = f'5.5mil_catboost_{timestamp}.json'\n",
    "# save_xgboost_model(clf, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xgboost_model():\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.load_model('XGBoost_50rounds_random_dbduck.json')\n",
    "    return model\n",
    "# clf.save_model('XGBoost_50rounds_random_dbduck.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier Results\n",
    "**MACCS Keys Test**\n",
    "1. Weight_ratio = class_imbalance/8, class_imbance = 300000/2562\n",
    "1. Precision: 0.4114906832298137\n",
    "2. Recall: 0.3035509736540664\n",
    "3. F1 Score: 0.34937376400791037\n",
    "\n",
    "**RDKit Fingerprints radius = 2, n_bits = 1024**\n",
    "1. Weight_ratio = class_imbalance/8, class_inbalance = 300000/2562\n",
    "2. precisiion = 0.642\n",
    "3. Recall     = 0.4899\n",
    "4. F1 Score   = 0.556\n",
    "\n",
    "**RdKit Fingerprints radius = 2, n_bits = 2048**\n",
    "1. Precision: 0.687888198757764\n",
    "2. Recall: 0.3983812949640288\n",
    "3. F1 Score: 0.5045558086560364\n",
    "\n",
    "**Rdkit Fingerprints radius = 3, n_bits = 1024**\n",
    "1. Precision: 0.7096273291925466\n",
    "2. Recall: 0.3079514824797844\n",
    "3. F1 Score: 0.42951127819548873\n",
    "\n",
    "**RDKit Fingerprints raidus = 3, n_bits = 2048**\n",
    "1. Precision: 0.7142857142857143\n",
    "2. Recall: 0.40069686411149824\n",
    "3. F1 Score: 0.5133928571428571\n",
    "\n",
    "# XGBoost Classier Results\n",
    "**Boosting_rounds = 50**\n",
    "1. Precision: 0.5714285714285714\n",
    "2. Recall: 0.9363867684478372\n",
    "3. F1 Score: 0.7097396335583414\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nueral Network with 5.5 Million Training/Test ResModel**\n",
    "1. Precision: 1.0\n",
    "2. Recall: 0.2844247168289711\n",
    "3. F1 Score: 0.4428826588313684\n",
    "\n",
    "**XGBoost with 5.5 Million Training/Test Num_boosting_rounds = 30**\n",
    "Precision: 0.5395745228946182\n",
    "Recall: 0.7427490728564885\n",
    "F1 Score: 0.6250660565630692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38721 instead\n",
      "  warnings.warn(\n",
      "  1%|          | 1/84 [00:17<24:53, 17.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 108.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/84 [00:35<24:23, 17.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 108.02 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/84 [00:54<24:30, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 108.01 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4/84 [01:12<24:13, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 108.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/84 [01:30<23:53, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 108.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/84 [01:48<23:42, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.99 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 7/84 [02:07<23:27, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.99 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 8/84 [02:25<23:12, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.98 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 9/84 [02:45<23:16, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.98 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 10/84 [03:03<22:52, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.97 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 11/84 [03:21<22:27, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.97 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 12/84 [03:39<22:05, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.96 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 13/84 [03:58<21:47, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.95 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 14/84 [04:16<21:32, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.95 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 15/84 [04:35<21:18, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.95 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 16/84 [04:54<20:58, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.94 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 17/84 [05:11<20:26, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.94 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 18/84 [05:30<20:10, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 19/84 [05:48<19:53, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 20/84 [06:07<19:33, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.92 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 21/84 [06:26<19:32, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.92 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 22/84 [06:45<19:17, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.92 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 23/84 [07:03<18:58, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.91 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 24/84 [07:22<18:50, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.91 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 25/84 [07:41<18:26, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.90 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 26/84 [08:00<18:04, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.90 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 27/84 [08:19<18:00, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.89 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 28/84 [08:38<17:47, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.89 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 29/84 [08:57<17:27, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.89 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 30/84 [09:15<16:50, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.88 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 31/84 [09:34<16:25, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.88 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 32/84 [09:52<16:04, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.87 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 33/84 [10:11<15:45, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.86 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 34/84 [10:30<15:43, 18.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.86 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 35/84 [10:49<15:27, 18.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.86 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 36/84 [11:08<15:06, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.86 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 37/84 [11:27<14:46, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.85 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 38/84 [11:45<14:22, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.85 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 39/84 [12:04<14:02, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.84 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 40/84 [12:23<13:42, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.84 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 41/84 [12:43<13:39, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.84 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 42/84 [13:02<13:22, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.83 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 43/84 [13:20<12:54, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.83 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 44/84 [13:39<12:31, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.82 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 45/84 [13:57<12:09, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.82 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 46/84 [14:16<11:54, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.81 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 47/84 [14:36<11:41, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.81 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 48/84 [14:54<11:18, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.81 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 49/84 [15:14<11:04, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.81 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 50/84 [15:32<10:39, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.80 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 51/84 [15:51<10:21, 18.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.79 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 52/84 [16:09<09:59, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.79 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 53/84 [16:28<09:39, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.79 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 54/84 [16:47<09:21, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.79 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 55/84 [17:06<09:07, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.78 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 56/84 [17:25<08:48, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.77 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 57/84 [17:44<08:28, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.77 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 58/84 [18:03<08:10, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.76 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 59/84 [18:22<07:52, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.76 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 60/84 [18:41<07:36, 19.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.76 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 61/84 [19:00<07:16, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.76 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 62/84 [19:18<06:54, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.76 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 63/84 [19:37<06:37, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.75 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 64/84 [19:56<06:16, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.75 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 65/84 [20:14<05:54, 18.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.75 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 66/84 [20:32<05:31, 18.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.75 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 67/84 [20:51<05:17, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.74 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 68/84 [21:11<05:02, 18.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.74 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 69/84 [21:30<04:45, 19.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.74 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 70/84 [21:49<04:27, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.72 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 71/84 [22:09<04:10, 19.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.73 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 72/84 [22:29<03:52, 19.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.73 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 73/84 [22:48<03:31, 19.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.71 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 74/84 [23:06<03:09, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.71 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 75/84 [23:24<02:49, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.71 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 76/84 [23:43<02:29, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.70 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 77/84 [24:01<02:09, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.70 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 78/84 [24:20<01:51, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.70 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 79/84 [24:39<01:33, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.70 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 80/84 [24:57<01:14, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.69 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 81/84 [25:16<00:55, 18.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.69 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 82/84 [25:34<00:36, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.69 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 83/84 [25:52<00:18, 18.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.69 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [26:06<00:00, 18.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 107.68 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test(model, model_type = 'xgboost'):\n",
    "    test_preds       = []\n",
    "    raw_preds        = []\n",
    "    client           = Client()\n",
    "    batch_size       = 20000\n",
    "    batches          = math.ceil(len(test_data) / batch_size)\n",
    "    generator        = rdFingerprintGenerator.GetMorganGenerator(radius = 3, fpSize = 2048)\n",
    "    for index, batch in tqdm(enumerate(range(batches)), total = batches):\n",
    "        smiles_list  = []\n",
    "\n",
    "        smiles       = test_data['molecule_smiles'][index*batch_size:(index + 1)*batch_size]\n",
    "        protein_test = test_data['protein_name'][index*batch_size:(index + 1)*batch_size]\n",
    "        protein_test = list(protein_test.apply(lambda x: embedded_sequences[x]))\n",
    "\n",
    "        for smile in smiles:\n",
    "            mol      = Chem.MolFromSmiles(smile)\n",
    "            fp       = generator.GetFingerprint(mol).ToList()   \n",
    "            smiles_list.append(fp)\n",
    "\n",
    "        smiles            = np.array(smiles_list)\n",
    "        \n",
    "\n",
    "#         test_eval        = pd.DataFrame(combined_features)\n",
    "        \n",
    "        # Convert to Dask DataFrame\n",
    "#         test_eval          = dd.from_pandas(test_eval, npartitions=10)\n",
    "        \n",
    "        if model_type == 'xgboost':\n",
    "            combined_features = combine_features_vectorized(smiles_list,\n",
    "                                                        protein_test,\n",
    "                                                        chunk_size = batch_size, \n",
    "                                                        metadata = protein_metadata)\n",
    "            test_eval          = da.from_array(combined_features, chunks = (batch_size, -1))\n",
    "            test_eval          = DaskDMatrix(client, test_eval)\n",
    "            predictions        = predict(client, model, test_eval)\n",
    "\n",
    "            test_preds.extend(predictions.compute())\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            combined_features = None\n",
    "            protein_metadata  = None\n",
    "            test_eval = combine_features_vectorized(smiles_list,\n",
    "                                                        protein_test,\n",
    "                                                        chunk_size = batch_size, \n",
    "                                                        metadata = protein_metadata)\n",
    "            probabilities     = clf.predict_proba(test_eval)\n",
    "            predictions       = (probabilities >= 0.5).astype(int)\n",
    "            test_preds.extend(probabilities)\n",
    "            \n",
    "        elif model_type == 'nueral_network':\n",
    "            test_eval           = None\n",
    "            combined_features   = None\n",
    "            smiles              = torch.tensor(smiles)\n",
    "            protein_test        = torch.tensor(protein_test, dtype = torch.float32)\n",
    "            predictions         = model(smiles, protein_test).detach().numpy()\n",
    "            \n",
    "            test_preds.extend(predictions)\n",
    "                \n",
    "        del predictions, test_eval, smiles, combined_features, protein_test\n",
    "\n",
    "        gc.collect()\n",
    "        get_memory_usage()\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "\n",
    "test_preds = evaluate_test(clf, model_type = 'catboost')\n",
    "# test_preds = evaluate_test(model, model_type = 'nueral_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m test_preds:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m(\u001b[38;5;28mmax\u001b[39m(pred)))\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# for pred in test_preds:\n",
    "#     print(pred.in(max(pred)))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred1 = np.array(test_preds)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49956091395458363\n"
     ]
    }
   ],
   "source": [
    "pred\n",
    "for pred in test_preds:\n",
    "    print(pred[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.concat([test_data['id'], pd.Series(test_preds, name = 'binds')], axis = 1)\n",
    "\n",
    "submission = pd.concat([test_data['id'], pd.Series(test_pred1, name = 'binds')], axis = 1)\n",
    "submission.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'DeepChem_100k.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MACCS Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_test = torch.from_numpy(final_test)\n",
    "# final_test = final_test.to('cuda')\n",
    "# x_test_p   = [protein.to('cuda') for protein in x_test_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 9.61 GB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fingerprints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         p_test  \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_name\u001b[39m\u001b[38;5;124m'\u001b[39m][index\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100000\u001b[39m:]\n\u001b[1;32m     25\u001b[0m     p_test      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(p_test\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: embedded_sequences[x]))\n\u001b[0;32m---> 26\u001b[0m     batch       \u001b[38;5;241m=\u001b[39m TensorDataset(\u001b[43mfingerprints\u001b[49m)\n\u001b[1;32m     27\u001b[0m     batch       \u001b[38;5;241m=\u001b[39m DataLoader(batch, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Train Code\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     p_test       = x_test_p[index*batch_size: (index + 1) * batch_size]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fingerprints' is not defined"
     ]
    }
   ],
   "source": [
    "test_preds       = []\n",
    "raw_preds        = []\n",
    "batch_size       = 32\n",
    "batches          = math.ceil(len(train_data) // 100000)\n",
    "for index, batch in enumerate(range(batches)):\n",
    "    get_memory_usage()\n",
    "    # Separate Test code\n",
    "#     p_test, final_test = test_workflow(batch, fingerprints)\n",
    "    \n",
    "#     fingerprints = []\n",
    "#         file_prefix = f'Batched Morgan'\n",
    "#         for batch in range(batches):\n",
    "#             get_memory_usage()\n",
    "       \n",
    "    \n",
    "#     file_name    = f'Batched Morgan/test_morgan_{batch}.pckl'\n",
    "#     fingerprints = pickle.load(open(file_name, 'rb'))\n",
    "    \n",
    "    if index < batches - 1:\n",
    "        p_test  = test_data['protein_name'][index*100000:(index + 1)*100000]\n",
    "        \n",
    "    else:\n",
    "        p_test  = test_data['protein_name'][index*100000:]\n",
    "        \n",
    "    p_test      = list(p_test.apply(lambda x: embedded_sequences[x]))\n",
    "    batch       = TensorDataset(fingerprints)\n",
    "    batch       = DataLoader(batch, batch_size = 32)\n",
    "    \n",
    "    \n",
    "    # Train Code\n",
    "#     p_test       = x_test_p[index*batch_size: (index + 1) * batch_size]\n",
    "    if device.type == 'cuda':\n",
    "        p_test     = protein_real_time(p_test).to(device)\n",
    "        features   = batch[0].to(device)\n",
    "#         features  = batch.to(device)\n",
    "        \n",
    "    test_results = model(features, p_test)\n",
    "    raw_prob     = copy.copy(test_results)\n",
    "        \n",
    "    test_results = [1 if x > 0.5 else 0 for x in test_results]\n",
    "    test_preds.append(test_results)\n",
    "    raw_preds.append(raw_prob)\n",
    "    \n",
    "    \n",
    "predictions      = []\n",
    "for sub in test_preds:\n",
    "    for result in sub:\n",
    "        predictions.append(result)\n",
    "\n",
    "probabilities    = []\n",
    "for sub in raw_preds:\n",
    "    for result in sub:\n",
    "        probabilities.append(float(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 13.09 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m get_memory_usage()\n\u001b[1;32m      9\u001b[0m file_name          \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatched Morgan/test_morgan_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpckl_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pckl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m fingerprints       \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m batch                \u001b[38;5;241m=\u001b[39m test_data[pckl_batch \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100000\u001b[39m:(pckl_batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100000\u001b[39m]\n\u001b[1;32m     12\u001b[0m proteins, final_test \u001b[38;5;241m=\u001b[39m test_workflow(batch, fingerprints)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/storage.py:336\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;129m@_share_memory_lock_protected\u001b[39m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_share_filename_cpu_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_share_filename_cpu_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(io\u001b[38;5;241m.\u001b[39mBytesIO(b))\n\u001b[1;32m    340\u001b[0m _StorageBase\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m _type  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions      = []\n",
    "probabilities    = []\n",
    "batch_size       = 32\n",
    "batches          = math.ceil(len(test_data) // 100000)\n",
    "\n",
    "start_batch      = 10\n",
    "for pckl_batch in range(start_batch, batches+1):\n",
    "    get_memory_usage()\n",
    "    file_name          = f'Batched Morgan/test_morgan_{pckl_batch}.pckl'\n",
    "    fingerprints       = pickle.load(open(file_name, 'rb'))\n",
    "    batch                = test_data[pckl_batch * 100000:(pckl_batch + 1)*100000]\n",
    "    proteins, final_test = test_workflow(batch, fingerprints)\n",
    "    \n",
    "    test_preds         = []\n",
    "    raw_preds          = []\n",
    "    \n",
    "    for index, batch in enumerate(final_test):\n",
    "     \n",
    "        # Train Code\n",
    "        p_test         = proteins[index*batch_size: (index + 1) * batch_size]\n",
    "        if device.type == 'cuda':\n",
    "            p_test     = protein_real_time(p_test).to(device)\n",
    "            features   = batch[0].to(device)\n",
    "#             features  = batch.to(device)\n",
    "\n",
    "        test_results = model(features, p_test)\n",
    "        raw_prob     = copy.copy(test_results)\n",
    "\n",
    "        test_results = [1 if x > 0.5 else 0 for x in test_results]\n",
    "        test_preds.append(test_results)\n",
    "        raw_preds.append(raw_prob)\n",
    "    \n",
    "    for sub in test_preds:\n",
    "        for result in sub:\n",
    "            predictions.append(result)\n",
    "\n",
    " \n",
    "    for sub in raw_preds:\n",
    "        for result in sub:\n",
    "            probabilities.append(float(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Example function to count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Function to print the model summary\n",
    "def print_model_summary(model):\n",
    "    print(f\"{'Layer Name':<25} {'Output Shape':<25} {'Param #':<15}\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    model_modules = []\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and name != \"\":\n",
    "            params_list = list(module.parameters())\n",
    "            output_shape = params_list[0].size() if params_list else 'N/A'\n",
    "#             param_num = count_parameters(module)\n",
    "#             meta_info = f\"{name:<25} {str(output_shape):<25} {param_num}\"\n",
    "#             print(meta_info)\n",
    "#             total_params += param_num\n",
    "#             model_modules.append(meta_info)\n",
    "\n",
    "#     print(\"=\" * 65)\n",
    "#     print(f\"Total Trainable Parameters: {total_params}\")\n",
    "#     return model_modules\n",
    "\n",
    "# for index, batch in enumerate(final_test):\n",
    "#     p_test       = x_test_p[index*batch_size: (index + 1) * batch_size]\n",
    "#     test_results = model(batch, p_test) \n",
    "#     break\n",
    "\n",
    "# Print the model summary now that the parameters are initialized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_main(index, vocab_tokens, mode = 'morgan', batch_size = 100000):\n",
    "    ''' \n",
    "    This function handles all the function calls for loading\n",
    "    data in batches all the way to feeding it to the model for inference\n",
    "    and saving the predicted probabilites to CSV file for competition_submission\n",
    "    \n",
    "    Parameters:\n",
    "    - index:\n",
    "    - vocab_tokens:\n",
    "    - batch_size: \n",
    "    \n",
    "    Returns:\n",
    "    - final_test:\n",
    "    - proteins: \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if mode == 'junction_tree'\n",
    "    # Data Batch is a list containing matricies\n",
    "        data_batch            = load_embeddings(base_path = 'Batched Data/test_preprocessed_Batch', index = index)\n",
    "        final_test, proteins  = _prepare_data(data_batch = data_batch,\n",
    "                                         batch_size = batch_size, \n",
    "                                         index = index, \n",
    "                                         vocab_tokens = vocab_tokens,\n",
    "                                         mode = 'Evaluate')\n",
    "#     final_test    = create_gcn_data(mode = 'Evaluate', final_index = batch_size)\n",
    "\n",
    "    elif mode == 'morgan':\n",
    "        final_test = \n",
    "        \n",
    "    \n",
    "    return final_test, proteins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Batch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [11:34<00:00, 143.95it/s]\n",
      "100%|██████████| 100000/100000 [01:59<00:00, 836.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:10<00:00, 1415.18it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 220888.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Batch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [11:35<00:00, 143.88it/s]\n",
      "100%|██████████| 100000/100000 [02:13<00:00, 748.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:12<00:00, 1377.57it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 196980.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Batch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [11:28<00:00, 145.17it/s]\n",
      "100%|██████████| 100000/100000 [02:03<00:00, 811.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:10<00:00, 1419.65it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 215042.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Batch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [11:24<00:00, 146.19it/s]\n",
      "100%|██████████| 100000/100000 [01:50<00:00, 907.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:09<00:00, 1435.38it/s]\n",
      "100%|██████████| 100000/100000 [00:04<00:00, 20192.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Batch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74896/74896 [08:19<00:00, 149.81it/s]\n",
      "100%|██████████| 74896/74896 [01:23<00:00, 892.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74896/74896 [00:50<00:00, 1493.40it/s]\n",
      "100%|██████████| 74896/74896 [00:00<00:00, 232644.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# create_graph_embeddings(test_data, output_name = 'test_processed_batched', batch_size = 100000, start_index = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, final_test, proteins, batch_size = 32):\n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - model:\n",
    "    - final_test:\n",
    "    - proteins:\n",
    "    - batch_size:\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities:\n",
    "    - raw_preds:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    test_preds       = []\n",
    "    raw_preds        = []\n",
    "\n",
    "    for index, batch in enumerate(final_test):\n",
    "        p_test       = proteins[index*batch_size: (index + 1) * batch_size]\n",
    "        test_results = model(batch, p_test)\n",
    "        raw_prob     = copy.copy(test_results)\n",
    "        \n",
    "        test_results = [1 if x > 0.5 else 0 for x in test_results]\n",
    "        test_preds.append(test_results)\n",
    "        raw_preds.append(raw_prob)\n",
    "\n",
    "    predictions      = []\n",
    "    for sub in test_preds:\n",
    "        for result in sub:\n",
    "            predictions.append(result)\n",
    "\n",
    "    probabilities    = []\n",
    "    for sub in raw_preds:\n",
    "        for result in sub:\n",
    "            probabilities.append(result)\n",
    "\n",
    "    \n",
    "    return probabilities, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_model_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m model_paths     \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrained_Models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# model_name      = f'{model_paths}/GCN_{class_0_size}_{class_1_size}_{timestamp}.pth'\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m model_modules   \u001b[38;5;241m=\u001b[39m \u001b[43mprint_model_summary\u001b[49m(model)\n\u001b[1;32m     63\u001b[0m model_name      \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_paths\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/GCN_Fingerprints_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_0_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_1_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     65\u001b[0m experiment_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_0_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (No Bind) and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_1_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Bind)\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'print_model_summary' is not defined"
     ]
    }
   ],
   "source": [
    "def export_results(experiment_name, module_info, model_name, json_master = 'model_experiments.json', view_only = True):\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - experiment_name\n",
    "    - module_info\n",
    "    - model_name\n",
    "    - json_master\n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    '''\n",
    "    # Could Add better Comprehension Here\n",
    "    if not os.path.exists(model_name):\n",
    "        torch.save(model, model_name)\n",
    "    else:\n",
    "        warnings.warn(f'{model_name} already exists. Model is not being saved')\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(predictions, y_test)\n",
    "    recall    = recall_score(predictions, y_test)\n",
    "    f1        = f1_score(predictions, y_test)\n",
    "    accuracy  = accuracy_score(predictions, y_test)\n",
    "    mae       = mean_absolute_error(predictions, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    metrics_dictionary = {'Precision': precision,\n",
    "                         'Recall': recall,\n",
    "                         'F1 Score:': f1,\n",
    "                         'Accuracy': accuracy,\n",
    "                         'MAE': mae}\n",
    "    \n",
    "    \n",
    "    if not view_only:\n",
    "        summary_dict                  = json.load(open(json_master, 'r'))\n",
    "        if experiment_name not in summary_dict:\n",
    "            summary_dict[experiment_name] = {'Model Metadata': module_info,\n",
    "                                            'Results': metrics_dictionary}\n",
    "\n",
    "            json.dump(summary_dict, open(json_master, 'w'), indent=4)\n",
    "        else:\n",
    "            raise ValueError('Duplicate Experiment Name. Choose a different experiment name')\n",
    "\n",
    "    return\n",
    "\n",
    "## Get the current date\n",
    "current_date    = datetime.now()\n",
    "timestamp       = current_date.strftime(\"%d-%m-%Y\")  # You can change the format as needed\n",
    "\n",
    "model_modules    = None\n",
    "class_0_size    = 25000\n",
    "class_1_size    = 2750\n",
    "model_paths     = 'Trained_Models'\n",
    "# model_name      = f'{model_paths}/GCN_{class_0_size}_{class_1_size}_{timestamp}.pth'\n",
    "\n",
    "model_modules   = print_model_summary(model)\n",
    "model_name      = f'{model_paths}/GCN_Fingerprints_{class_0_size}_{class_1_size}_{timestamp}.pth'\n",
    "\n",
    "experiment_name = f'{class_0_size} (No Bind) and {class_1_size} (Bind)'\n",
    "export_results(experiment_name, model_modules, model_name, view_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1674896it [01:33, 17967.34it/s]\n"
     ]
    }
   ],
   "source": [
    "def inference_csv(final_test, \n",
    "                  test_data,\n",
    "                  proteins,\n",
    "                  submission_name, \n",
    "                  model_name = 'GCN_15000_2576_08-05-2024.pth'):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    - final_test: DataLoader with graphs\n",
    "    - test_data: Raw CSV test file from Kaggle\n",
    "    - submission_name: Name\n",
    "    - model_name: \n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    '''\n",
    "    \n",
    "    protein_ids   = test_data['id']\n",
    "    model_path    = 'Trained_Models/' + model_name\n",
    "    model         = torch.load(model_path)\n",
    "    \n",
    "    probabilities, predictions = run_model(model, final_test, proteins, batch_size = 32)\n",
    "    \n",
    "    probabilities = [float(x.detach().numpy()) for x in probabilities]\n",
    "    probabilities = pd.DataFrame(probabilities)#.apply(lambda x: x.detach().numpy())\n",
    "    probabilities.index = protein_ids.index\n",
    "    submissions   = pd.concat([protein_ids, probabilities], axis = 1)\n",
    "    \n",
    "    submissions.columns = ['id', 'binds']\n",
    "    # Avoid overriding previous submissions\n",
    "#     submission_name     = submission_name + f'{index}'\n",
    "#     if os.path.exists(submission_name):\n",
    "#         raise ValueError(f'{submission_name}.csv already exists')\n",
    "#     else:\n",
    "#         submissions.to_csv(submission_name, index = False)\n",
    "        \n",
    "    return model, submissions\n",
    "\n",
    "\n",
    "# The size data was batched in pckl files DO NOT CHANGE\n",
    "batch_size                       = 100000\n",
    "batches                          = math.ceil(len(test_data) / batch_size)\n",
    "vocab_tokens                     = load_vocab_tokens(base_path = 'test_preprocessed')\n",
    "vocab_keys                       = list(vocab_tokens.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left off Here 19May2024**\n",
    "1. Model saved need to rebuild pipeline for morgan fingerpints\n",
    "2. Could explore the molecular graph embeddings as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 9.52 GB\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prepare_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 3: Create a new dictionary with these keys\u001b[39;00m\n\u001b[1;32m      8\u001b[0m token_subset                 \u001b[38;5;241m=\u001b[39m {key: vocab_tokens[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m subset_keys}\n\u001b[0;32m---> 10\u001b[0m final_test, proteins         \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mvocab_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model2, submissions          \u001b[38;5;241m=\u001b[39m inference_csv(final_test,\n\u001b[1;32m     16\u001b[0m                                test_data[index\u001b[38;5;241m*\u001b[39mbatch_size:(index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size],\n\u001b[1;32m     17\u001b[0m                                proteins,\n\u001b[1;32m     18\u001b[0m                                submission_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Submissions/submission_b\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m                                model_name      \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGCN_Fingerprints_25000_2750_20-05-2024.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m submission_df                \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([submission_df, submissions], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 19\u001b[0m, in \u001b[0;36mpreprocess_main\u001b[0;34m(index, vocab_tokens, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Data Batch is a list containing matricies\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     data_batch            \u001b[38;5;241m=\u001b[39m load_embeddings(base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatched Data/test_preprocessed_Batch\u001b[39m\u001b[38;5;124m'\u001b[39m, index \u001b[38;5;241m=\u001b[39m index)\n\u001b[0;32m---> 19\u001b[0m     final_test, proteins  \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m(data_batch \u001b[38;5;241m=\u001b[39m data_batch,\n\u001b[1;32m     20\u001b[0m                                      batch_size \u001b[38;5;241m=\u001b[39m batch_size, \n\u001b[1;32m     21\u001b[0m                                      index \u001b[38;5;241m=\u001b[39m index, \n\u001b[1;32m     22\u001b[0m                                      vocab_tokens \u001b[38;5;241m=\u001b[39m vocab_tokens,\n\u001b[1;32m     23\u001b[0m                                      mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     final_test    = create_gcn_data(mode = 'Evaluate', final_index = batch_size)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_test, proteins\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_data' is not defined"
     ]
    }
   ],
   "source": [
    "submission_df                    =   pd.DataFrame()\n",
    "for index in range(batches):\n",
    "    # Need to fix the logic for this no no need to open full json file everytime\n",
    "    # Implement the batching into all function calls\n",
    "    \n",
    "    subset_keys                  = vocab_keys[index*batch_size:(index + 1)*batch_size]\n",
    "    # Step 3: Create a new dictionary with these keys\n",
    "    token_subset                 = {key: vocab_tokens[key] for key in subset_keys}\n",
    "    \n",
    "    final_test, proteins         = preprocess_main(batch_size   = batch_size, \n",
    "                                                   index        = index, \n",
    "                                                   vocab_tokens = token_subset,\n",
    "                                                  )\n",
    "\n",
    "    model2, submissions          = inference_csv(final_test,\n",
    "                                   test_data[index*batch_size:(index + 1)*batch_size],\n",
    "                                   proteins,\n",
    "                                   submission_name = f'Model Submissions/submission_b{index}.csv',\n",
    "                                   model_name      = 'GCN_Fingerprints_25000_2750_20-05-2024.pth')\n",
    "    \n",
    "       \n",
    "    \n",
    "    submission_df                = pd.concat([submission_df, submissions], axis = 0)\n",
    "    \n",
    "    # Delete Variables to deal with memory\n",
    "    del subset_keys\n",
    "    del token_subset\n",
    "    del final_test\n",
    "    del proteins\n",
    "    \n",
    "    # Force Garbage Collection for Memory\n",
    "    gc.collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  binds\n",
       "0        True   True\n",
       "1        True  False\n",
       "2        True  False\n",
       "3        True   True\n",
       "4        True   True\n",
       "...       ...    ...\n",
       "1674891  True  False\n",
       "1674892  True   True\n",
       "1674893  True  False\n",
       "1674894  True   True\n",
       "1674895  True   True\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_sub = pd.read_csv('submission.csv')\n",
    "\n",
    "equal = other_sub == submission_df\n",
    "equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binds\n",
       "True     1433295\n",
       "False     241601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal['binds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       1574896\n",
       "binds    1574896\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>protein_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC1CCC(CCN)CC1</td>\n",
       "      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC1CCC(CCN)CC1</td>\n",
       "      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                              buildingblock1_smiles  \\\n",
       "0        295246830    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "1        295246831    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "2        295246832    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "3        295246833    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "4        295246834    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "...            ...                                                ...   \n",
       "1674891  296921721  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674892  296921722  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674893  296921723  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674894  296921724  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674895  296921725  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "\n",
       "        buildingblock2_smiles   buildingblock3_smiles  \\\n",
       "0              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "1              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "2              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "3              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "4              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "...                       ...                     ...   \n",
       "1674891     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n",
       "1674892     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n",
       "1674893     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "1674894     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "1674895     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "\n",
       "                                           molecule_smiles protein_name  \n",
       "0        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...         BRD4  \n",
       "1        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          HSA  \n",
       "2        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          sEH  \n",
       "3        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...         BRD4  \n",
       "4        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...          HSA  \n",
       "...                                                    ...          ...  \n",
       "1674891  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...          HSA  \n",
       "1674892  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...          sEH  \n",
       "1674893  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...         BRD4  \n",
       "1674894  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...          HSA  \n",
       "1674895  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...          sEH  \n",
       "\n",
       "[1674896 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reset    = 0\n",
    "reset_index    = []\n",
    "for index in submission_df.index:\n",
    "    if index < index_reset:\n",
    "        reset_index.append(index_reset)\n",
    "        index_reset = 0\n",
    "    else:\n",
    "        index_reset = index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[199999,\n",
       " 299999,\n",
       " 399999,\n",
       " 499999,\n",
       " 599999,\n",
       " 699999,\n",
       " 799999,\n",
       " 899999,\n",
       " 999999,\n",
       " 1099999,\n",
       " 1199999,\n",
       " 1299999,\n",
       " 1399999,\n",
       " 1499999,\n",
       " 1599999,\n",
       " 1674895]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830.0</td>\n",
       "      <td>0.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831.0</td>\n",
       "      <td>0.913789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832.0</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833.0</td>\n",
       "      <td>0.812792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834.0</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74891</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74892</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.985396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74894</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.975440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74895</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3249792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     binds\n",
       "0      295246830.0  0.947388\n",
       "1      295246831.0  0.913789\n",
       "2      295246832.0  0.927355\n",
       "3      295246833.0  0.812792\n",
       "4      295246834.0  0.718755\n",
       "...            ...       ...\n",
       "74891          NaN  0.956296\n",
       "74892          NaN  0.963440\n",
       "74893          NaN  0.985396\n",
       "74894          NaN  0.975440\n",
       "74895          NaN  0.979522\n",
       "\n",
       "[3249792 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830.0</td>\n",
       "      <td>0.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831.0</td>\n",
       "      <td>0.913789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832.0</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833.0</td>\n",
       "      <td>0.812792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834.0</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721.0</td>\n",
       "      <td>0.956296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722.0</td>\n",
       "      <td>0.963440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723.0</td>\n",
       "      <td>0.985396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724.0</td>\n",
       "      <td>0.975440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725.0</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     binds\n",
       "0        295246830.0  0.947388\n",
       "1        295246831.0  0.913789\n",
       "2        295246832.0  0.927355\n",
       "3        295246833.0  0.812792\n",
       "4        295246834.0  0.718755\n",
       "...              ...       ...\n",
       "1674891  296921721.0  0.956296\n",
       "1674892  296921722.0  0.963440\n",
       "1674893  296921723.0  0.985396\n",
       "1674894  296921724.0  0.975440\n",
       "1674895  296921725.0  0.979522\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
