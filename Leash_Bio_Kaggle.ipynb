{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchsummary, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 torchsummary-1.5.1 transformers-4.42.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2024.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (10.2.0)\n",
      "Downloading rdkit-2024.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2024.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d2l\n",
      "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (1.0.0)\n",
      "Collecting numpy==1.23.5 (from d2l)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting matplotlib==3.7.2 (from d2l)\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (0.1.6)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (2.31.0)\n",
      "Collecting pandas==2.0.3 (from d2l)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy==1.10.1 (from d2l)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: notebook in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.1.2)\n",
      "Requirement already satisfied: qtconsole in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.16.3)\n",
      "Requirement already satisfied: ipykernel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.29.3)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (10.2.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->d2l)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (2.9.0)\n",
      "Requirement already satisfied: traitlets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib-inline==0.1.6->d2l) (5.14.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.22.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (6.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.42)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.17.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.13.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.25.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (4.1.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->d2l) (4.2.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.5.3)\n",
      "Requirement already satisfied: overrides in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
      "Requirement already satisfied: websocket-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.2.4)\n",
      "Requirement already satisfied: tomli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.9.24)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (4.21.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.18.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.4)\n",
      "Requirement already satisfied: uri-template in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.9.0.20240316)\n",
      "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, numpy, scipy, pandas, matplotlib, d2l\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.12.0\n",
      "    Uninstalling scipy-1.12.0:\n",
      "      Successfully uninstalled scipy-1.12.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.0.3 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed d2l-1.0.3 matplotlib-3.7.2 numpy-1.23.5 pandas-2.0.3 pyparsing-3.0.9 scipy-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.1.3)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (5.9.8)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: multidict, h5py, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 h5py-3.11.0 multidict-6.0.5 torch-geometric-2.5.3 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tape-proteins\n",
      "  Downloading tape_proteins-0.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (4.66.2)\n",
      "Collecting tensorboardX (from tape-proteins)\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (1.10.1)\n",
      "Collecting lmdb (from tape-proteins)\n",
      "  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (1.34.101)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (2.31.0)\n",
      "Collecting biopython (from tape-proteins)\n",
      "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tape-proteins) (3.13.3)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from biopython->tape-proteins) (1.23.5)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.101 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3->tape-proteins) (1.34.101)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3->tape-proteins) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3->tape-proteins) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->tape-proteins) (2024.2.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX->tape-proteins) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboardX->tape-proteins) (4.25.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.101->boto3->tape-proteins) (2.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging->tensorboardX->tape-proteins) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.101->boto3->tape-proteins) (1.16.0)\n",
      "Downloading tape_proteins-0.5-py3-none-any.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m890.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lmdb, biopython, tensorboardX, tape-proteins\n",
      "Successfully installed biopython-1.84 lmdb-1.5.1 tape-proteins-0.5 tensorboardX-2.6.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tape-proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepchem\n",
      "  Downloading deepchem-2.8.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.4.1.post1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.12)\n",
      "Requirement already satisfied: scipy>=1.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (1.10.1)\n",
      "Requirement already satisfied: rdkit in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepchem) (2024.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->deepchem) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->deepchem) (2024.1)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit->deepchem) (10.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.4.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->deepchem) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
      "Downloading deepchem-2.8.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: deepchem\n",
      "Successfully installed deepchem-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2024.7.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dask-ml\n",
      "  Downloading dask_ml-2024.4.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: click>=8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (21.3)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (6.0.1)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask) (6.11.0)\n",
      "Collecting dask-glm>=0.2.0 (from dask-ml)\n",
      "  Downloading dask_glm-0.3.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting distributed>=2.4.0 (from dask-ml)\n",
      "  Downloading distributed-2024.7.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting multipledispatch>=0.4.9 (from dask-ml)\n",
      "  Downloading multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (0.59.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-ml) (1.10.1)\n",
      "Collecting sparse>=0.7.0 (from dask-glm>=0.2.0->dask-ml)\n",
      "  Downloading sparse-0.15.4-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[array,dataframe]>=2.4.0->dask-ml)\n",
      "  Downloading dask_expr-1.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (3.1.3)\n",
      "Collecting locket>=1.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting msgpack>=1.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: psutil>=5.7.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (5.9.8)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (6.4)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.2.1)\n",
      "Collecting zict>=3.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask) (3.17.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from numba>=0.51.0->dask-ml) (0.42.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->dask) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (3.4.0)\n",
      "Requirement already satisfied: pyarrow>=7.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dask-expr<1.2,>=1.1->dask[array,dataframe]>=2.4.0->dask-ml) (15.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->dask-ml) (1.16.0)\n",
      "Downloading dask-2024.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dask_ml-2024.4.4-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.8/149.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dask_glm-0.3.2-py2.py3-none-any.whl (13 kB)\n",
      "Downloading distributed-2024.7.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m621.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dask_expr-1.1.7-py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.1/385.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading sparse-0.15.4-py2.py3-none-any.whl (237 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.3/237.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, multipledispatch, zict, toolz, msgpack, locket, sparse, partd, dask, distributed, dask-expr, dask-glm, dask-ml\n",
      "Successfully installed dask-2024.7.0 dask-expr-1.1.7 dask-glm-0.3.2 dask-ml-2024.4.4 distributed-2024.7.0 locket-1.0.0 msgpack-1.0.8 multipledispatch-1.0.0 partd-1.4.2 sortedcontainers-2.4.0 sparse-0.15.4 toolz-0.12.1 zict-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.0-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.10.1)\n",
      "Downloading xgboost-2.1.0-py3-none-manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckdb\n",
      "  Downloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)\n",
      "Downloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: duckdb\n",
      "Successfully installed duckdb-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (2.0.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: plotly in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (5.20.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (10.2.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from plotly->catboost) (8.2.3)\n",
      "Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.5 graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "--46iLsBsl_W"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from   d2l import torch as d2l\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from   torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from   transformers import AutoTokenizer\n",
    "from   torch.nn.utils.rnn import pad_sequence\n",
    "from   transformers import AutoModel, AutoTokenizer\n",
    "from   torch import nn\n",
    "\n",
    "from torch_geometric.nn import GATConv, GCNConv, global_mean_pool\n",
    "from torch.nn import LazyLinear, Transformer, Conv1d, Dropout, BatchNorm1d\n",
    "import torch.nn.utils as utils\n",
    "from torch.nn import init\n",
    "\n",
    "from torch.nn import Embedding\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader  # Correct import\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "from tape import ProteinBertModel, TAPETokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "import h5py\n",
    "import ast\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "import psutil\n",
    "\n",
    "# Self Created Code\n",
    "from JunctionTree.main    import Datapreprocess\n",
    "from leash_utils.utils    import create_protein_embeddings, open_train_test, create_graph_embeddings, get_memory_usage\n",
    "from leash_utils.utils    import load_embeddings, load_vocab_tokens, preprocess_build, _prepare_data\n",
    "from leash_utils.models   import Model1, Model2, MorganModel, ResModel, SimpleNN\n",
    "from leash_utils.s3_utils import load_s3_files\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import deepchem as dc\n",
    "from deepchem.feat import MolGraphConvFeaturizer\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import dask_ml\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "# from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from xgboost.dask import DaskDMatrix, train, predict\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left off trying to get test data in same format at train data 06/Jul/2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zVJ7PakpNTnx"
   },
   "outputs": [],
   "source": [
    "# Load with m5.12xlarge instance to load all the data into memory\n",
    "# dataframes = load_s3_files()\n",
    "\n",
    "def load_raw_data(duck_db = False, test_only = True, protein_encoding = 'one_hot', class0_size = 4000000, class1_size = 1589906):\n",
    "    ''' \n",
    "    Loads the raw data or a subset from the Kaggle BELKA competition\n",
    "    Website: https://www.kaggle.com/competitions/leash-BELKA/data\n",
    "    if test_only is True, protein_encoding is the only other variable that the user should define\n",
    "    \n",
    "    Parameters:\n",
    "        duck_db (bool): load parquet file using duck_db package\n",
    "        test_only (bool): whether to load only the test data\n",
    "        protein_encoding (str): embedding type for the proteins (one_hot, Rostlab/prot_bert, facebook/esm2_t33_650M_UR50D, ProteinBert)\n",
    "        class0_size (int): the number of non binding examples to pull from the parquet files\n",
    "        class1_size(int): the number of binding examples to \n",
    "    \n",
    "    Returns:\n",
    "        train_data (pd.DataFrame): raw training data\n",
    "        test_data (pd.DataFrame): raw test data\n",
    "        embedded_sequences (dict): dictionary containing protein embeddings \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    device                = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_data             = pickle.load(open('test_df.pckl', 'rb'))\n",
    "\n",
    "    if protein_encoding   == 'one_hot':\n",
    "        embedded_sequences    = {'sEH': 0, 'BRD4': 1, 'HSA': 2}\n",
    "    else:\n",
    "        embedded_sequences    = create_protein_embeddings(model_name = protein_encoding)\n",
    "    \n",
    "    if duck_db:\n",
    "        print(f'Loading data of size {class_0_size + class1_size} may take a long time')\n",
    "        \n",
    "        train_path    = 'train.parquet'\n",
    "        con           = duckdb.connect()\n",
    "        train_data    = con.query(f\"\"\"(SELECT *\n",
    "                                FROM parquet_scan('{train_path}')\n",
    "                                WHERE binds = 0\n",
    "                                ORDER BY random()\n",
    "                                LIMIT {class0_size})\n",
    "                                UNION ALL\n",
    "                                (SELECT *\n",
    "                                FROM parquet_scan('{train_path}')\n",
    "                                WHERE binds = 1\n",
    "                                ORDER BY random()\n",
    "                                LIMIT {class1_size})\"\"\").df()\n",
    "\n",
    "        con.close()\n",
    "\n",
    "    else:\n",
    "        test_data         = pickle.load(open('test_df.pckl', 'rb'))\n",
    "        if test_only:\n",
    "            train_data    = None\n",
    "        else:\n",
    "            train_data    = pd.read_csv('5.5million_train.csv')\n",
    "            \n",
    "            \n",
    "    return train_data, test_data, embedded_sequences\n",
    "\n",
    "train_data, test_data, embedded_sequences = load_raw_data(duck_db = False, test_only = True, protein_encoding = 'one_hot')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_block_embeddings(load_existing_subset = True, train_data = None, block1 = None, block2 = None, block3 = None):\n",
    "    '''\n",
    "    A function that loads smiles blocks embeddings or processes no block embeddings \n",
    "    \n",
    "    Parameters:\n",
    "        load_existing_subset (bool): load the 5.5million subset blocks dict or not\n",
    "        train_data (pd.DataFrame): the raw train_data from load_raw_data() method\n",
    "        block1 (str): the name of pickle serialized block1\n",
    "        block2 (str): the name of pickle serialized block2\n",
    "        block3 (str): the name of pickle serialized block3\n",
    "            \n",
    "    Returns:\n",
    "        block_1_dict (dict): block1 embeddings\n",
    "        block_2_dict (dict): block2 embeddings\n",
    "        block_3_dict (dict): block3 embeddings\n",
    "        \n",
    "    '''\n",
    "    block_none = None in [block1, block2, block3]\n",
    "    \n",
    "    if load_existing_subset and not block_none:\n",
    "        # Avoid loading train_data (5.5 million datapoints)\n",
    "        block_1_dict = pickle.load(open(block1, 'rb'))\n",
    "        block_2_dict = pickle.load(open(block2, 'rb'))\n",
    "        block_3_dict = pickle.load(open(block3, 'rb'))\n",
    "    \n",
    "    elif not load_existing_subset and train_data is not None:\n",
    "#       Reserve 0 token for any building blocks not in the training data\n",
    "        block_1_dict  = {smile: index+1 for index, smile in enumerate(train_data['buildingblock1_smiles'].unique())}\n",
    "        block_2_dict  = {smile: index+1 for index, smile in enumerate(train_data['buildingblock2_smiles'].unique())}\n",
    "        block_3_dict  = {smile: index+1 for index, smile in enumerate(train_data['buildingblock3_smiles'].unique())}\n",
    "    \n",
    "    \n",
    "    return block_1_dict, block_2_dict, block_3_dict\n",
    "\n",
    "\n",
    "block_1_dict, block_2_dict, block_3_dict = train_block_embeddings(load_existing_subset = True, \n",
    "                                                                  train_data = train_data, \n",
    "                                                                  block1 = 'block_1_dict.pckl',\n",
    "                                                                  block2 = 'block_2_dict.pckl', \n",
    "                                                                  block3 = 'block_3_dict.pckl')\n",
    "\n",
    "def safe_conversion(x, dictionary):\n",
    "    '''\n",
    "    Used for test data to encode blocks that weren't in the training set\n",
    "    \n",
    "    Parameters:\n",
    "        x (str): building block token\n",
    "        dictionary (dict): buidling block dictionary for token conversion\n",
    "        \n",
    "    Returns:\n",
    "        x (int): coverted block token\n",
    "        \n",
    "    '''\n",
    "    try:\n",
    "        x = dictionary[x]\n",
    "    except KeyError:\n",
    "        x = 0\n",
    "        \n",
    "    return x\n",
    "        \n",
    "def build_smiles_blocks(data, print_metrics = False):\n",
    "    '''\n",
    "    Apply the smiles blocks_dictionaries to train_data\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): raw training or test data\n",
    "    \n",
    "    Returns:\n",
    "        x_features (np.array): tokenized building blocks\n",
    "        targets (np.array): tokenized binding class\n",
    "    '''\n",
    "    smiles_blocks = ['buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles']\n",
    "    \n",
    "    try:\n",
    "        targets       = data['binds'].values   \n",
    "    except KeyError:\n",
    "        targets       = None\n",
    "        \n",
    "    x_features    = data[smiles_blocks]\n",
    "    x_features['buildingblock1_smiles'] = x_features['buildingblock1_smiles'].apply(lambda x: safe_conversion(x, block_1_dict))\n",
    "    x_features['buildingblock2_smiles'] = x_features['buildingblock2_smiles'].apply(lambda x: safe_conversion(x, block_2_dict))\n",
    "    x_features['buildingblock3_smiles'] = x_features['buildingblock3_smiles'].apply(lambda x: safe_conversion(x, block_3_dict))\n",
    "    x_features    = x_features.values\n",
    "    \n",
    "    # print metrics for test data to see missing building blocks\n",
    "    if print_metrics:\n",
    "        \n",
    "        print('Total Test Length: ', len(test_data))\n",
    "        print('Missing in block1: ', block1_0)\n",
    "        print('Missing in block2: ', block2_0)\n",
    "        print('Missing in block3; ', block3_0)\n",
    "    return x_features, targets\n",
    "\n",
    "# x_features, targets = build_smiles(train_data)\n",
    "\n",
    "# smiles_blocks, _ = build_smiles_blocks(test_data, print_metrics = True)\n",
    "\n",
    "def batch_smiles_blocks(x_features, targets):\n",
    "    '''\n",
    "    This function is required because the Morgan Fingerprint embeddings\n",
    "    were processed in batches. Thus, the smiles blocks need to processed\n",
    "    in the same way to make the data points line correctly. \n",
    "    \n",
    "    Paramters:\n",
    "        x_features (np.array) : tokenized x features\n",
    "        targets (np.array): target binds 0 or 1\n",
    "    \n",
    "    Returns:\n",
    "        smile_train (np.array): final encoded training smiles blocks\n",
    "        smile_test (np.array): final encoded testings smiles blocks\n",
    "    \n",
    "    '''\n",
    "    smile_train   = {}\n",
    "    smile_test    = {}\n",
    "\n",
    "    batch_size    = 500000\n",
    "    batches       = math.ceil(len(x_features) / batch_size)\n",
    "    \n",
    "    # Process Data in Batches\n",
    "    for batch in tqdm(range(batches), total = batches):\n",
    "        try:\n",
    "            x_batch              = x_features[batch * batch_size:(batch+1)*batch_size]\n",
    "            y_batch              = targets[batch*batch_size:(batch+1)*batch_size]\n",
    "            \n",
    "            # Targets not needed\n",
    "            x_tr, x_te, _, _     = train_test_split(x_batch, y_batch, stratify = y_batch, random_state = 42)\n",
    "\n",
    "            smile_train[batch]   = x_tr\n",
    "            smile_test[batch]    = x_te\n",
    "        \n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    smile_train = np.concatenate([arr for arr in smile_train.values()], axis = 0)\n",
    "    smile_test  = np.concatenate([arr for arr in smile_test.values()], axis = 0)\n",
    "    return smile_train, smile_test\n",
    "\n",
    "# smiles_train, smiles_test = batch_smiles_blocks(x_features, targets)\n",
    "\n",
    "def load_smiles_batch(smile_train, smile_test):\n",
    "    '''\n",
    "    Load the one hot encoded smiles building blocks from the\n",
    "    buildingblock[1-3]_smiles columns in train_data\n",
    "    \n",
    "    Parameters:\n",
    "        smile_train (str): file path of the pickle serialized train smiles\n",
    "        smile_test (str): file path of the pickle serialized test smiles\n",
    "    \n",
    "    Returns:\n",
    "        smiles_train (np.array): array of train smiles for model building\n",
    "        smiles_test (np.arary): array of test smiles for model evaluation\n",
    "    \n",
    "    '''\n",
    "    smiles_train = pickle.load(open(smile_train, 'rb'))\n",
    "    smiles_test  = pickle.load(open(smile_test, 'rb'))\n",
    "\n",
    "    return smiles_train, smiles_test\n",
    "\n",
    "smiles_train, smiles_test = load_smiles_batch('smile_train.pckl', 'smile_test.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_metadata(load_test = False, train_data = None, test_data = None):\n",
    "    '''\n",
    "    Load the protein metadta dictionary or the tokenized array containing metadata\n",
    "    \n",
    "    Parameters:\n",
    "        load_test (bool): load test data or load the metadata dictionary\n",
    "        \n",
    "    Returns:\n",
    "        protein metadata (tuple or dict): Either the dictionary or the array containing \n",
    "                                          protein metadata\n",
    "    '''\n",
    "    \n",
    "    train_test_exists = train_data is not None and test_data is not None\n",
    "    \n",
    "    # Taken from protparam after feeding amino acid sequences\n",
    "    if not load_test and train_test_exists:\n",
    "        protein_metadata = {'sEH': {'Molecular Weight': 62615.75, \n",
    "                                    'Theoretical pI': 5.91,\n",
    "                                    'Negatively Charged Residues': 67,\n",
    "                                    'Positively Charged Residues': 60,\n",
    "                                    'Aliphatic Index': 87.15,\n",
    "                                    'GRAVY': -0.135}, \n",
    "                           'BRD4': {'Molecular Weight': 152218.76, \n",
    "                                    'Theoretical pI': 9.23,\n",
    "                                    'Negatively Charged Residues': 150,\n",
    "                                    'Positively Charged Residues': 175,\n",
    "                                    'Aliphatic Index': 56.03,\n",
    "                                    'GRAVY': -1.081},\n",
    "                            'HSA': {'Molecular Weight': 69366.68, \n",
    "                                    'Theoretical pI': 5.92,\n",
    "                                    'Negatively Charged Residues': 98,\n",
    "                                    'Positively Charged Residues': 87,\n",
    "                                    'Aliphatic Index': 77.57,\n",
    "                                    'GRAVY': -0.354}\n",
    "                                   }\n",
    "    elif load_test:\n",
    "        test_metadata    = np.load('test_protparm_metadata.npz')['test_protparm_metadata']\n",
    "        train_metadata   = np.load('train_protparm_metadata.npz')['train_protparm_metadata']\n",
    "        protein_metadata = (train_metadata, test_metadata)\n",
    "        \n",
    "    return protein_metadata\n",
    "\n",
    "# protein_metadata = get_protein_metadata(amino_data = False, load_test = True)\n",
    "train_metadata, test_metadata = get_protein_metadata(load_test = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write code to check for exploding gradients** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):\n",
    "    '''The base class for training nueral network models with'''\n",
    "    def __init__(self, \n",
    "                 max_epochs,\n",
    "                 bypass_prepare = False,\n",
    "                 num_gpus=0, \n",
    "                 gradient_clip_val=0, \n",
    "                 batch_size = 32):\n",
    "        \n",
    "        '''\n",
    "        Parameters:\n",
    "            max_epochs (int): training epochs to apply at training\n",
    "            bypass_prepare (bool): bypass DataLoader object creation\n",
    "            num_gpus (int): num_gpus for training\n",
    "            gardient_clip_val (float): value to apply gradient clipping to nueral network weights\n",
    "            batch_size (int): batch_size used at training\n",
    "        '''\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.bypass_prepare     = bypass_prepare\n",
    "        self.max_epochs         = max_epochs\n",
    "        self.batch_size         = batch_size\n",
    "        # assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, train_data, val_data):\n",
    "        '''\n",
    "        Convert data to dataloader objects\n",
    "        \n",
    "        Parameters:\n",
    "            train_data (np.array): Training data for model weights\n",
    "            val_data (np.array): Validation data for training evaluation\n",
    "        \n",
    "        Returns:\n",
    "            train_dataloader (torch DataLoader): DataLoader training for efficient loading\n",
    "            val_datalaoder (torch DataLoader or None): DataLoader validation for efficient loading\n",
    "        '''\n",
    "        # Convert MACCS Keys to tensor\n",
    "        if isinstance(train_data, list):\n",
    "            print(type(train_data[0]))\n",
    "            train_data[0]      = torch.from_numpy(train_data[0])\n",
    "            train_data         = TensorDataset(train_data[0], train_data[1])\n",
    "            \n",
    "        train_dataloader       = DataLoader(train_data, batch_size = self.batch_size)\n",
    "        if val_data is not None:\n",
    "            val_dataloader     = DataLoader(val_data, batch_size = self.batch_size)\n",
    "        else:\n",
    "            val_dataloader     = None\n",
    "            \n",
    "        self.num_train_batches = len(train_dataloader)\n",
    "        self.num_val_batches   = (len(val_dataloader)\n",
    "                                if val_dataloader is not None else 0)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            model,\n",
    "            train_data, \n",
    "            other_data   = None,\n",
    "            val_data     = None, \n",
    "            optimizer    = None, \n",
    "            loss_fn      = None, \n",
    "            lr_scheduler = None):\n",
    "        \n",
    "        '''\n",
    "        The main training function for a nueral network \n",
    "        \n",
    "        Parameters:\n",
    "            model (torch.nn.Module):\n",
    "            train_data (np.array):\n",
    "            val_data (np.array):\n",
    "            optimizer (torch.optim.Optimizer):\n",
    "            loss_fn (torch):\n",
    "            lr_scheduler:\n",
    "            \n",
    "        Returns\n",
    "            None:\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        self.model           = model\n",
    "        self.optimizer       = optimizer\n",
    "        self.loss_fn         = loss_fn\n",
    "        self.lr_scheduler    = lr_scheduler\n",
    "        \n",
    "        # Bass other data (Placeholder for now 05May2024)\n",
    "        if other_data is not None:\n",
    "            self.supplement_data = other_data\n",
    "   \n",
    "        if not self.bypass_prepare:\n",
    "            self.train_dataloader, self.val_dataloader = self.prepare_data(train_data, val_data)\n",
    "            loader_type      = None\n",
    "            \n",
    "        else:\n",
    "            self.train_dataloader, self.val_dataloader = train_data, val_data\n",
    "            loader_type  = 'Graph'\n",
    "        \n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx   = 0\n",
    "        for self.epoch in tqdm(range(self.max_epochs), total = self.max_epochs):\n",
    "            self.fit_epoch(loader_type = loader_type, p_data = self.supplement_data)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                loss  = self.loss.to('cpu').item()\n",
    "                print(f'Epoch {self.epoch + 1} LOSS: {loss}')\n",
    "            else:\n",
    "                print(f'Epoch {self.epoch + 1} LOSS: {self.loss.detach().numpy()}')\n",
    "            \n",
    "    def fit_epoch(self, loader_type = 'Graph', p_data = None, gradients = True):\n",
    "        \n",
    "        if loader_type == 'Graph':\n",
    "            for index, data in enumerate(self.train_dataloader):\n",
    "                \n",
    "                p_batch     = p_data[index*self.batch_size:(index + 1)*self.batch_size]\n",
    "#                 p_batch     = protein_real_time(p_batch)\n",
    "                if isinstance(p_batch, list):\n",
    "                    p_batch = torch.concatenate([p for p in p_batch], axis = 0)\n",
    "                \n",
    "                print(data)\n",
    "                targets     = data.y.unsqueeze(1).float()\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    p_batch  = p_batch.to(device)\n",
    "                    data     = data.to(device)\n",
    "                    targets  = targets.to(device)\n",
    "                    \n",
    "                predictions = self.model.forward(data, p_batch)\n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                self.loss.backward()\n",
    "                self.optimizer.step()\n",
    "     \n",
    "                \n",
    "        if loader_type != 'Graph':\n",
    "            for index, data in tqdm(enumerate(self.train_dataloader), total = 32753, desc=\"Training\"):\n",
    "                targets     = data[1].float()\n",
    "                features    = data[0].float()\n",
    "                \n",
    "                if len(list(targets.shape)) < 2:\n",
    "                    targets     = targets.unsqueeze(1).float()\n",
    "                p_batch     = p_data[index*self.batch_size:(index + 1)*self.batch_size]\n",
    "                \n",
    "#                 # Real time batch processing\n",
    "#                 print(p_batch)\n",
    "                p_batch     = torch.tensor(p_batch, dtype = torch.float32)\n",
    "    \n",
    "                # Code for protein batches for protein embeddings\n",
    "#                 p_batch     = protein_real_time(p_batch)\n",
    "                \n",
    "                if device.type == 'cuda':\n",
    "                    p_batch  = p_batch.to(device)\n",
    "                    features = features.to(device)\n",
    "                    targets  = targets.to(device)\n",
    "                    \n",
    "                    \n",
    "                predictions = self.model.forward(features, p_batch) \n",
    "                \n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                self.loss.backward()\n",
    "                \n",
    "            \n",
    "                self.optimizer.step()\n",
    "#                 self.lr_scheduler.step()\n",
    "                # Left off here trying to fix the generalizatin of trainer 05May2024\n",
    "                if self.val_dataloader is not None:\n",
    "                  with torch.no_grad():\n",
    "                    for val_features, val_targets in self.val_dataloader:\n",
    "                      val_predictions = model.forward(val_features)\n",
    "                      val_loss        = self.loss_fn(val_predictions, val_targets)\n",
    "\n",
    "                      print(f'Epoch {self.epoch} w/ Loss: ', val_loss)\n",
    "                \n",
    "                # Make sure GPU does not get overloaded with unnecessary_memory\n",
    "                del p_batch, features, targets\n",
    "                \n",
    "                if index % 100 == 0:\n",
    "                    gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Gradients Are Vanishing Confirmed 18May2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Train data is of type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 79\u001b[0m\n\u001b[1;32m     68\u001b[0m         final_train, final_test, x_train_p, x_test_p, y_train, y_test \u001b[38;5;241m=\u001b[39m preprocess_build(train_data, \n\u001b[1;32m     69\u001b[0m                                                                     embedded_sequences,\n\u001b[1;32m     70\u001b[0m                                                                     batch_size     \u001b[38;5;241m=\u001b[39m class_0_size, \n\u001b[1;32m     71\u001b[0m                                                                     batch_loading  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m                                                                     embedding_type \u001b[38;5;241m=\u001b[39m chemical_embedding,\n\u001b[1;32m     73\u001b[0m                                                                     protein_dtype \u001b[38;5;241m=\u001b[39m protein_dtype)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_train, final_test, x_train_p, x_test_p, y_train, y_test\n\u001b[0;32m---> 79\u001b[0m final_train, final_test, x_train_p, x_test_p, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                                                                                   \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                                                                   \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                                                                                   \u001b[49m\u001b[43mclass_0_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                                                                                   \u001b[49m\u001b[43mchemical_embedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmorgan_fingerprints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                                                                                   \u001b[49m\u001b[43mprotein_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumpy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mprocess_split_data\u001b[0;34m(batch_data, batch_size, train_data, class_0_size, chemical_embedding, protein_dtype)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train data was not loaded in load_raw_data() method\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain data is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Process data in batches\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_data \u001b[38;5;129;01mand\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Initialize dictionary for batch data\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Train data is of type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "def process_split_data(batch_data = False, \n",
    "                       batch_size = None,\n",
    "                       train_data = None,\n",
    "                       class_0_size = 3000000, \n",
    "                       chemical_embedding = 'morgan_fingerprints',\n",
    "                       protein_dtype = 'numpy'):\n",
    "    ''' \n",
    "    Process the data and split data into train and test splits\n",
    "    \n",
    "    Parameters:\n",
    "        batch_data (bool): \n",
    "        batches (int):\n",
    "        class_0_size (int):\n",
    "        chemical_embedding (str): # DeepChem, self-made, MACCS, morgan_fingerprints\n",
    "        protein_dtype (str):\n",
    "    \n",
    "    double check this returns (08Jul2024)\n",
    "    Returns:\n",
    "        final_train (np.array): final train data\n",
    "        final_test (np.array): final test data\n",
    "        x_train_p (np.array or torch.tensor): protein train data\n",
    "        x_test_p (np.array or torch.tensor): protein test data\n",
    "        y_train (torch.tensor): y train data\n",
    "        y_test (torch.tensor):  y test data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Train data was not loaded in load_raw_data() method\n",
    "    if train_data is None:\n",
    "        raise ValueError(f'Train data is of type {type(train_data)}')\n",
    "    \n",
    "    # Process data in batches\n",
    "    if batch_data and batch_size is not None:\n",
    "        \n",
    "        # Initialize dictionary for batch data\n",
    "        final_train = {}\n",
    "        final_test  = {}\n",
    "        x_train_p   = {}\n",
    "        x_test_p    = {}\n",
    "        y_train     = {}\n",
    "        y_test      = {}\n",
    "        batches     = math.ceil(len(train_data.index) / batch_size)\n",
    "        \n",
    "        for batch in tqdm(range(batches), total = batches):\n",
    "            try:\n",
    "                t_batch   = train_data[batch * batch_size:(batch + 1)*batch_size]\n",
    "\n",
    "                f_train, f_test, x_p_train, x_p_test, y_tr, y_te      = preprocess_build(t_batch, \n",
    "                                                                        embedded_sequences,\n",
    "                                                                        batch_size     = class_0_size, \n",
    "                                                                        batch_loading  = False,\n",
    "                                                                        embedding_type = chemical_embedding,\n",
    "                                                                        protein_dtype = protein_dtype)\n",
    "                final_train[batch] = f_train\n",
    "                final_test[batch]  = f_test\n",
    "                x_train_p[batch]   = x_p_train\n",
    "                x_test_p[batch]    = x_p_test\n",
    "                y_train[batch]     = y_tr\n",
    "                y_test[batch]      = y_te\n",
    "\n",
    "                del f_train, f_test, x_p_train, x_p_test, y_tr, y_te\n",
    "                gc.collect()\n",
    "                get_memory_usage()\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "    else:\n",
    "        final_train, final_test, x_train_p, x_test_p, y_train, y_test = preprocess_build(train_data, \n",
    "                                                                    embedded_sequences,\n",
    "                                                                    batch_size     = class_0_size, \n",
    "                                                                    batch_loading  = False,\n",
    "                                                                    embedding_type = chemical_embedding,\n",
    "                                                                    protein_dtype = protein_dtype)\n",
    "    \n",
    "    \n",
    "    return final_train, final_test, x_train_p, x_test_p, y_train, y_test\n",
    "\n",
    "\n",
    "final_train, final_test, x_train_p, x_test_p, y_train, y_test = process_split_data(batch_size = 500000, \n",
    "                                                                                   batch_data = True,\n",
    "                                                                                   train_data = train_data,\n",
    "                                                                                   class_0_size = 3000000,\n",
    "                                                                                   chemical_embedding = 'morgan_fingerprints',\n",
    "                                                                                   protein_dtype = 'numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loaded\n",
      "test_loaded\n"
     ]
    }
   ],
   "source": [
    "def load_processed_batches(test_only       = False, \n",
    "                           include_smiles  = True,\n",
    "                           final_train     = 'final_train.npz',\n",
    "                           final_test      = 'final_test.npz',\n",
    "                           x_train_protein = 'x_train_p.pckl',\n",
    "                           x_test_protein  = 'x_test_p.pckl',\n",
    "                           y_train         = 'y_train.pckl',\n",
    "                           y_test          = 'y_test.pckl'):\n",
    "    '''\n",
    "    Load the processed batches for the 5.5million data point subset\n",
    "    \n",
    "    Parameters:\n",
    "        test_only (bool): whether to include the test_only \n",
    "        include_smiles (bool): whether to include the smiles in train and test\n",
    "        final_train (str): path for the final train data\n",
    "        final_test (str): path for the final tet data\n",
    "        x_train_protein(str): path for the pickle serialized protein train data\n",
    "        x_test_protein (str): path for the pickle serialized protein test data\n",
    "        y_train (str): path for the pickle serialized y train data\n",
    "        y_test (str): path for the pickle serialized y test data\n",
    "        \n",
    "    Returns:\n",
    "        final_train (np.array): numpy array containing final train data\n",
    "        final_test (np.array): numpy array containing final test data\n",
    "        x_train_pcat (list): list containing final protein train data\n",
    "        x_test_pcat (list): list containing final protein test data\n",
    "        y_train (torch.tensor): tensor containing the final y train data\n",
    "        y_test (torch.tensor): tensor contaiing the fianl y test data\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    if not test_only:\n",
    "        final_train  = np.load(final_train)\n",
    "        final_train  = {key: value for key, value in final_train.items()}\n",
    "        final_train  = np.concatenate([final_train[index] for index in final_train.keys()])\n",
    "        print('train_loaded')\n",
    "    else:\n",
    "        final_train  = None\n",
    "    \n",
    "    final_test   = np.load(final_test)\n",
    "    final_test   = {key: value for key, value in final_test.items()}\n",
    "    final_test   = np.concatenate([final_test[index] for index in final_test.keys()])\n",
    "    print('test_loaded')\n",
    "    x_train_p    = pickle.load(open(x_train_protein, 'rb'))\n",
    "    x_test_p     = pickle.load(open(x_test_protein, 'rb'))\n",
    "    y_train      = pickle.load(open(y_train, 'rb'))\n",
    "    y_test       = pickle.load(open(y_test, 'rb'))\n",
    "    \n",
    "    y_train      = torch.concatenate([y_train[index] for index in y_train.keys()], axis = 0)\n",
    "    y_test       = torch.concatenate([y_test[index] for index in y_test.keys()], axis = 0)\n",
    "    \n",
    "    x_train_pcat = []\n",
    "    x_test_pcat  = []\n",
    "    for key in x_train_p.keys():\n",
    "        x_train_pcat += x_train_p[key]\n",
    "        x_test_pcat  += x_test_p[key]\n",
    "        \n",
    "    if include_smiles:\n",
    "        # Pre-allocate since Concatenating arrays is very memory intensive\n",
    "        train_concatenated = np.empty((final_train.shape[0], final_train.shape[1] + smiles_train.shape[1]), dtype=np.int16)\n",
    "\n",
    "        train_concatenated[:, :final_train.shape[1]] = final_train\n",
    "        train_concatenated[:, final_train.shape[1]:] = smiles_train\n",
    "        \n",
    "        test_concatenated  = np.empty((final_test.shape[0], final_test.shape[1] + smiles_test.shape[1]), dtype = np.int16)\n",
    "        \n",
    "        test_concatenated[:, :final_test.shape[1]]   = final_test\n",
    "        test_concatenated[:, final_test.shape[1]:]   = smiles_test\n",
    "\n",
    "        del final_test, final_train\n",
    "        gc.collect()\n",
    "        \n",
    "        final_train  = train_concatenated\n",
    "        final_test   = test_concatenated\n",
    "        \n",
    "        del train_concatenated, test_concatenated\n",
    "        gc.collect()\n",
    "        get_memory_usage()\n",
    "        \n",
    "\n",
    "    return final_train, final_test, x_train_pcat, x_test_pcat, y_train, y_test\n",
    "\n",
    "final_train, final_test, x_train_p, x_test_p, y_train, y_test = load_processed_batches(test_only = False, include_smiles = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_smiles(final_train, smiles_train, final_test = None, smiles_test = None):\n",
    "    '''\n",
    "    Concatenated the smiles for the train/test data for model building, as well as any other\n",
    "    data that we would like to evaluate with the model\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        final_train (np.array): train data containing chemical fingerprints/embeddings\n",
    "        smiles_train (np.array): test data containing chemical fingerprints/embeddings\n",
    "        final_test (np.array): optional test data if train/test data is being fed for model building\n",
    "        smiles_test (np.array): optional smiles data if train/test data is being fed for model building\n",
    "    \n",
    "    Returns:\n",
    "        final_train (np.array): final train concatenated smiles and morgan fingerprints\n",
    "        final_test (np.array): final test concatenated smiles and morgan fingerprints\n",
    "        \n",
    "    '''\n",
    "\n",
    "    if final_test is not None or smiles_test is not None:\n",
    "        train_concatenated = np.empty((final_train.shape[0], final_train.shape[1] + smiles_train.shape[1]), dtype=np.int16)\n",
    "\n",
    "        train_concatenated[:, :final_train.shape[1]] = final_train\n",
    "        train_concatenated[:, final_train.shape[1]:] = smiles_train\n",
    "\n",
    "\n",
    "        test_concatenated  = np.empty((final_test.shape[0], final_test.shape[1] + smiles_test.shape[1]), dtype = np.int16)\n",
    "\n",
    "        test_concatenated[:, :final_test.shape[1]]   = final_test\n",
    "        test_concatenated[:, final_test.shape[1]:]   = smiles_test\n",
    "\n",
    "        del final_test, final_train\n",
    "        gc.collect()\n",
    "\n",
    "        final_train  = train_concatenated\n",
    "        final_test   = test_concatenated\n",
    "\n",
    "        del train_concatenated, test_concatenated\n",
    "        gc.collect()\n",
    "        get_memory_usage()\n",
    "    \n",
    "    else:\n",
    "        train_concatenated = np.empty((final_train.shape[0], final_train.shape[1] + smiles_train.shape[1]), dtype=np.int16)\n",
    "\n",
    "        train_concatenated[:, :final_train.shape[1]] = final_train\n",
    "        train_concatenated[:, final_train.shape[1]:] = smiles_train\n",
    "        del final_train\n",
    "        gc.collect()\n",
    "        \n",
    "        final_train = train_concatenated\n",
    "        \n",
    "        del train_concatenated\n",
    "        gc.collect()\n",
    "    \n",
    "    return final_train, final_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_train(final_train, y_train, x_train_p, train_metadata, train_0 = 500000, train_1 = 3500000):\n",
    "    '''\n",
    "    Load a subset of the original data to memory constraints\n",
    "    -------------------\n",
    "    \n",
    "    Parameters:\n",
    "        final_train (np.array): train chemical embeddings and any other associated chemical data\n",
    "        y_train (torch.tensor): y_train data\n",
    "        x_train_p (list): the train_proteins\n",
    "        train_metadata (np.array): the metadata associated with the proteins\n",
    "        train_0 (int): Final index of the non binding training examples to load\n",
    "        train_1 (int): Initial index of binding training examples to load (must know how many total are in dataset)\n",
    "    \n",
    "    Returns:\n",
    "        final_train (np.array): concatenated final train\n",
    "        y_train (): concatenated y_train\n",
    "        x_train_p (): concatenated x_train_p\n",
    "        train_metadata (): concatenated metadata\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    final_train    = np.concatenate([final_train[:train_0], final_train[train_1:]], axis = 0)\n",
    "    y_t            = pd.Series(y_train.squeeze(1))\n",
    "    y_train        = pd.concat([y_t[y_t==0][:train_0], y_t[y_t==1]], axis = 0)\n",
    "    y_train        = torch.tensor(list(y_train.reset_index().drop(columns = ['index'])[0])).unsqueeze(1)\n",
    "    x_train_p      = x_train_p[:train_0] + x_train_p[train_1:]\n",
    "    train_metadata = np.concatenate([train_metadata[:train_0], train_metadata[train_1:]], axis = 0)\n",
    "    \n",
    "    return final_train, y_train, x_train_p, train_metadata\n",
    "\n",
    "final_train, y_train, x_train_p, train_metadata = truncate_train(final_train, y_train, x_train_p, train_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_data(x_data, y_data, metadata):\n",
    "    \n",
    "    '''\n",
    "    final_train, y_train, test_metadata\n",
    "    \n",
    "    Evenly spread out binding and non binding for model later\n",
    "    So that we don't exhaust memory indexing large arrays\n",
    "    The test evaluation is done in batches, so precision, recall, and F1 score\n",
    "    would be undefined until final batches if this isn't due to how data was\n",
    "    batched in the original splitting of data\n",
    "    \n",
    "    Parameters:\n",
    "        x_data (np.array): chemical embedding data\n",
    "        y_data (torch.tensor): drug binding data\n",
    "        metadata (pd.DataFrame): additional metadata\n",
    "    \n",
    "    Returns:\n",
    "        final_train (np.array):\n",
    "        y_train (torch.tensor):\n",
    "        interleaved_meta (pd.DataFrame):\n",
    "\n",
    "    '''\n",
    "    class_0_data = x_data[:len(y_data[y_data==0])]\n",
    "    class_1_data = x_data[len(y_data[y_data==0]):]\n",
    "\n",
    "    class_0_meta = metadata[:len(y_data[y_data==0])]\n",
    "    class_1_meta = metadata[len(y_data[y_data==0]):]\n",
    "    \n",
    "    \n",
    "    # Determine the number of points to interleave\n",
    "    num_class_1  = len(class_1_data)\n",
    "    num_class_0  = len(class_0_data)\n",
    "    \n",
    "   \n",
    "    # Calculate the step size to interleave class 1 data\n",
    "    step_size          = num_class_0 // num_class_1\n",
    "    \n",
    "\n",
    "    # Initialize arrays to hold the interleaved data\n",
    "    interleaved_data   = []\n",
    "    interleaved_labels = []\n",
    "    interleaved_meta   = []\n",
    "    \n",
    "    # Interleave class 1 data into class 0 data\n",
    "    j = 0\n",
    "    for i in range(num_class_0):\n",
    "        \n",
    "        class_0_joined = np.concatenate([class_0_data[i], class_0_meta[i]])\n",
    "        interleaved_data.append(class_0_data[i])\n",
    "        interleaved_meta.append(class_0_meta[i])\n",
    "        interleaved_labels.append(0)\n",
    "        if (i + 1) % step_size == 0 and j < num_class_1:\n",
    "            class_1_joined = np.concatenate([class_1_data[j], class_1_meta[j]])\n",
    "            interleaved_data.append(class_1_data[j])\n",
    "            interleaved_meta.append(class_1_meta[j])\n",
    "            interleaved_labels.append(1)\n",
    "            j += 1\n",
    "\n",
    "    # Add any remaining class 1 data if there's any left\n",
    "    while j < num_class_1:\n",
    "        class_1_joined = np.concatenate([class_1_data[j], class_1_meta[j]])\n",
    "        interleaved_data.append(class_1_data[j])\n",
    "        interleaved_meta.append(class_1_meta[j])\n",
    "        interleaved_labels.append(1)\n",
    "        j += 1\n",
    "\n",
    "    x_data      = np.array(interleaved_data)\n",
    "    y_data      = torch.tensor(interleaved_labels).unsqueeze(1)\n",
    "    metadata    = np.array(interleaved_meta)\n",
    "    del interleaved_data, interleaved_labels\n",
    "    gc.collect()\n",
    "    \n",
    "    return x_data, y_data, metadata\n",
    "\n",
    "# final_train, y_train = stratify_data(final_train, y_train)\n",
    "final_test, y_test, test_metadata = stratify_data(final_test, y_test, test_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_real_time(protein_batch):\n",
    "    '''\n",
    "    Process In real time to prevent memory and performance drops for large protein tensors\n",
    "    Large protein tensors come from transformer embeddings\n",
    "    \n",
    "    Parameters:\n",
    "        protein_batch: \n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    '''\n",
    "    return torch.concatenate([p for p in protein_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Rate becomes too low after one epoch for JunctionTree Embeddings**\n",
    "1. Might have to rethink how we want to go about this. We don't have computing power to deal with both the protein and chemical embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb(params):\n",
    "    return xgb.XGBClassifier(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features_vectorized(maccs_list, embedding_list, metadata = None, chunk_size = None):\n",
    "    '''\n",
    "    \n",
    "    Parameters:\n",
    "        maccs_list:\n",
    "        embedding_list:\n",
    "        metadata:\n",
    "        chunk_size:\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Pull Metadata for Protein\n",
    "    embedded_seq_flipped     = {value: key for key, value in embedded_sequences.items()}\n",
    "    \n",
    "    # If metadata is fed, add to numpy object\n",
    "    embedding_list        = np.expand_dims(np.array(embedding_list), axis = 1)\n",
    "    if metadata is None:\n",
    "        combined_features = np.hstack((maccs_list, embedding_list))\n",
    "    else:\n",
    "        \n",
    "#         metadata          = np.array(metadata)\n",
    "        combined_features = np.hstack((maccs_list, embedding_list, metadata))\n",
    "    \n",
    "    del embedded_seq_flipped, embedding_list, metadata\n",
    "    gc.collect()\n",
    "    \n",
    "#     print(combined_features.shape)\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "def get_available_memory():\n",
    "    mem = psutil.virtual_memory()\n",
    "    available_memory = mem.available / (1024 ** 2)  # Convert from bytes to MB\n",
    "    print(f\"Available Memory: {available_memory:.2f} MB\")\n",
    "    return available_memory\n",
    "\n",
    "def process_and_train_xgboost(maccs_list, embedding_list, y_list, chunk_size=None, weight = {0:1, 1: 1}, metadata = None, dd_pandas = False):\n",
    "    \n",
    "    if isinstance(maccs_list, dict):\n",
    "        maccs_list = da.concatenate([maccs_list[index] for index in maccs_list.keys()], axis = 0)\n",
    "    \n",
    "    if chunk_size is None:\n",
    "        chunk_size = len(maccs_list)\n",
    "    \n",
    "    # Do or don't include protein data\n",
    "    if metadata is None:\n",
    "        combined_features       = combine_features_vectorized(maccs_list, embedding_list)\n",
    "    else:\n",
    "        metadata           \n",
    "        combined_features       = combine_features_vectorized(maccs_list, embedding_list, metadata, chunk_size)\n",
    "\n",
    "    \n",
    "    if dd_pandas:\n",
    "        chunk_df            = pd.DataFrame(combined_features)\n",
    "        chunk_df['targets'] = y_train\n",
    "    else:\n",
    "        final_train         = combined_features\n",
    "        y_train             = np.array(y_list)\n",
    "    \n",
    "    del maccs_list, embedding_list, y_list\n",
    "    get_memory_usage()\n",
    "    gc.collect()\n",
    "\n",
    "    final_train = da.from_array(final_train)\n",
    "    y_train     = da.from_array(y_train) \n",
    "\n",
    "    print('Dask Arrays Finalized: Available memory in gb:', get_memory_usage())\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    params = {\n",
    "    'objective': 'binary:logistic',  # Change based on your task\n",
    "    'eval_metric': 'map',\n",
    "    'seed': '42'}\n",
    "\n",
    "\n",
    "    chunk_size = final_train.shape[0]\n",
    "    clf        = None\n",
    "    num_chunks = math.ceil(final_train.shape[0] / chunk_size)\n",
    "    num_boost_round = 20\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(num_chunks), total = num_chunks):\n",
    "\n",
    "        # Load chunk into memory\n",
    "        X_chunk = final_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        y_chunk = y_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        \n",
    "        # Create DMatrix for XGBoost\n",
    "        dtrain  = xgb.DMatrix(X_chunk, label=y_chunk)\n",
    "   \n",
    "        if clf is None:\n",
    "            clf = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "        else:\n",
    "            clf = xgb.train(params, dtrain, num_boost_round=num_boost_round, xgb_model=clf)\n",
    "\n",
    "        # Free up memory\n",
    "        del X_chunk, y_chunk, dtrain\n",
    "        gc.collect()\n",
    "    \n",
    "    print('you did it')\n",
    "       \n",
    "    gc.collect()\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_train_catboost(maccs_list, embedding_list, y_list, chunk_size=1000, weight={0:1, 1:1}, metadata=None, dd_pandas=False):\n",
    "    client = Client()\n",
    "    \n",
    "    if isinstance(maccs_list, dict):\n",
    "        maccs_list = da.concatenate([maccs_list[index] for index in maccs_list.keys()], axis=0)\n",
    "        \n",
    "    if chunk_size is None:\n",
    "        chunk_size = len(maccs_list)\n",
    "\n",
    "    # Combine features for the current chunk\n",
    "    if metadata is None:\n",
    "        combined_features = combine_features_vectorized(maccs_list, embedding_list)\n",
    "    else:\n",
    "        combined_features = combine_features_vectorized(maccs_list, embedding_list, metadata, chunk_size)\n",
    "\n",
    "    if dd_pandas:\n",
    "        chunk_df = pd.DataFrame(combined_features)\n",
    "        chunk_df['targets'] = y_list\n",
    "    else:\n",
    "        final_train = combined_features\n",
    "        y_train = np.array(y_list)\n",
    "    \n",
    "    del maccs_list, embedding_list, y_list\n",
    "    get_memory_usage()\n",
    "    gc.collect()\n",
    "\n",
    "    final_train = da.from_array(final_train)\n",
    "    y_train = da.from_array(y_train)\n",
    "\n",
    "    get_memory_usage()\n",
    "    gc.collect()\n",
    "\n",
    "    params = {\n",
    "        'loss_function': 'Logloss',  # Use 'CrossEntropy' for multi-class problems\n",
    "        'eval_metric': 'AUC',\n",
    "        'iterations': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'depth': 6,\n",
    "        'verbose': 200,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "\n",
    "    num_chunks = math.ceil(final_train.shape[0] / chunk_size)\n",
    "    clf = CatBoostClassifier(**params)\n",
    "    \n",
    "    print('Stratified training in chunks')\n",
    "    for i in tqdm(range(num_chunks), total=num_chunks):\n",
    "\n",
    "        # Load chunk into memory\n",
    "        X_chunk = final_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        y_chunk = y_train[i * chunk_size: (i + 1) * chunk_size].compute()\n",
    "        \n",
    "        # Create Pool for CatBoost\n",
    "        train_pool = Pool(X_chunk, y_chunk)\n",
    "        \n",
    "        # Train the model incrementally\n",
    "        if i == 0:\n",
    "            clf.fit(train_pool, init_model=None)\n",
    "        else:\n",
    "            clf.fit(train_pool, init_model=clf, use_best_model=False)\n",
    "\n",
    "        # Free up memory\n",
    "        del X_chunk, y_chunk, train_pool\n",
    "        gc.collect()\n",
    "        \n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible Solution To Memory Issue with XGBOOST**\n",
    "1. https://xgboost.readthedocs.io/en/latest/tutorials/external_memory.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 17.32 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     protein_metadata       = train_metadata\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     class_weights          \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m: weight\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m}\n\u001b[0;32m---> 35\u001b[0m     clf                    \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_train_xgboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mx_train_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdd_pandas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m model_types[\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m     44\u001b[0m     chunk_size            \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 66\u001b[0m, in \u001b[0;36mprocess_and_train_xgboost\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     63\u001b[0m get_memory_usage()\n\u001b[1;32m     64\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m---> 66\u001b[0m final_train \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m y_train     \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mfrom_array(y_train) \n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDask Arrays Finalized: Available memory in gb:\u001b[39m\u001b[38;5;124m'\u001b[39m, get_memory_usage())\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/array/core.py:3489\u001b[0m, in \u001b[0;36mfrom_array\u001b[0;34m(x, chunks, name, lock, asarray, fancy, getitem, meta, inline_array)\u001b[0m\n\u001b[1;32m   3484\u001b[0m chunks \u001b[38;5;241m=\u001b[39m normalize_chunks(\n\u001b[1;32m   3485\u001b[0m     chunks, x\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, previous_chunks\u001b[38;5;241m=\u001b[39mprevious_chunks\n\u001b[1;32m   3486\u001b[0m )\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 3489\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masarray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfancy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minline_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3490\u001b[0m     name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m token\n\u001b[1;32m   3491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/base.py:1034\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(ensure_deterministic, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Deterministic token\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m>>> tokenize([1, 2, '3'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    Defaults to the `tokenize.ensure-deterministic` configuration parameter.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _seen_ctx(reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), _ensure_deterministic_ctx(ensure_deterministic):\n\u001b[0;32m-> 1034\u001b[0m     token: \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize_seq_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m   1036\u001b[0m         token \u001b[38;5;241m=\u001b[39m token, _normalize_seq_func(\u001b[38;5;28msorted\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/base.py:1160\u001b[0m, in \u001b[0;36m_normalize_seq_func\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1159\u001b[0m         seen[\u001b[38;5;28mid\u001b[39m(item)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(seen), item\n\u001b[0;32m-> 1160\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m     out\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/utils.py:773\u001b[0m, in \u001b[0;36mDispatch.__call__\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03mCall the corresponding method based on type of argument.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mtype\u001b[39m(arg))\n\u001b[0;32m--> 773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/base.py:1213\u001b[0m, in \u001b[0;36mnormalize_object\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _normalize_dataclass(o)\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_normalize_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1215\u001b[0m     _maybe_raise_nondeterministic(\n\u001b[1;32m   1216\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m cannot be deterministically hashed. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.dask.org/en/latest/custom-collections.html#implementing-deterministic-hashing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1219\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/base.py:1251\u001b[0m, in \u001b[0;36m_normalize_pickle\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   1249\u001b[0m     pik \u001b[38;5;241m=\u001b[39m cloudpickle\u001b[38;5;241m.\u001b[39mdumps(o, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, buffer_callback\u001b[38;5;241m=\u001b[39mbuffers\u001b[38;5;241m.\u001b[39mappend)\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hash_buffer_hex(pik), [hash_buffer_hex(buf) \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m buffers]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/base.py:1251\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     buffers\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   1249\u001b[0m     pik \u001b[38;5;241m=\u001b[39m cloudpickle\u001b[38;5;241m.\u001b[39mdumps(o, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, buffer_callback\u001b[38;5;241m=\u001b[39mbuffers\u001b[38;5;241m.\u001b[39mappend)\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hash_buffer_hex(pik), [\u001b[43mhash_buffer_hex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m buffers]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/hashing.py:98\u001b[0m, in \u001b[0;36mhash_buffer_hex\u001b[0;34m(buf, hasher)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash_buffer_hex\u001b[39m(buf, hasher\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Same as hash_buffer, but returns its result in hex-encoded form.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mhash_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     s \u001b[38;5;241m=\u001b[39m binascii\u001b[38;5;241m.\u001b[39mb2a_hex(h)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39mdecode()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/hashing.py:88\u001b[0m, in \u001b[0;36mhash_buffer\u001b[0;34m(buf, hasher)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hasher \u001b[38;5;129;01min\u001b[39;00m hashers:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhasher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mOverflowError\u001b[39;00m):\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/dask/hashing.py:67\u001b[0m, in \u001b[0;36m_hash_sha1\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_hash_sha1\u001b[39m(buf):\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Produce a 20-bytes hash of *buf* using SHA1.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhashlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msha1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdigest()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_types                 = ['nueral_network', 'xgboost', 'catboost']\n",
    "model_type                  = model_types[1]\n",
    "# if train_data was not loaded, try tensor operations\n",
    "try:\n",
    "    class1                  = len(train_data[train_data['binds'] == 1].dropna())\n",
    "    class0                  = len(train_data[train_data['binds'] == 0].dropna()[:class_0_size])\n",
    "except TypeError:\n",
    "    class1                  = len(y_test[y_test==1]) + len(y_train[y_train==1])\n",
    "    class0                  = len(y_test[y_test==0]) + len(y_train[y_train==0])\n",
    "    \n",
    "weight                      = class0 / class1\n",
    "\n",
    "try:\n",
    "    protein_size            = max(embedded_sequences['sEH'].size())\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "loss_function               = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weight))\n",
    "rf_features                 = []\n",
    "if model_type == model_types[0]:\n",
    "    clf                     = SimpleNN().to(device)\n",
    "\n",
    "    batch_size              = 256\n",
    "    epochs                  = 4\n",
    "    trainer                 = Trainer(max_epochs = epochs, bypass_prepare = False, batch_size = batch_size)\n",
    "    optimizer               = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "    scheduler               = lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, factor=0.1)\n",
    "    trainer.fit(model, [final_train, y_train], x_train_p, None, optimizer, loss_function, scheduler)\n",
    "    \n",
    "elif model_type == model_types[1]:\n",
    "#     chunk_size             = 50000\n",
    "    chunk_size             = None\n",
    "#     protein_metadata       = train_metadata\n",
    "    class_weights          = {0: 1, 1: weight/8}\n",
    "    clf                    = process_and_train_xgboost(final_train, \n",
    "                                                       x_train_p, \n",
    "                                                       y_train, \n",
    "                                                       chunk_size=chunk_size, \n",
    "                                                       weight = class_weights, \n",
    "                                                       metadata = train_metadata, \n",
    "                                                       dd_pandas = False) \n",
    "\n",
    "elif model_type == model_types[2]:\n",
    "    chunk_size            = None\n",
    "    class_weights         = {0: 1, 1: weight/8}\n",
    "    clf                   = process_and_train_catboost(final_train,\n",
    "                                                      x_train_p, \n",
    "                                                      y_train, \n",
    "                                                      chunk_size=chunk_size,\n",
    "                                                      weight = class_weights, \n",
    "                                                      metadata = train_metadata, \n",
    "                                                      dd_pandas = False) \n",
    "    \n",
    "else:\n",
    "    raise ValueError(f'Model must be in {model_types}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type, model_path, model_class = None):\n",
    "    '''\n",
    "    Load an existing trained model\n",
    "    \n",
    "    Parameters:\n",
    "        model_type (str): model type so correct methods are called\n",
    "        model_path (str): path to the model weights or parameters\n",
    "        model_class (torch.nn.Module): If pytorch model is loaded, the model class must be\n",
    "                                       specified so the weights match the model class\n",
    "    \n",
    "    Returns:\n",
    "        model: loaded model instance with weights or parameters\n",
    "    '''\n",
    "    \n",
    "    if model_type == 'xgboost':\n",
    "        model = xgb.XGBClassifier()\n",
    "        model.load_model(model_path)\n",
    "    elif model_type == 'nueral_network':\n",
    "        model            = ResModel()\n",
    "        state_dict       = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    elif model_type == 'catboost':\n",
    "        clf = CatBoostClassifier()\n",
    "        clf.load_model()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model(model_type = 'xgboost', model_path = '5.5mil_xgboost_protparam.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m y_t    \u001b[38;5;241m=\u001b[39m y_test[batch\u001b[38;5;241m*\u001b[39mbatch_s:(batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_s]\n\u001b[1;32m    117\u001b[0m t_meta \u001b[38;5;241m=\u001b[39m test_metadata[batch\u001b[38;5;241m*\u001b[39mbatch_s:(batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_s]\n\u001b[0;32m--> 118\u001b[0m pred, prob, metrics_dictionary[batch] \u001b[38;5;241m=\u001b[39m evaluate_model(\u001b[43mclf\u001b[49m, f_test, x_tp, y_t, t_meta)\n\u001b[1;32m    119\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(pred)\n\u001b[1;32m    120\u001b[0m probabilities\u001b[38;5;241m.\u001b[39mextend(prob)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_test_data(x_test_chem, x_test_protein, test_metadata = None):\n",
    "    '''\n",
    "    Combine and preprocess test data similarly to training data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_test_chem (np.array): x chemical embeddings\n",
    "        x_test_protein (np.array): x protein embeddings\n",
    "        test_metadata (np.array): test metadata\n",
    "        \n",
    "    Returns:\n",
    "        x_test_combined ():\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Flatten the protein embeddings and concatenate with maccs features\n",
    "    if test_metadata is None:\n",
    "        x_test_combined = combine_features_vectorized(x_test_chem, x_test_protein)\n",
    "    else:\n",
    "        x_test_combined = combine_features_vectorized(x_test_chem, x_test_protein, test_metadata)\n",
    "\n",
    "    return x_test_combined\n",
    "\n",
    "def evaluate_model(clf, x_test_chem, x_test_protein, y_test, test_metadata = None, batch_size = 200000):\n",
    "    '''\n",
    "    Evaluate the model using the prepared test data.\n",
    "    \n",
    "    Parameters:\n",
    "        clf : classification model used to evaluate test data\n",
    "        x_test_chem (np.array): x test chemical embeddings\n",
    "        x_test_protein (np.array): x test protein embeddings\n",
    "        y_test (np.array): the y_test with binding data\n",
    "        test_metadata (np.array): the metadata of the test class\n",
    "        batch_size (int): batch size of evaluation\n",
    "    \n",
    "    Returns:\n",
    "        predictions (np.array): binary predictions of clf model\n",
    "        probabilities: probability of binding for clf model\n",
    "        metrics_dictionary: dictionary of precision, recall, f1_score, and accuracy. \n",
    "        \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Nueral network is a bad choice at least how I have it defined\n",
    "\n",
    "    gradient_boost =  isinstance(clf, xgb.core.Booster) or isinstance(clf, xgb.sklearn.XGBClassifier)\n",
    "    if gradient_boost:\n",
    "        X_test_prepared   = prepare_test_data(x_test_chem, x_test_protein, test_metadata)\n",
    "        X_test_prepared   = dd.from_array(X_test_prepared)\n",
    "#         X_test_prepared   = xgb.DMatrix(X_test_prepared)\n",
    "        probabilities     = clf.predict(X_test_prepared)\n",
    "        predictions       = (probabilities >= 0.5).astype(int)\n",
    "        \n",
    "    elif isinstance(clf, catboost.core.CatBoostClassifier):\n",
    "        X_test_prepared   = prepare_test_data(x_test_chem, x_test_protein, test_metadata)\n",
    "        probabilities     = clf.predict_proba(X_test_prepared)[:, 1]\n",
    "        predictions       = (probabilities >= 0.5).astype(int)\n",
    "    \n",
    "    # improve to check class or super class of nueral network\n",
    "    else:\n",
    "        # Predict using the prepared test data\n",
    "#          if 'leash_utils' in str(type(clf)) or 'ResModel' in str(clf):\n",
    "        batches           = math.ceil(len(X_test_maccs) / batch_size)\n",
    "       \n",
    "        x_test_chem       = torch.tensor(x_test_chem, device = device)\n",
    "        x_test_protein    = torch.tensor(x_test_protein, dtype = torch.float32, device = device)\n",
    "        predictions       = []\n",
    "        for batch in tqdm(range(batches), total = batches):\n",
    "            subset_chem  = x_test_chem[batch*batch_size:(batch + 1)*batch_size]\n",
    "            subset_prot   = X_test_protein[batch*batch_size:(batch + 1)*batch_size]\n",
    "            subset_pred   = clf(subset_chem, subset_prot).cpu().detach().numpy()\n",
    "            predictions.extend(subset_pred)\n",
    "            \n",
    "            \n",
    "            del subset_maccs, subset_prot, subset_pred\n",
    "            gc.collect()\n",
    "            \n",
    "        probabilities     = np.array(predictions) \n",
    "        predictions       = (probabilities >= 0.5).astype(int)\n",
    "        del x_test_chem, x_test_protein\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(predictions, y_test)\n",
    "    recall    = recall_score(predictions, y_test)\n",
    "    f1        = f1_score(predictions, y_test)\n",
    "    accuracy  = accuracy_score(predictions, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    metrics_dictionary = {'Precision': precision,\n",
    "                         'Recall': recall,\n",
    "                         'F1 Score:': f1,\n",
    "                         'Accuracy': accuracy,\n",
    "                         }\n",
    "\n",
    "    return predictions, probabilities, metrics_dictionary\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Assuming X_test_maccs and X_test_protein are defined similarly to how training data was defined\n",
    "# These should be appropriately reshaped or transformed to match the training data setup\n",
    "\n",
    "batch_s    = math.ceil(final_test.shape[0] / 5)\n",
    "\n",
    "predictions        = [] \n",
    "probabilities      = []\n",
    "metrics_dictionary = {}\n",
    "for batch in range(5):\n",
    "    f_test = final_test[batch*batch_s:(batch+1)*batch_s]\n",
    "    x_tp   = x_test_p[batch*batch_s:(batch+1)*batch_s]\n",
    "    y_t    = y_test[batch*batch_s:(batch+1)*batch_s]\n",
    "    t_meta = test_metadata[batch*batch_s:(batch+1)*batch_s]\n",
    "    pred, prob, metrics_dictionary[batch] = evaluate_model(clf, f_test, x_tp, y_t, t_meta)\n",
    "    predictions.extend(pred)\n",
    "    probabilities.extend(prob)\n",
    "\n",
    "# predictions = evaluate_model(clf, final_test, x_test_p, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier Results\n",
    "**MACCS Keys Test**\n",
    "1. Weight_ratio = class_imbalance/8, class_imbance = 300000/2562\n",
    "1. Precision: 0.4114906832298137\n",
    "2. Recall: 0.3035509736540664\n",
    "3. F1 Score: 0.34937376400791037\n",
    "\n",
    "**RDKit Fingerprints radius = 2, n_bits = 1024**\n",
    "1. Weight_ratio = class_imbalance/8, class_inbalance = 300000/2562\n",
    "2. precisiion = 0.642\n",
    "3. Recall     = 0.4899\n",
    "4. F1 Score   = 0.556\n",
    "\n",
    "**RdKit Fingerprints radius = 2, n_bits = 2048**\n",
    "1. Precision: 0.687888198757764\n",
    "2. Recall: 0.3983812949640288\n",
    "3. F1 Score: 0.5045558086560364\n",
    "\n",
    "**Rdkit Fingerprints radius = 3, n_bits = 1024**\n",
    "1. Precision: 0.7096273291925466\n",
    "2. Recall: 0.3079514824797844\n",
    "3. F1 Score: 0.42951127819548873\n",
    "\n",
    "**RDKit Fingerprints raidus = 3, n_bits = 2048**\n",
    "1. Precision: 0.7142857142857143\n",
    "2. Recall: 0.40069686411149824\n",
    "3. F1 Score: 0.5133928571428571\n",
    "\n",
    "# XGBoost Classier Results\n",
    "**Boosting_rounds = 50**\n",
    "1. Precision: 0.5714285714285714\n",
    "2. Recall: 0.9363867684478372\n",
    "3. F1 Score: 0.7097396335583414\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nueral Network with 5.5 Million Training/Test ResModel**\n",
    "1. Precision: 1.0\n",
    "2. Recall: 0.2844247168289711\n",
    "3. F1 Score: 0.4428826588313684\n",
    "\n",
    "**XGBoost with 5.5 Million Training/Test Num_boosting_rounds = 30**\n",
    "Precision: 0.5395745228946182\n",
    "Recall: 0.7427490728564885\n",
    "F1 Score: 0.6250660565630692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 68\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#         get_memory_usage()\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_preds\n\u001b[0;32m---> 68\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m evaluate_test(\u001b[43mclf\u001b[49m, model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# test_preds = evaluate_test(model, model_type = 'nueral_network')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_test(model, model_type = 'xgboost', smiles_blocks = False):\n",
    "    '''\n",
    "    Evaluate the model built with the Kaggle BELKA test dataset\n",
    "    To evaluate, model must be submitted to competition site\n",
    "    https://www.kaggle.com/competitions/leash-BELKA\n",
    "    \n",
    "    Parameters:\n",
    "        model : model to evaluate data with\n",
    "        model_type : xgboost, nueral network, or catboost model types\n",
    "        smiles_blocks : whether to include the smiles blocks\n",
    "        \n",
    "    Returns:\n",
    "        test_preds: list containing the prediction probabilites from the model\n",
    "        \n",
    "    '''\n",
    "    test_preds       = []\n",
    "    raw_preds        = []\n",
    "    batch_size       = 20000\n",
    "    batches          = math.ceil(len(test_data) / batch_size)\n",
    "    generator        = rdFingerprintGenerator.GetMorganGenerator(radius = 3, fpSize = 2048)\n",
    "    smiles_test      = np.load('test_data_smiles.npz')['test_smiles']\n",
    "    protein_meta     = np.load('test_eval.npz')['test_eval']\n",
    "    proteins         = pickle.load(open('test_protein.pckl', 'rb'))\n",
    "    \n",
    "    # Did not see an improvement adding these\n",
    "    if smiles_blocks:\n",
    "        smiles_blocks    = np.load('test_smiles_blocks.npz')['smiles_blocks']\n",
    "        smiles_test, _   = concatenate_smiles(smiles_blocks, smiles_test)\n",
    "    \n",
    "    \n",
    "    for index, batch in tqdm(enumerate(range(batches)), total = batches):\n",
    "        smiles_list          = smiles_test[batch*batch_size:(batch+1)*batch_size]\n",
    "        protein_test         = proteins[batch*batch_size:(batch+1)*batch_size]\n",
    "        protein_metadata     = protein_meta[batch*batch_size:(batch+1)*batch_size]\n",
    "        if model_type == 'xgboost':\n",
    "            combined_features = combine_features_vectorized(smiles_list,\n",
    "                                                        protein_test,\n",
    "                                                        chunk_size = batch_size, \n",
    "                                                        metadata   = protein_metadata)\n",
    "            test_eval          = da.from_array(combined_features, chunks = (batch_size, -1))\n",
    "            predictions        = model.predict_proba(test_eval)\n",
    "            test_preds.extend(predictions)\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            combined_features = None\n",
    "            protein_metadata  = None\n",
    "            test_eval         = combine_features_vectorized(smiles_list,\n",
    "                                                        protein_test,\n",
    "                                                        chunk_size = batch_size, \n",
    "                                                        metadata = protein_metadata)\n",
    "            probabilities     = clf.predict_proba(test_eval)\n",
    "            predictions       = (probabilities >= 0.5).astype(int)\n",
    "            test_preds.extend(probabilities)\n",
    "            \n",
    "        elif model_type == 'nueral_network':\n",
    "            test_eval           = None\n",
    "            combined_features   = None\n",
    "            smiles              = torch.tensor(smiles_list)\n",
    "            protein_test        = torch.tensor(protein_test, dtype = torch.float32)\n",
    "            predictions         = model(smiles, protein_test).detach().numpy()\n",
    "            \n",
    "            test_preds.extend(predictions)\n",
    "                \n",
    "        del predictions, test_eval, smiles_list, combined_features, protein_test\n",
    "\n",
    "        if index % 10 == 9:\n",
    "            gc.collect()\n",
    "#         get_memory_usage()\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "\n",
    "model_type = 'xgboost'\n",
    "test_preds = evaluate_test(clf, model_type = model_type)\n",
    "# test_preds = evaluate_test(model, model_type = 'nueral_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xgboost_preds(test_preds):\n",
    "    ''' \n",
    "    Pulls the probability of class1 from a list containing two elements\n",
    "    \n",
    "    Parameters:\n",
    "        test_preds (list): list containing probability of class0 and class1\n",
    "        \n",
    "    Returns:\n",
    "        preds (list): list containing the probability of class1\n",
    "        \n",
    "    '''\n",
    "\n",
    "    preds = []\n",
    "    for pred in test_preds:\n",
    "        preds.append(pred[1])\n",
    "\n",
    "    return preds\n",
    "\n",
    "if model_type == 'xgboost':\n",
    "    test_preds = process_xgboost_preds(test_preds)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kaggle_submission(test_preds):\n",
    "    submission = pd.concat([test_data['id'], pd.Series(test_preds, name = 'binds')], axis = 1)\n",
    "    # submission = pd.concat([test_data['id'], pd.Series(pred1, name = 'binds')], axis = 1)\n",
    "    submission.to_csv('submission.csv', index = None)\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
