{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchsummary, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.0 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 torchsummary-1.5.1 transformers-4.41.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (10.2.0)\n",
      "Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2023.9.6\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d2l\n",
      "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (1.0.0)\n",
      "Collecting numpy==1.23.5 (from d2l)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting matplotlib==3.7.2 (from d2l)\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (0.1.6)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (2.31.0)\n",
      "Collecting pandas==2.0.3 (from d2l)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy==1.10.1 (from d2l)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: notebook in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.1.2)\n",
      "Requirement already satisfied: qtconsole in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.16.3)\n",
      "Requirement already satisfied: ipykernel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.29.3)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (10.2.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->d2l)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (2.9.0)\n",
      "Requirement already satisfied: traitlets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib-inline==0.1.6->d2l) (5.14.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.22.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (6.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.42)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.17.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.13.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.25.4)\n",
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (4.1.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->d2l) (4.2.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.5.3)\n",
      "Requirement already satisfied: overrides in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
      "Requirement already satisfied: websocket-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.2.4)\n",
      "Requirement already satisfied: tomli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.9.24)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (4.21.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.18.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.4)\n",
      "Requirement already satisfied: uri-template in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.9.0.20240316)\n",
      "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, numpy, scipy, pandas, matplotlib, d2l\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.12.0\n",
      "    Uninstalling scipy-1.12.0:\n",
      "      Successfully uninstalled scipy-1.12.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.0.3 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed d2l-1.0.3 matplotlib-3.7.2 numpy-1.23.5 pandas-2.0.3 pyparsing-3.0.9 scipy-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.1.3)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (5.9.8)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: multidict, h5py, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 h5py-3.11.0 multidict-6.0.5 torch-geometric-2.5.3 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "--46iLsBsl_W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:616: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from   d2l import torch as d2l\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from   torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from   torchsummary import summary\n",
    "from   torchtext.data.utils import get_tokenizer\n",
    "from   torchtext.datasets import AG_NEWS\n",
    "from   transformers import AutoTokenizer\n",
    "from   torch.nn.utils.rnn import pad_sequence\n",
    "from   torch.utils.data import TensorDataset, DataLoader\n",
    "from   transformers import AutoModel, AutoTokenizer\n",
    "from   torch import nn\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "import h5py\n",
    "import ast\n",
    "import gc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
    "from datetime import datetime\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "from JunctionTree.main import Datapreprocess\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoyZdy6Emr2E"
   },
   "source": [
    "**First Protein Target from Competition Description**\n",
    "1. Epoxide Hydrolase 2\n",
    "2. High Blood pressure and diabetes target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-OBEQKDrmVyf"
   },
   "outputs": [],
   "source": [
    "EPHX2 = '''MTLRAAVFDLDGVLALPAVFGVLGRTEEALALPRGLLNDAFQKGGPEGATTRLMKGEI\n",
    "TLSQWIPLMEENCRKCSETAKVCLPKNFSIKEIFDKAISARKINRPMLQAALMLRKKGFTTA\n",
    "ILTNTWLDDRAERDGLAQLMCELKMHFDFLIESCQVGMVKPEPQIYKFLLDTLKASPSEVVF\n",
    "LDDIGANLKPARDLGMVTILVQDTDTALKELEKVTGIQLLNTPAPLPTSCNPSDMSHGYVTV\n",
    "KPRVRLHFVELGSGPAVCLCHGFPESWYSWRYQIPALAQAGYRVLAMDMKGYGESSAPPEIE\n",
    "EYCMEVLCKEMVTFLDKLGLSQAVFIGHDWGGMLVWYMALFYPERVRAVASLNTPFIPANPNM\n",
    "SPLESIKANPVFDYQLYFQEPGVAEAELEQNLSRTFKSLFRASDESVLSMHKVCEAGGLFVNS\n",
    "PEEPSLSRMVTEEEIQFYVQQFKKSGFRGPLNWYRNMERNWKWACKSLGRKILIPALMVTAEK\n",
    "DFVLVPQMSQHMEDWIPHLKRGHIEDCGHWTQMDKPTEVNQILIKWLDSDARNPPVVSKM'''\n",
    "\n",
    "EPHX2 = EPHX2[1:554] # Commerically Purchased Variant is amino 2-555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq4dugrKnU8e"
   },
   "source": [
    "**Second Protein Target from Competition Description**\n",
    "1. Bromo Domain 4\n",
    "2. Cancer disease progression (histone protein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sNBvC9xOnjt3"
   },
   "outputs": [],
   "source": [
    "BRD4 = '''MSAESGPGTRLRNLPVMGDGLETSQMSTTQAQAQPQPANAASTNPPPPETSNPNKPKRQTN\n",
    "QLQYLLRVVLKTLWKHQFAWPFQQPVDAVKLNLPDYYKIIKTPMDMGTIKKRLENNYYWNAQEC\n",
    "IQDFNTMFTNCYIYNKPGDDIVLMAEALEKLFLQKINELPTEETEIMIVQAKGRGRGRKETGTA\n",
    "KPGVSTVPNTTQASTPPQTQTPQPNPPPVQATPHPFPAVTPDLIVQTPVMTVVPPQPLQTPPPV\n",
    "PPQPQPPPAPAPQPVQSHPPIIAATPQPVKTKKGVKRKADTTTPTTIDPIHEPPSLPPEPKTTK\n",
    "LGQRRESSRPVKPPKKDVPDSQQHPAPEKSSKVSEQLKCCSGILKEMFAKKHAAYAWPFYKPVD\n",
    "VEALGLHDYCDIIKHPMDMSTIKSKLEAREYRDAQEFGADVRLMFSNCYKYNPPDHEVVAMARK\n",
    "LQDVFEMRFAKMPDEPEEPVVAVSSPAVPPPTKVVAPPSSSDSSSDSSSDSDSSTDDSEEERAQ\n",
    "RLAELQEQLKAVHEQLAALSQPQQNKPKKKEKDKKEKKKEKHKRKEEVEENKKSKAKEPPPKKT\n",
    "KKNNSSNSNVSKKEPAPMKSKPPPTYESEEEDKCKPMSYEEKRQLSLDINKLPGEKLGRVVHII\n",
    "QSREPSLKNSNPDEIEIDFETLKPSTLRELERYVTSCLRKKRKPQAEKVDVIAGSSKMKGFSSS\n",
    "ESESSSESSSSDSEDSETEMAPKSKKKGHPGREQKKHHHHHHQQMQQAPAPVPQQPPPPPQQPP\n",
    "PPPPPQQQQQPPPPPPPPSMPQQAAPAMKSSPPPFIATQVPVLEPQLPGSVFDPIGHFTQPILH\n",
    "LPQPELPPHLPQPPEHSTPPHLNQHAVVSPPALHNALPQQPSRPSNRAAALPPKPARPPAVSPA\n",
    "LTQTPLLPQPPMAQPPQVLLEDEEPPAPPLTSMQMQLYLQQLQKVQPPTPLLPSVKVQSQPPPP\n",
    "LPPPPHPSVQQQLQQQPPPPPPPQPQPPPQQQHQPPPRPVHLQPMQFSTHIQQPPPPQGQQPPH\n",
    "PPPGQQPPPPQPAKPQQVIQHHHSPRHHKSDPYSTGHLREAPSPLMIHSPQMSQFQSLTHQSPP\n",
    "QQNVQPKKQELRAASVVQPQPLVVVKEEKIHSPIIRSEPFSPSLRPEPPKHPESIKAPVHLPQR\n",
    "PEMKPVDVGRPVIRPPEQNAPPPGAPDKDKQKQEPKTPVAPKKDLKIKNMGSWASLVQKHPTTP\n",
    "SSTAKSSSDSFEQFRRAAREKEEREKALKAQAEHAEKEKERLRQERMRSREDEDALEQARRAHE\n",
    "EARRRQEQQQQQRQEQQQQQQQQAAAVAAAATPQAQSSQPQSMLDQQRELARKREQERRRREAM\n",
    "AATIDMNFQSDLLSIFEENLF'''\n",
    "\n",
    "\n",
    "BRD4   =   BRD4[3:459] # Positions 4-460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlGXTKTGo3Jy"
   },
   "source": [
    "**Third Protein From Competition Description**\n",
    "1. Serum Albumin\n",
    "2. The most common protein in the blood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dIcPq8XhohfA"
   },
   "outputs": [],
   "source": [
    "P02768 = ''' MKWVTFISLLFLFSSAYSRGVFRRDAHKSEVAHRFKDLGEENFKALVLIAFAQYLQQCPFE\n",
    "DHVKLVNEVTEFAKTCVADESAENCDKSLHTLFGDKLCTVATLRETYGEMADCCAKQEPERNECF\n",
    "LQHKDDNPNLPRLVRPEVDVMCTAFHDNEETFLKKYLYEIARRHPYFYAPELLFFAKRYKAAFTE\n",
    "CCQAADKAACLLPKLDELRDEGKASSAKQRLKCASLQKFGERAFKAWAVARLSQRFPKAEFAEVS\n",
    "KLVTDLTKVHTECCHGDLLECADDRADLAKYICENQDSISSKLKECCEKPLLEKSHCIAEVENDE\n",
    "MPADLPSLAADFVESKDVCKNYAEAKDVFLGMFLYEYARRHPDYSVVLLLRLAKTYETTLEKCCA\n",
    "AADPHECYAKVFDEFKPLVEEPQNLIKQNCELFEQLGEYKFQNALLVRYTKKVPQVSTPTLVEVS\n",
    "RNLGKVGSKCCKHPEAKRMPCAEDYLSVVLNQLCVLHEKTPVSDRVTKCCTESLVNRRPCFSALE\n",
    "VDETYVPKEFNAETFTFHADICTLSEKERQIKKQTALVELVKHKPKATKEQLKAVMDDFAAFVEK\n",
    "CCKADDKETCFAEEGKKLVAASQAALGL'''\n",
    "\n",
    "\n",
    "P02768 = P02768[24:608] # Positions 25-609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left off here 05May2024**\n",
    "1. Make the embedding dim shapes the same so that they can easily be batched in training\n",
    "2. Otherwise training is going to be a pain in the ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Yr0hEwTSsAdT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6e1c828bb5441fafa5ae8e0a989b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd975e96e632478384afd50d8793b783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beb2dca20f049eabfb8a4158b372363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d63ba0fef0f4b539c6d2b178ced06cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0917b0716b364bc290523f33ae865974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations from Kaggle\n",
    "\n",
    "def create_protein_embeddings():\n",
    "    proteins           = {'sEH': EPHX2, 'BRD4': BRD4, 'HSA': P02768}\n",
    "\n",
    "    embedded_sequences = {}\n",
    "\n",
    "    model_name         = \"Rostlab/prot_bert\"  # Example, check the actual model repository\n",
    "    tokenizer          = AutoTokenizer.from_pretrained(model_name)\n",
    "    model              = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    #Maximum Embedding dim 1 for tensor\n",
    "    embedding_max      = 12\n",
    "    for name, sequence in proteins.items():\n",
    "        inputs         = tokenizer(sequence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs       = model(**inputs)\n",
    "            embeddings    = outputs.last_hidden_state\n",
    "            pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Flatten into 1D vector for Nueral network Linear Layers\n",
    "        \n",
    "#         embeddings        = embeddings.squeeze(0)\n",
    "#         embeddings        = torch.concatenate([embeddings[i] for i in range(embeddings.size(0))])\n",
    "        \n",
    "        \n",
    "        embedded_sequences[name] = embeddings.permute(0, 2, 1)\n",
    "\n",
    "  \n",
    "    \n",
    "    # Add 0 Padding along embedding dim to homogenize shape of inputs\n",
    "    longest_padding       = max([i.size(2) for i in embedded_sequences.values()])\n",
    "    embedded_sequences    = {name: F.pad(tensor, (longest_padding - tensor.size(2), 0),\n",
    "                                               mode = 'constant', value = 0).permute(0, 2, 1)\n",
    "                                   for name, tensor in embedded_sequences.items()}\n",
    "\n",
    "    \n",
    "    return embedded_sequences\n",
    "\n",
    "embedded_sequences = create_protein_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQC94Bs_M_NZ"
   },
   "source": [
    "**Download Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PJzQFTECNBTy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session                 import s3_input, Session\n",
    "import io\n",
    "\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMboHhnhNNGp"
   },
   "source": [
    "**Retrieve Files from s3**\n",
    "1. Fun OSError may have to expand the local s3 space to work in this notebook\n",
    "2. SSH key Passphrase and GITHUB Secret Key: Y5d7fp32!@\n",
    "3. SSH Key GITHUB: SHA256:KFfeCtQipma5pu6OqX3BMrtckwIjZqqg6HvtqN7fdiI alexxaeljimenez@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_in_batches(file_path, batch_size):\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    for i in range(0, num_row_groups, batch_size):\n",
    "        yield parquet_file.read_row_groups(range(i, min(i + batch_size, num_row_groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zVJ7PakpNTnx"
   },
   "outputs": [],
   "source": [
    "# Retrieve Files From S3 Bucket\n",
    "def load_s3_files(bucket_name  = 'leashbio-kaggle',\n",
    "                  key          = 'leash-BELKA.zip',\n",
    "                  batch_data   = True):\n",
    "    \n",
    "    session     = boto3.Session()\n",
    "    s3          = session.client('s3')\n",
    "    obj         = s3.get_object(Bucket=bucket_name, Key=key)    \n",
    "    data        = obj['Body'].read()\n",
    "\n",
    "    stream      = obj['Body']\n",
    "    # Load the data into a bytes buffer\n",
    "    data_stream = io.BytesIO(data)\n",
    "    \n",
    "    \n",
    "    chunk_size  = 10 * 1024 * 1024\n",
    "    max_read    = 5 * 1024 * 1024 * 1024  # 5 GB\n",
    "    buffer      = io.BytesIO()\n",
    "    \n",
    "    dataframes  = {}\n",
    "    # The 'data_stream' is our zip file loaded into memory\n",
    "    with ZipFile(data_stream, 'r') as zip_ref:\n",
    "        # List all files contained in the zip\n",
    "        list_of_files = zip_ref.namelist()\n",
    "        print(\"Files in zip:\", list_of_files)\n",
    "        \n",
    "        total_read = 0\n",
    "        # Optionally, process each file within the zip\n",
    "        for file_name in list_of_files:\n",
    "            # Open the file\n",
    "            \n",
    "            if file_name.endswith('.parquet'):\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                     # Read the stream in chunks until we reach approximately 5 GB\n",
    "                    print(file_name)\n",
    "                    # If you need to process a parquet file, load it into pandas (example)\n",
    "                    if batch_data:\n",
    "                        batch_size = 100  # Adjust the batch size as needed\n",
    "\n",
    "                        # Process each batch\n",
    "                        for batch in read_parquet_in_batches(file_path, batch_size):\n",
    "                            df = batch.to_pandas()\n",
    "                            # Process your DataFrame here\n",
    "                            \n",
    "                        \n",
    "                    else:\n",
    "                        df    = pd.read_parquet(file)\n",
    "                        dataframes[file_name] = df\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "            else:\n",
    "                pass\n",
    "                    # For other file types, read the contents directly\n",
    "#                     content = file.read()\n",
    "#                     print(f\"Contents of {file_name}:\", content[:100])  # Show first 100 characters\n",
    "\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# s3.meta.client.upload_file('leash-BELKA.zip', bucket_name, 'leash-BELKA.zip')\n",
    "\n",
    "# Load with m5.12xlarge instance to load all the data into memory\n",
    "# dataframes = load_s3_files()\n",
    "\n",
    "def open_pickle_files(class_0_size = 1500):\n",
    "    ''' \n",
    "    Load the full test and partial train data\n",
    "    '''\n",
    "    test_data             = pickle.load(open('test_df.pckl', 'rb'))\n",
    "    train_truncated       = pickle.load(open('train_df_truncated.pckl', 'rb'))\n",
    "    \n",
    "    train_bind            = train_truncated[train_truncated['binds']== 1]\n",
    "    train_no              = train_truncated[train_truncated['binds'] == 0][:class_0_size]\n",
    "    \n",
    "    train_data            = pd.concat([train_bind, train_no])\n",
    "    train_data            = train_data.reset_index()\n",
    "    \n",
    "    train_data            = train_data.drop(columns = ['index'])\n",
    "    \n",
    "    return test_data, train_truncated\n",
    "\n",
    "# class_0_size = 1500\n",
    "\n",
    "# class_1_size          = 2576\n",
    "test_data, train_data = open_pickle_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update this next**\n",
    "1. Still need to process 1,500,000 to end of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_smiles(output_name = 'test_processed_batched', batch_size = 100000, start_index = 0):\n",
    "    ''' \n",
    "    Preprocess smiles in batches and save to files\n",
    "    \n",
    "    Parameters:\n",
    "    - output_name:\n",
    "    - batch_size: \n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \n",
    "    '''\n",
    "    smiles_group = {}\n",
    "    batches      = math.ceil(len(test_data) / batch_size)\n",
    "    train_smiles = list(test_data['molecule_smiles'])\n",
    "    index        = start_index\n",
    "    index_json   = start_index * batch_size\n",
    "    \n",
    "#     print(start_index)\n",
    "    for batch in range(start_index, batches):\n",
    "        print(f'Executing Batch {batch + 1}')\n",
    "        \n",
    "        if batch <= batches-1:\n",
    "            train_batch = train_smiles[batch*batch_size:(batch + 1)*batch_size]   \n",
    "        else:\n",
    "            train_batch = train_smiles[batch*batch_size:]\n",
    "\n",
    "        train_batch     = Datapreprocess(train_batch)\n",
    "        train_adjacency, graph_tokens  = train_batch.get_processed_data()\n",
    "        \n",
    "        # JSON File are being dynamically updated now\n",
    "#         token_dictionary = preprocess.token_dictionary\n",
    "\n",
    "        \n",
    "        pickle_file      = output_name + f'_Batch_{batch}.pckl'\n",
    "        pickle_file      = pickle.dump(train_adjacency, open(pickle_file, 'wb'))\n",
    "        \n",
    "#         if os.path.exists(output_name + '.h5'):\n",
    "#             mode = 'a'\n",
    "#         else:\n",
    "#             mode = 'w'\n",
    "            \n",
    "# #         pickle.dump(train_adjacency, open(pickle_file, 'wb'))\n",
    "#        # Write to HDF5 file with gzip compression\n",
    "#         index        = 0\n",
    "#         with h5py.File(f'{output_name}.h5', mode) as hdf:\n",
    "#             hdf.create_dataset(f\"Test_Preprocessed_B{batch}\", (len(train_adjacency),), compression='gzip')\n",
    "            \n",
    "#             for array in train_adjacency:\n",
    "#                 # Store each array with gzip compression\n",
    "#                 index += 1\n",
    "               \n",
    "#             print('Exited H5 Writing')\n",
    "            \n",
    "            \n",
    "#         dt      = h5py.vlen_dtype(np.dtype('float64'))  # Vlen dtype for variable-length arrays of floats\n",
    "#         dataset = hdf.create_dataset('var_length_arrays', (len(arrays),), dtype=dt)\n",
    "\n",
    "#         # Store each array in the dataset\n",
    "#         for i, arr in enumerate(arrays):\n",
    "#             dataset[i] = arr\n",
    "            \n",
    "#         json_file        = output_name + '.json' \n",
    "#         if os.path.exists(json_file):\n",
    "#             mode = 'a'\n",
    "#         else:\n",
    "#             mode = 'w'\n",
    "            \n",
    "# #         # Write Vocab Tokens to Json File to be loaded later\n",
    "#         with open(json_file, mode, buffering=1) as file:\n",
    "            \n",
    "#             for key, value in graph_tokens.items():\n",
    "#                 # Serialize and write each key-value pair as a JSON string per line\n",
    "                \n",
    "#                 json_str = json.dumps({index_json: value})\n",
    "#                 file.write(json_str + \"\\n\")\n",
    "#                 index_json += 1\n",
    "#             print('Exited Json Writing')\n",
    "               \n",
    "    return \n",
    "    \n",
    "\n",
    "    \n",
    "# batch_size                = 100000\n",
    "# create_smiles(output_name = 'test_preprocessed', batch_size = batch_size, start_index = 16)\n",
    "        \n",
    "# for \n",
    "# test_data['molecule_smiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opening H5py File Intermediate Stopping INdexes**\n",
    "1. 1455892\n",
    "2. 1499407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    # Get the memory usage statistics\n",
    "    memory           = psutil.virtual_memory()\n",
    "    gb_conversion    = 1024 ** 3\n",
    "    total_memory     = memory.total / gb_conversion\n",
    "    used_memory      = memory.used / gb_conversion\n",
    "    available_memory = memory.available / gb_conversion\n",
    "    \n",
    "#     print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "#     print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "    print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab_tokens(base_path, file_length = 1500000):\n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - base_path: \n",
    "    - final_index:\n",
    "    \n",
    "    Returns:\n",
    "    - vocab_tokens: \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    vocab_tokens  = {}\n",
    "    base_path     = base_path + '.txt'\n",
    "    \n",
    "    with open(base_path, 'r') as txt:\n",
    "        for index, line in tqdm(enumerate(txt), total = file_length):\n",
    "            dict_line           = ast.literal_eval(line.strip())\n",
    "            vocab_tokens.update(dict_line)  \n",
    "#             if index == (final_index - 1):\n",
    "#                 break\n",
    "    \n",
    "    \n",
    "    return vocab_tokens\n",
    "\n",
    "# vocab_tokens = load_vocab_tokens(base_path = 'test_preprocessed', final_index = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(base_path, index = None, mode = 'batch'):\n",
    "    \n",
    "    '''\n",
    "    Opening the prepocessed files with memory statistics\n",
    "    to understand the memory usage needed\n",
    "    \n",
    "    Parameters\n",
    "    - base_path:\n",
    "    - batches:\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - data_batches; \n",
    "    \n",
    "    '''\n",
    "    global data_batches\n",
    "    \n",
    "    data_batches                = {}\n",
    "    \n",
    "    if mode == 'all':\n",
    "        for index in tqdm(range(batches), total = batches):\n",
    "            get_memory_usage()\n",
    "            t1                  = time.time()\n",
    "            batch_path          = base_path + f'_{index}.pckl'\n",
    "            train_batch         = pickle.load(open(batch_path, 'rb'))\n",
    "            data_batches[index] = train_batch\n",
    "            \n",
    "            t2                  = time.time()\n",
    "        \n",
    "        print('Time Elapsed:', t2 - t1)\n",
    "        \n",
    "    elif mode == 'batch' and index is not None:\n",
    "            get_memory_usage()\n",
    "            batch_path          = base_path + f'_{index}.pckl'\n",
    "            data_batches        = pickle.load(open(batch_path, 'rb'))\n",
    "        \n",
    "    return data_batches\n",
    "\n",
    "# data_batches = load_embeddings(base_path = 'Batched Data/test_preprocessed_Batch', index = 0)\n",
    "# data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess                     = Datapreprocess(list(train_data['molecule_smiles']))\n",
    "# train_adjacency, graph_tokens  = preprocess.get_processed_data()\n",
    "\n",
    "# token_dictionary               = preprocess.token_dictionary\n",
    "# train_adjacency  = torch.tensor(train_adjacency, dtype = torch.float32)\n",
    "# train_adjacency  = train_adjacency.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Step**\n",
    "1. Pass an embedded version of the protein sequence into a parellel nueral network\n",
    "2. Combine the outputs of whatever network we decide before the full connected layer \n",
    "3. Stack a full connected layer on top of the concatenated out put and train neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(train_adjacency, graph_tokens, targets, test_size = 0.2, validation_split = 0.5):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    graph_tokens            =   list(graph_tokens.values())\n",
    "    train_features          =   (train_adjacency, graph_tokens)\n",
    "    \n",
    "    x_train_adj, x_test_adj, y_train, y_test = train_test_split(train_adjacency, targets,\n",
    "                                       test_size = test_size, random_state = 42, stratify = targets)\n",
    "    \n",
    "    x_train_g, x_test_g, _, _          = train_test_split(graph_tokens, targets,\n",
    "                                       test_size = test_size, random_state = 42, stratify = targets)\n",
    "    \n",
    "#     x_train = np.concatenate([x_train_adj, x_train_g], dim = 1)\n",
    "#     x_test  = np.concatenate([])\n",
    "    \n",
    "    return x_train_adj, x_test_adj, x_train_g, x_test_g, y_train, y_test\n",
    "\n",
    "# x_train_adj, x_test_adj, x_train_g, x_test_g, y_train, y_test = train_split(train_adjacency, graph_tokens, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader  # Correct import\n",
    "\n",
    "\n",
    "def create_gcn_data(train_adjacency, graph_tokens, batch_size = None, batch_index = None, targets = None, mode = 'Evaluate'):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    - train_adjacency:\n",
    "    - graph_tokens:\n",
    "    - targets:\n",
    "    \n",
    "    Returns\n",
    "    - final_data:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    graphs         = []\n",
    "    feature_size   = 0\n",
    "    if mode == 'Evaluate':\n",
    "        for index in range(len(train_adjacency)):\n",
    "\n",
    "            adj_matrix = train_adjacency[index]\n",
    "            row, col   = np.where(adj_matrix > 0)\n",
    "\n",
    "            # Bond information should flow both ways in graph\n",
    "            bi_message = [[row, col], [col, row]]\n",
    "            edge_index = torch.tensor(bi_message, dtype=torch.long)\n",
    "\n",
    "\n",
    "            # Expand dim so that dim =1 can be set in the model definition\n",
    "\n",
    "            # Vocab tokens were exported as 1-1,500,000\n",
    "            str_index  = str(index + (batch_size * batch_index))\n",
    "            x_features = torch.tensor(graph_tokens[str_index], dtype = torch.long)\n",
    "            x_features = x_features.unsqueeze(1)\n",
    "\n",
    "            if targets is not None:\n",
    "                y_data     = targets[index]\n",
    "                graph      = Data(x=x_features, edge_index=edge_index, y = y_data)\n",
    "            else:\n",
    "                graph      = Data(x=x_features, edge_index=edge_index)\n",
    "\n",
    "            graphs.append(graph)\n",
    "\n",
    "    elif mode == 'Build':\n",
    "        for index in range(len(train_adjacency)):\n",
    "\n",
    "            adj_matrix   = train_adjacency[index]\n",
    "            row, col     = np.where(adj_matrix > 0)\n",
    "            # Bond information should flow both ways in graph\n",
    "            bi_message   = [[row, col], [col, row]]\n",
    "            edge_index   = torch.tensor(bi_message, dtype=torch.long)\n",
    "\n",
    "            # Expand dim so that dim = 1 can be set in the model definition\n",
    "            x_features   = torch.tensor(graph_tokens[index], dtype = torch.long)\n",
    "            x_features   = x_features.unsqueeze(1)\n",
    " \n",
    "            \n",
    "            if targets is not None:\n",
    "                y_data   = targets[index]\n",
    "                graph    = Data(x=x_features, edge_index=edge_index, y = y_data)\n",
    "            \n",
    "            else:\n",
    "                graph    = Data(x=x_features, edge_index=edge_index)\n",
    "           \n",
    "            graphs.append(graph)\n",
    "\n",
    "#     print(feature_size)\n",
    "    final_data = DataLoader(graphs, batch_size = 32)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "\n",
    "def load_test_proteins(data_batch, batch_size, index, vocab_tokens, mode = 'Build', test = False, data = None):\n",
    "\n",
    "    if mode == 'Build':\n",
    "        proteins    = data['protein_name']#[:batch_size]\n",
    "        proteins    = list(proteins.apply(lambda x: embedded_sequences[x]))\n",
    "        proteins    = torch.concatenate([p for p in proteins], axis = 0)\n",
    "        targets     = torch.tensor(data['binds'], dtype = torch.long)\n",
    "        \n",
    "        if not test:\n",
    "            final_data  = create_gcn_data(data_batch, vocab_tokens, targets = targets, mode = mode)\n",
    "        else:\n",
    "            final_data  = create_gcn_data(data_batch, vocab_tokens, targets = None, mode = mode)\n",
    "\n",
    "    elif mode == 'Evaluate':\n",
    "        proteins      = test_data['protein_name'][index*batch_size:(index + 1)*batch_size]\n",
    "        proteins      = list(proteins.apply(lambda x: embedded_sequences[x]))\n",
    "        proteins      = torch.concatenate([p.unsqueeze(0) for p in proteins], axis = 0)\n",
    "        final_data    = create_gcn_data(data_batch, vocab_tokens, batch_size, index, mode = mode)\n",
    "    \n",
    "    return final_data, proteins\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left off here Need to re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, GCNConv, global_mean_pool\n",
    "from torch.nn import LazyLinear, Transformer, Conv1d, Dropout, BatchNorm1d\n",
    "\n",
    "# Example GCN model definition\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 hidden_dim = 256,\n",
    "                 protein_size = 1024, \n",
    "                 kernel_size = 3):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        padding_size      = kernel_size - 2\n",
    "        \n",
    "        # Layer Number Parameters\n",
    "        self.input_dim    = input_dim\n",
    "        self.hidden_dim   = hidden_dim\n",
    "        self.dim_p1       = 512\n",
    "        self.num_layers   = 4\n",
    "        \n",
    "        \n",
    "        # Convolutional Number Parameters:\n",
    "        self.protein_size = protein_size\n",
    "        self.protein_dim1 = protein_size // 2\n",
    "        self.protein_dim2 = protein_size // 4\n",
    "        self.protein_dim3 = protein_size // 8\n",
    "        self.concat_dim   = self.protein_dim2 + (hidden_dim * 2)\n",
    "        \n",
    "        \n",
    "        # Molecular Graph Layers\n",
    "        self.conv1        = GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1          = BatchNorm1d(hidden_dim)\n",
    "        self.dropout1     = Dropout(0.4)\n",
    "        self.conv2        = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2          = BatchNorm1d(hidden_dim)\n",
    "        self.dropout2     = Dropout(0.4)\n",
    "        \n",
    "        # Protein NN Layers\n",
    "        self.prot_conv1   = Conv1d(protein_size, self.protein_dim1, kernel_size, padding = padding_size)\n",
    "        self.prot_conv2   = Conv1d(self.protein_dim1, self.protein_dim2, kernel_size)\n",
    "#         self.prot_conv3   = Conv1d(self.protein_dim2, self.protein_dim3, kernel_size)\n",
    "        \n",
    "        \n",
    "        #         self.p_fc1        = LazyLinear(self.dim_p1)     \n",
    "        #         self.p_fc2        = LazyLinear(64)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Joined NN Layers\n",
    "#         self.tf1          = Transformer(d_model = self.concat_dim)\n",
    "        self.fc1          = LazyLinear(256)\n",
    "        self.fc2          = LazyLinear(128)\n",
    "        self.fc3          = LazyLinear(64)\n",
    "        self.out          = LazyLinear(1)\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, data, protein_data):\n",
    "        x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "        \n",
    "        # Molecular Graph\n",
    "        x            = x.float()\n",
    "        x            = self.dropout1(F.relu(self.bn1(self.conv1(x, edge_index))))\n",
    "        x            = self.dropout2(F.relu(self.bn2(self.conv2(x, edge_index))))\n",
    "#         x            = self.GAT1(x, edge_index)\n",
    "        x            = global_mean_pool(x, batch)\n",
    "        \n",
    "        \n",
    "        # Protein NN\n",
    "#         protein_data = self.p_fc1(protein_data)\n",
    "#         protein_data = self.p_fc2(protein_data)\n",
    "        protein_data = protein_data.permute(0, 2, 1)\n",
    "        \n",
    "        protein_data = F.relu(self.prot_conv1(protein_data))\n",
    "        protein_data = F.relu(self.prot_conv2(protein_data))\n",
    "#         protein_data = F.relu(self.prot_conv3(protein_data))\n",
    "        \n",
    "        # FLatten to (batch_size, features)\n",
    "        protein_data = protein_data.reshape(protein_data.size(0), -1)\n",
    "        \n",
    "        # Joined NN\n",
    "        x            = torch.concatenate([x, protein_data], axis = 1)\n",
    "#         x            = F.sigmoid(self.tf1(x, tgt = y))\n",
    "        x            = F.relu(self.fc1(x))\n",
    "        x            = F.relu(self.fc2(x))\n",
    "        x            = F.relu(self.fc3(x))\n",
    "        x            = F.sigmoid(self.out(x))\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, GCNConv, global_mean_pool\n",
    "from torch.nn import LazyLinear, Transformer, Conv1d, Dropout, BatchNorm1d\n",
    "\n",
    "# Example GCN model definition\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 hidden_dim = 256,\n",
    "                 protein_size = 1024, \n",
    "                 kernel_size = 3):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        padding_size      = kernel_size - 2\n",
    "        \n",
    "        # Layer Number Parameters\n",
    "        self.input_dim    = input_dim\n",
    "        self.hidden_dim   = hidden_dim\n",
    "        self.dim_p1       = 256\n",
    "        self.num_layers   = 4\n",
    "        \n",
    "        \n",
    "        # Convolutional Number Parameters:\n",
    "#         self.concat_dim   = self.protein_dim2 + (hidden_dim * 2)\n",
    "        \n",
    "        \n",
    "        # Molecular Graph Layers\n",
    "        self.conv1        = GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1          = BatchNorm1d(hidden_dim)\n",
    "        self.dropout1     = Dropout(0.4)\n",
    "        self.conv2        = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2          = BatchNorm1d(hidden_dim)\n",
    "        self.dropout2     = Dropout(0.4)\n",
    "        \n",
    "        # Protein NN Layers\n",
    "        self.p_fc1        = LazyLinear(self.dim_p1)     \n",
    "        self.p_fc2        = LazyLinear(64)\n",
    "\n",
    "        # Joined NN Layers\n",
    "        self.fc1          = LazyLinear(1)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, data, protein_data):\n",
    "        x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "        \n",
    "        # Molecular Graph\n",
    "        x            = x.float()\n",
    "        x            = self.dropout1(F.relu(self.bn1(self.conv1(x, edge_index))))\n",
    "        x            = self.dropout2(F.relu(self.bn2(self.conv2(x, edge_index))))\n",
    "#         x            = self.GAT1(x, edge_index)\n",
    "        x            = global_mean_pool(x, batch)\n",
    "        \n",
    "        \n",
    "        # Protein NN\n",
    "\n",
    "        protein_data = protein_data.reshape(protein_data.size(0), - 1)\n",
    "        protein_data = self.p_fc1(protein_data)\n",
    "        protein_data = self.p_fc2(protein_data)\n",
    "        \n",
    "        # Joined NN\n",
    "        x            = torch.concatenate([x, protein_data], axis = 1)\n",
    "        x            = F.sigmoid(self.fc1(x))\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write code to check for exploding gradients** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):\n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, \n",
    "                 max_epochs,\n",
    "                 bypass_prepare = False,\n",
    "                 num_gpus=0, \n",
    "                 gradient_clip_val=0, \n",
    "                 batch_size = 32):\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.bypass_prepare     = bypass_prepare\n",
    "        self.max_epochs         = max_epochs\n",
    "        self.batch_size         = 32\n",
    "        # assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, train_data, val_data):\n",
    "        train_dataloader       = DataLoader(train_data, batch_size = self.batch_size)\n",
    "        if val_data is not None:\n",
    "            val_dataloader     = DataLoader(val_data, batch_size = self.batch_size)\n",
    "        else:\n",
    "            val_dataloader     = None\n",
    "            \n",
    "        self.num_train_batches = len(train_dataloader)\n",
    "        self.num_val_batches   = (len(val_dataloader)\n",
    "                                if val_dataloader is not None else 0)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "#     def prepare_model(self, model, optimizer, loss_fn):\n",
    "#         # model.trainer = self\n",
    "#         # model.board.xlim = [0, self.max_epochs]\n",
    "#         self.model     = model\n",
    "#         self.optimizer = optimizer\n",
    "#         self.loss_fn   = loss_fn\n",
    "\n",
    "    def fit(self, \n",
    "            model,\n",
    "            train_data, \n",
    "            other_data   = None,\n",
    "            val_data     = None, \n",
    "            optimizer    = None, \n",
    "            loss_fn      = None, \n",
    "            lr_scheduler = None):\n",
    "        \n",
    "        self.model           = model\n",
    "        self.optimizer       = optimizer\n",
    "        self.loss_fn         = loss_fn\n",
    "        self.lr_scheduler    = lr_scheduler\n",
    "        \n",
    "        # Bass other data (Placeholder for now 05May2024)\n",
    "        if other_data is not None:\n",
    "            self.supplement_data = other_data\n",
    "   \n",
    "        if not self.bypass_prepare:\n",
    "            self.train_dataloader, self.val_dataloader = self.prepare_data(train_data, val_data)\n",
    "            loader_type      = None\n",
    "            \n",
    "        else:\n",
    "            self.train_dataloader, self.val_dataloader = train_data, val_data\n",
    "            loader_type  = 'Graph'\n",
    "        \n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx   = 0\n",
    "        for self.epoch in tqdm(range(self.max_epochs), total = self.max_epochs):\n",
    "            self.fit_epoch(loader_type = loader_type, p_data = self.supplement_data)\n",
    "            print(f'Epoch {self.epoch + 1} LOSS: {self.loss.detach().numpy()}')\n",
    "            \n",
    "    def fit_epoch(self, loader_type = 'Graph', p_data = None, gradients = True):\n",
    "        \n",
    "        if loader_type == 'Graph':\n",
    "            for index, data in enumerate(self.train_dataloader):\n",
    "                targets     = data.y.unsqueeze(1).float()\n",
    "                p_batch     = p_data[index*self.batch_size:(index + 1)*self.batch_size]\n",
    "                predictions = self.model.forward(data, p_batch)\n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                self.loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "#             if gradients:\n",
    "#                 for name, param in model.named_parameters():\n",
    "#                     if param.requires_grad:\n",
    "#                         print(f\"Gradients for {name}: {param.grad}\")\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "        if loader_type != 'Graph':\n",
    "            for features, targets in self.train_dataloader:\n",
    "                predictions = self.model.forward(features)\n",
    "                \n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                self.loss.backward()\n",
    "                \n",
    "            \n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                # Left off here trying to fix the generalizatin of trainer 05May2024\n",
    "                if self.val_dataloader is not None:\n",
    "                  with torch.no_grad():\n",
    "                    for val_features, val_targets in self.val_dataloader:\n",
    "                      val_predictions = model.forward(val_features)\n",
    "                      val_loss        = self.loss_fn(val_predictions, val_targets)\n",
    "\n",
    "                      print(f'Epoch {self.epoch} w/ Loss: ', val_loss)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left Off here... The dimension of each dataBatch are not agreeing with batch_size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_build(train_data, batch_size = 100000, batch_loading = True):\n",
    "    \n",
    "    ''' \n",
    "    Preprocessing build funtion for batch preprocessing, loading,\n",
    "    and evaluating of test_data\n",
    "    \n",
    "    Parameters:\n",
    "    - \n",
    "    - \n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    - \n",
    "    \n",
    "    '''\n",
    "    index       = 0 \n",
    "    batches     = math.ceil(len(train_data) / batch_size)\n",
    "    \n",
    "    if batch_loading:\n",
    "        for batch in range(batches):\n",
    "\n",
    "            batch_start  = index*batch_size\n",
    "            batch_end    = (index + 1)*batch_size\n",
    "            batch_subset = train_data[batch_start: batch_end]['']\n",
    "\n",
    "            train_batch                    = Datapreprocess(batch_subset)\n",
    "            train_adjacency, graph_tokens  = train_batch.get_processed_data()\n",
    "            final_test, proteins           = load_test_proteins(data_batch = data_batch,\n",
    "                                             batch_size = batch_size, \n",
    "                                             index = index, \n",
    "                                             vocab_tokens = vocab_tokens,\n",
    "                                             mode = 'Build')\n",
    "            \n",
    "            break\n",
    "#         data_batches    = load_embeddings(base_path = 'Batched Data/test_preprocessed_Batch', index = index)\n",
    "            \n",
    "    else:\n",
    "        train_bind                       = train_data[train_data['binds'] == 1].dropna()\n",
    "        train_no_bind                    = train_data[train_data['binds'] == 0].dropna()[:batch_size] \n",
    "        train_input                      = pd.concat([train_bind, train_no_bind], axis = 0)\n",
    "        train_input                      = train_input.reset_index()\n",
    "        train_input                      = train_input.drop(columns = ['index'])\n",
    "        \n",
    "        targets                          = torch.tensor(train_input['binds'], dtype = torch.long)\n",
    "        train_batch                      = Datapreprocess(train_input['molecule_smiles'])\n",
    "        data_batch, vocab_tokens         = train_batch.get_processed_data()\n",
    "        \n",
    "        # Split Train and Test Data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data_batch, targets, stratify = targets, random_state = 42)\n",
    "        train_input, test_input, _, _    = train_test_split(train_input, targets, stratify = targets, random_state = 42)\n",
    "        train_token, test_token, _, _    = train_test_split(vocab_tokens, targets, stratify = targets, random_state = 42)\n",
    "        \n",
    "        train_input                      = train_input.reset_index()\n",
    "        train_input                      = train_input.drop(columns = ['index'])\n",
    "        test_input                       = test_input.reset_index()\n",
    "        test_input                       = test_input.drop(columns = ['index'])\n",
    "        \n",
    "        # Load Final Model Inputs\n",
    "        final_train, x_train_p           = load_test_proteins(data_batch = x_train,\n",
    "                                                         batch_size      = batch_size, \n",
    "                                                         index           = index, \n",
    "                                                         vocab_tokens    = train_token,\n",
    "                                                         mode            = 'Build',\n",
    "                                                         data            = train_input)\n",
    "\n",
    "        final_test, x_test_p             = load_test_proteins(data_batch = x_test,\n",
    "                                                         batch_size      = batch_size, \n",
    "                                                         index           = index, \n",
    "                                                         vocab_tokens    = test_token,\n",
    "                                                         mode            = 'Build',\n",
    "                                                         test            = True,\n",
    "                                                         data            = test_input)\n",
    "\n",
    "\n",
    "    return final_train, final_test, x_train_p, x_test_p, y_test\n",
    "\n",
    "# final_train, final_test, x_train_p, x_test_p, y_test = preprocess_build(train_data, batch_size = 25000, batch_loading = False)\n",
    "# final_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Gradients Are Vanishing Confirmed 18May2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:15<04:56, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 LOSS: 0.08679664880037308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:31<04:49, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 LOSS: 0.08716123551130295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:46<04:24, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 LOSS: 0.07026151567697525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:02<04:08, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 LOSS: 0.07404708862304688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:17<03:51, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 LOSS: 0.07860900461673737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:33<03:38, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 LOSS: 0.06768007576465607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:46<03:11, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 LOSS: 0.07928551733493805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:58<02:45, 13.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 LOSS: 0.07813657820224762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [02:09<02:23, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 LOSS: 0.06782238185405731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:23<02:13, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 LOSS: 0.057447440922260284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:41<02:12, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 LOSS: 0.07429199665784836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [02:56<01:58, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 LOSS: 0.08931029587984085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [03:11<01:44, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 LOSS: 0.072553351521492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [03:27<01:30, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 LOSS: 0.0735490471124649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [03:43<01:17, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 LOSS: 0.07027865946292877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:59<01:02, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 LOSS: 0.060541123151779175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [04:16<00:47, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 LOSS: 0.08152768760919571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [04:31<00:31, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 LOSS: 0.07939334213733673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [04:47<00:15, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 LOSS: 0.058918215334415436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:02<00:00, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 LOSS: 0.07547293603420258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def train_model(model, final_train, x_train_p, loss_function):\n",
    "#     batch_size          = 32\n",
    "#     # train_dataset, test_dataset, val_dataset = split_data(padded_sequences, targets)\n",
    "\n",
    "#     # train_dataset = TensorDataset(train_adjacency, targets)\n",
    "#     trainer             = Trainer(max_epochs = 20, bypass_prepare = True, batch_size = batch_size)\n",
    "#     optimizer           = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "#     trainer.fit(model, final_train, x_train_p,  None, optimizer, loss_function)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "class1              = len(train_data[train_data['binds'] == 1].dropna())\n",
    "class0              = len(train_data[train_data['binds'] == 0].dropna()[:15000])\n",
    "weight              = class0 / class1\n",
    "protein_size        = embedded_sequences['sEH'].size(2)\n",
    "\n",
    "\n",
    "# loss_function       = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weight))\n",
    "loss_function     = nn.BCELoss()\n",
    "\n",
    "model               = Model(input_dim    = 1, \n",
    "                            hidden_dim   = 16, \n",
    "                            protein_size = protein_size)\n",
    "\n",
    "\n",
    "batch_size          = 32\n",
    "# train_dataset, test_dataset, val_dataset = split_data(padded_sequences, targets)\n",
    "\n",
    "# train_dataset = TensorDataset(train_adjacency, targets)\n",
    "trainer             = Trainer(max_epochs = 20, bypass_prepare = True, batch_size = batch_size)\n",
    "optimizer           = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler           = lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "trainer.fit(model, final_train, x_train_p,  None, optimizer, loss_function, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_preds       = []\n",
    "raw_preds        = []\n",
    "\n",
    "for index, batch in enumerate(final_test):\n",
    "    p_test       = x_test_p[index*batch_size: (index + 1) * batch_size]\n",
    "    test_results = model(batch, p_test)\n",
    "    raw_prob     = copy.copy(test_results)\n",
    "        \n",
    "    test_results = [1 if x > 0.5 else 0 for x in test_results]\n",
    "    test_preds.append(test_results)\n",
    "    raw_preds.append(raw_prob)\n",
    "    \n",
    "    \n",
    "predictions      = []\n",
    "for sub in test_preds:\n",
    "    for result in sub:\n",
    "        predictions.append(result)\n",
    "\n",
    "probabilities    = []\n",
    "for sub in raw_preds:\n",
    "    for result in sub:\n",
    "        probabilities.append(float(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07195968180894852,\n",
       " 0.1646060049533844,\n",
       " 0.06175100430846214,\n",
       " 0.09463785588741302,\n",
       " 0.11883524060249329,\n",
       " 0.0746956542134285,\n",
       " 0.03984811529517174,\n",
       " 0.08569736033678055,\n",
       " 0.29798346757888794,\n",
       " 0.052208151668310165,\n",
       " 0.07691548019647598,\n",
       " 0.06654912978410721,\n",
       " 0.17173512279987335,\n",
       " 0.020367678254842758,\n",
       " 0.17106719315052032,\n",
       " 0.1606796830892563,\n",
       " 0.033023566007614136,\n",
       " 0.06617739051580429,\n",
       " 0.21835750341415405,\n",
       " 0.03283965215086937,\n",
       " 0.1364310085773468,\n",
       " 0.04126329347491264,\n",
       " 0.12084383517503738,\n",
       " 0.044419270008802414,\n",
       " 0.18116724491119385,\n",
       " 0.10116077214479446,\n",
       " 0.11253836750984192,\n",
       " 0.09861189872026443,\n",
       " 0.0801692008972168,\n",
       " 0.15489192306995392,\n",
       " 0.08785511553287506,\n",
       " 0.22140374779701233,\n",
       " 0.07392258942127228,\n",
       " 0.04424189776182175,\n",
       " 0.02788873016834259,\n",
       " 0.18278257548809052,\n",
       " 0.013732081279158592,\n",
       " 0.07440140843391418,\n",
       " 0.09911154955625534,\n",
       " 0.07019778341054916,\n",
       " 0.07312483340501785,\n",
       " 0.09629794210195541,\n",
       " 0.043390411883592606,\n",
       " 0.026534153148531914,\n",
       " 0.01827356591820717,\n",
       " 0.039045482873916626,\n",
       " 0.49697616696357727,\n",
       " 0.12419470399618149,\n",
       " 0.033320970833301544,\n",
       " 0.048411402851343155,\n",
       " 0.053470440208911896,\n",
       " 0.03193162754178047,\n",
       " 0.23072616755962372,\n",
       " 0.15358717739582062,\n",
       " 0.04157685115933418,\n",
       " 0.039731934666633606,\n",
       " 0.09106110036373138,\n",
       " 0.37782779335975647,\n",
       " 0.04110914468765259,\n",
       " 0.037305474281311035,\n",
       " 0.15215261280536652,\n",
       " 0.06658071279525757,\n",
       " 0.2760941982269287,\n",
       " 0.027966421097517014,\n",
       " 0.03624463453888893,\n",
       " 0.09260280430316925,\n",
       " 0.2473309487104416,\n",
       " 0.0339733362197876,\n",
       " 0.062228765338659286,\n",
       " 0.055709801614284515,\n",
       " 0.10926619917154312,\n",
       " 0.16536666452884674,\n",
       " 0.09325544536113739,\n",
       " 0.014078927226364613,\n",
       " 0.012441331520676613,\n",
       " 0.12682029604911804,\n",
       " 0.08571923524141312,\n",
       " 0.13334746658802032,\n",
       " 0.029224617406725883,\n",
       " 0.1303998827934265,\n",
       " 0.06106159836053848,\n",
       " 0.017978673800826073,\n",
       " 0.10466844588518143,\n",
       " 0.034508444368839264,\n",
       " 0.030144276097416878,\n",
       " 0.12095575779676437,\n",
       " 0.03597071021795273,\n",
       " 0.006196900270879269,\n",
       " 0.10547962039709091,\n",
       " 0.04867851361632347,\n",
       " 0.13679315149784088,\n",
       " 0.06341976672410965,\n",
       " 0.48078060150146484,\n",
       " 0.18176834285259247,\n",
       " 0.16967126727104187,\n",
       " 0.338377982378006,\n",
       " 0.11011224240064621,\n",
       " 0.03340458869934082,\n",
       " 0.10249757021665573,\n",
       " 0.0576634518802166,\n",
       " 0.03598439693450928,\n",
       " 0.1782209873199463,\n",
       " 0.30060702562332153,\n",
       " 0.16388562321662903,\n",
       " 0.037713225930929184,\n",
       " 0.04077300429344177,\n",
       " 0.2566344738006592,\n",
       " 0.023756103590130806,\n",
       " 0.1338300108909607,\n",
       " 0.056289415806531906,\n",
       " 0.03777031973004341,\n",
       " 0.11961331218481064,\n",
       " 0.13302896916866302,\n",
       " 0.12222795188426971,\n",
       " 0.036739468574523926,\n",
       " 0.11489040404558182,\n",
       " 0.05853985249996185,\n",
       " 0.06927207112312317,\n",
       " 0.02082045190036297,\n",
       " 0.06974606215953827,\n",
       " 0.10910941660404205,\n",
       " 0.0268496572971344,\n",
       " 0.08038441091775894,\n",
       " 0.0211833193898201,\n",
       " 0.18197289109230042,\n",
       " 0.04932412877678871,\n",
       " 0.02041683904826641,\n",
       " 0.09531066566705704,\n",
       " 0.1417492926120758,\n",
       " 0.0301045011729002,\n",
       " 0.025441627949476242,\n",
       " 0.09511107206344604,\n",
       " 0.07924274355173111,\n",
       " 0.1088579073548317,\n",
       " 0.1817479282617569,\n",
       " 0.08480557054281235,\n",
       " 0.03936136141419411,\n",
       " 0.04798201471567154,\n",
       " 0.06346949189901352,\n",
       " 0.055932167917490005,\n",
       " 0.021486300975084305,\n",
       " 0.013717570342123508,\n",
       " 0.019496988505125046,\n",
       " 0.21175357699394226,\n",
       " 0.11806286126375198,\n",
       " 0.06395231932401657,\n",
       " 0.09275437146425247,\n",
       " 0.03530694916844368,\n",
       " 0.15373894572257996,\n",
       " 0.11030222475528717,\n",
       " 0.0810052827000618,\n",
       " 0.14176011085510254,\n",
       " 0.18182146549224854,\n",
       " 0.14768309891223907,\n",
       " 0.015604347921907902,\n",
       " 0.14633451402187347,\n",
       " 0.05295203998684883,\n",
       " 0.04032735154032707,\n",
       " 0.11657027900218964,\n",
       " 0.07539643347263336,\n",
       " 0.07062655687332153,\n",
       " 0.018088340759277344,\n",
       " 0.1048334464430809,\n",
       " 0.3124220371246338,\n",
       " 0.02314561977982521,\n",
       " 0.0315733477473259,\n",
       " 0.03798196092247963,\n",
       " 0.1003282219171524,\n",
       " 0.05021722987294197,\n",
       " 0.09679901599884033,\n",
       " 0.16451935470104218,\n",
       " 0.14587298035621643,\n",
       " 0.13419322669506073,\n",
       " 0.02639642357826233,\n",
       " 0.06497665494680405,\n",
       " 0.30705946683883667,\n",
       " 0.16098451614379883,\n",
       " 0.032351840287446976,\n",
       " 0.2782655954360962,\n",
       " 0.06965348869562149,\n",
       " 0.22344176471233368,\n",
       " 0.05518506094813347,\n",
       " 0.21356652677059174,\n",
       " 0.08316043764352798,\n",
       " 0.13365980982780457,\n",
       " 0.09002505987882614,\n",
       " 0.05178782343864441,\n",
       " 0.1567862182855606,\n",
       " 0.08736475557088852,\n",
       " 0.03150644898414612,\n",
       " 0.17500334978103638,\n",
       " 0.15179328620433807,\n",
       " 0.030011441558599472,\n",
       " 0.16319185495376587,\n",
       " 0.3147948980331421,\n",
       " 0.03686133772134781,\n",
       " 0.20147837698459625,\n",
       " 0.17808732390403748,\n",
       " 0.041518595069646835,\n",
       " 0.19552887976169586,\n",
       " 0.0452139675617218,\n",
       " 0.03221646696329117,\n",
       " 0.1793857365846634,\n",
       " 0.08190064132213593,\n",
       " 0.06455375999212265,\n",
       " 0.0928051620721817,\n",
       " 0.06589953601360321,\n",
       " 0.06298962980508804,\n",
       " 0.028594931587576866,\n",
       " 0.1383553445339203,\n",
       " 0.056898050010204315,\n",
       " 0.2104654312133789,\n",
       " 0.05495395511388779,\n",
       " 0.03788973391056061,\n",
       " 0.03313753381371498,\n",
       " 0.05390293896198273,\n",
       " 0.11225864291191101,\n",
       " 0.05250589922070503,\n",
       " 0.07514700293540955,\n",
       " 0.050330858677625656,\n",
       " 0.06178979203104973,\n",
       " 0.08122795075178146,\n",
       " 0.14383675158023834,\n",
       " 0.06336815655231476,\n",
       " 0.12658268213272095,\n",
       " 0.04663759469985962,\n",
       " 0.0728839710354805,\n",
       " 0.02540500834584236,\n",
       " 0.16708362102508545,\n",
       " 0.21567225456237793,\n",
       " 0.13212791085243225,\n",
       " 0.054615240544080734,\n",
       " 0.06099391356110573,\n",
       " 0.09245835989713669,\n",
       " 0.05927743762731552,\n",
       " 0.11722269654273987,\n",
       " 0.0866631269454956,\n",
       " 0.03088817000389099,\n",
       " 0.022131161764264107,\n",
       " 0.18159164488315582,\n",
       " 0.036625880748033524,\n",
       " 0.05819401144981384,\n",
       " 0.06844666600227356,\n",
       " 0.08563917130231857,\n",
       " 0.02724192664027214,\n",
       " 0.16166839003562927,\n",
       " 0.15818284451961517,\n",
       " 0.008882304653525352,\n",
       " 0.06858087331056595,\n",
       " 0.06101270020008087,\n",
       " 0.07071492075920105,\n",
       " 0.13024699687957764,\n",
       " 0.06455819308757782,\n",
       " 0.06526349484920502,\n",
       " 0.07249189913272858,\n",
       " 0.059296637773513794,\n",
       " 0.13298432528972626,\n",
       " 0.06153189018368721,\n",
       " 0.0697125643491745,\n",
       " 0.03564898297190666,\n",
       " 0.03901363164186478,\n",
       " 0.0341043546795845,\n",
       " 0.03378035128116608,\n",
       " 0.08537538349628448,\n",
       " 0.04779558256268501,\n",
       " 0.08610077202320099,\n",
       " 0.014846614561975002,\n",
       " 0.21225963532924652,\n",
       " 0.05767831578850746,\n",
       " 0.19223915040493011,\n",
       " 0.05715130269527435,\n",
       " 0.0428866483271122,\n",
       " 0.14340440928936005,\n",
       " 0.10686805844306946,\n",
       " 0.10590145736932755,\n",
       " 0.02648412436246872,\n",
       " 0.017903337255120277,\n",
       " 0.04387050122022629,\n",
       " 0.07776059955358505,\n",
       " 0.2963784337043762,\n",
       " 0.05320478230714798,\n",
       " 0.08244948834180832,\n",
       " 0.0622275248169899,\n",
       " 0.21052934229373932,\n",
       " 0.13349243998527527,\n",
       " 0.059528812766075134,\n",
       " 0.14057493209838867,\n",
       " 0.06984008848667145,\n",
       " 0.11464790999889374,\n",
       " 0.07249505072832108,\n",
       " 0.17227166891098022,\n",
       " 0.039650075137615204,\n",
       " 0.07289891690015793,\n",
       " 0.0586848258972168,\n",
       " 0.060442302376031876,\n",
       " 0.08702436089515686,\n",
       " 0.10011979192495346,\n",
       " 0.08695176243782043,\n",
       " 0.2630575895309448,\n",
       " 0.07052475214004517,\n",
       " 0.2689601480960846,\n",
       " 0.16890184581279755,\n",
       " 0.18085409700870514,\n",
       " 0.13560618460178375,\n",
       " 0.04004890099167824,\n",
       " 0.04804310202598572,\n",
       " 0.05674475058913231,\n",
       " 0.07792196422815323,\n",
       " 0.04990292340517044,\n",
       " 0.03351800516247749,\n",
       " 0.06232169643044472,\n",
       " 0.02728559449315071,\n",
       " 0.06410824507474899,\n",
       " 0.0489841066300869,\n",
       " 0.0421346016228199,\n",
       " 0.07950737327337265,\n",
       " 0.10132084041833878,\n",
       " 0.03348347544670105,\n",
       " 0.060622695833444595,\n",
       " 0.05510244145989418,\n",
       " 0.03508584201335907,\n",
       " 0.1063309833407402,\n",
       " 0.050226256251335144,\n",
       " 0.009493682533502579,\n",
       " 0.13961119949817657,\n",
       " 0.03987099975347519,\n",
       " 0.07527197152376175,\n",
       " 0.07709764689207077,\n",
       " 0.24314501881599426,\n",
       " 0.08941905200481415,\n",
       " 0.11595089733600616,\n",
       " 0.3018346428871155,\n",
       " 0.0836506262421608,\n",
       " 0.04813147336244583,\n",
       " 0.07121865451335907,\n",
       " 0.044391557574272156,\n",
       " 0.01907811500132084,\n",
       " 0.10853211581707001,\n",
       " 0.1435631364583969,\n",
       " 0.0322309285402298,\n",
       " 0.054919395595788956,\n",
       " 0.010992881841957569,\n",
       " 0.057157933712005615,\n",
       " 0.06912980228662491,\n",
       " 0.07080279290676117,\n",
       " 0.16128034889698029,\n",
       " 0.3475359082221985,\n",
       " 0.03601929917931557,\n",
       " 0.08801659941673279,\n",
       " 0.1214989572763443,\n",
       " 0.05487547814846039,\n",
       " 0.07044082880020142,\n",
       " 0.14595621824264526,\n",
       " 0.13543912768363953,\n",
       " 0.04609712213277817,\n",
       " 0.24111980199813843,\n",
       " 0.12738727033138275,\n",
       " 0.1006111353635788,\n",
       " 0.07043091207742691,\n",
       " 0.048854101449251175,\n",
       " 0.09193473309278488,\n",
       " 0.12943610548973083,\n",
       " 0.033422134816646576,\n",
       " 0.07354306429624557,\n",
       " 0.08397861570119858,\n",
       " 0.1427941769361496,\n",
       " 0.061472974717617035,\n",
       " 0.04537355899810791,\n",
       " 0.0873008519411087,\n",
       " 0.0667385458946228,\n",
       " 0.04593788459897041,\n",
       " 0.05892438814043999,\n",
       " 0.15168733894824982,\n",
       " 0.08138719946146011,\n",
       " 0.025982994586229324,\n",
       " 0.0721176341176033,\n",
       " 0.15129747986793518,\n",
       " 0.06470994651317596,\n",
       " 0.3279587924480438,\n",
       " 0.04666287451982498,\n",
       " 0.11443760991096497,\n",
       " 0.07448270171880722,\n",
       " 0.09001672267913818,\n",
       " 0.059240084141492844,\n",
       " 0.025743378326296806,\n",
       " 0.06781230866909027,\n",
       " 0.06309204548597336,\n",
       " 0.06473782658576965,\n",
       " 0.3422028720378876,\n",
       " 0.06641646474599838,\n",
       " 0.0792129710316658,\n",
       " 0.15367764234542847,\n",
       " 0.1674300581216812,\n",
       " 0.12116961181163788,\n",
       " 0.05834602564573288,\n",
       " 0.09900388866662979,\n",
       " 0.032864924520254135,\n",
       " 0.06682472676038742,\n",
       " 0.1356411725282669,\n",
       " 0.07717780023813248,\n",
       " 0.07786715030670166,\n",
       " 0.11293579638004303,\n",
       " 0.18562325835227966,\n",
       " 0.08213839679956436,\n",
       " 0.0376146137714386,\n",
       " 0.08366899937391281,\n",
       " 0.09120839834213257,\n",
       " 0.03752405568957329,\n",
       " 0.02629883773624897,\n",
       " 0.02233138494193554,\n",
       " 0.031399376690387726,\n",
       " 0.036625124514102936,\n",
       " 0.04520072042942047,\n",
       " 0.17767879366874695,\n",
       " 0.2141999453306198,\n",
       " 0.014972853474318981,\n",
       " 0.04192065820097923,\n",
       " 0.14717227220535278,\n",
       " 0.0883883684873581,\n",
       " 0.006010374985635281,\n",
       " 0.1941625326871872,\n",
       " 0.0724247395992279,\n",
       " 0.035955410450696945,\n",
       " 0.04616552218794823,\n",
       " 0.012848086655139923,\n",
       " 0.16104350984096527,\n",
       " 0.08753121644258499,\n",
       " 0.08147735148668289,\n",
       " 0.04963520169258118,\n",
       " 0.031320177018642426,\n",
       " 0.15920735895633698,\n",
       " 0.016978178173303604,\n",
       " 0.03981052339076996,\n",
       " 0.2494545876979828,\n",
       " 0.07637712359428406,\n",
       " 0.036185555160045624,\n",
       " 0.2514370083808899,\n",
       " 0.092476487159729,\n",
       " 0.15436111390590668,\n",
       " 0.1862652599811554,\n",
       " 0.06916497647762299,\n",
       " 0.04015661031007767,\n",
       " 0.08169902116060257,\n",
       " 0.14980469644069672,\n",
       " 0.05948728695511818,\n",
       " 0.05012238398194313,\n",
       " 0.058602482080459595,\n",
       " 0.05990660563111305,\n",
       " 0.1395752876996994,\n",
       " 0.08170932531356812,\n",
       " 0.23491047322750092,\n",
       " 0.027942098677158356,\n",
       " 0.132001593708992,\n",
       " 0.05963616445660591,\n",
       " 0.04364864528179169,\n",
       " 0.12308673560619354,\n",
       " 0.1259191632270813,\n",
       " 0.08823081105947495,\n",
       " 0.07248296588659286,\n",
       " 0.04575753211975098,\n",
       " 0.027272213250398636,\n",
       " 0.030459115281701088,\n",
       " 0.08003842085599899,\n",
       " 0.059037599712610245,\n",
       " 0.048644255846738815,\n",
       " 0.08736975491046906,\n",
       " 0.039131004363298416,\n",
       " 0.041231460869312286,\n",
       " 0.08914411813020706,\n",
       " 0.13320425152778625,\n",
       " 0.022785967215895653,\n",
       " 0.06657367199659348,\n",
       " 0.07230743020772934,\n",
       " 0.029257677495479584,\n",
       " 0.05702678859233856,\n",
       " 0.12916992604732513,\n",
       " 0.16696275770664215,\n",
       " 0.18284465372562408,\n",
       " 0.08242198824882507,\n",
       " 0.014912457205355167,\n",
       " 0.08792565762996674,\n",
       " 0.40782517194747925,\n",
       " 0.1146591454744339,\n",
       " 0.14804190397262573,\n",
       " 0.019106633961200714,\n",
       " 0.17817729711532593,\n",
       " 0.1976860761642456,\n",
       " 0.0745108425617218,\n",
       " 0.1354082226753235,\n",
       " 0.1355791687965393,\n",
       " 0.04885219410061836,\n",
       " 0.14859840273857117,\n",
       " 0.01002529077231884,\n",
       " 0.025112677365541458,\n",
       " 0.061929620802402496,\n",
       " 0.11081017553806305,\n",
       " 0.06584035605192184,\n",
       " 0.06610091775655746,\n",
       " 0.11703849583864212,\n",
       " 0.04060126468539238,\n",
       " 0.056814078241586685,\n",
       " 0.03260357677936554,\n",
       " 0.05446816608309746,\n",
       " 0.07823041826486588,\n",
       " 0.13908575475215912,\n",
       " 0.04211917147040367,\n",
       " 0.11091198772192001,\n",
       " 0.14635492861270905,\n",
       " 0.2510199248790741,\n",
       " 0.031342413276433945,\n",
       " 0.03569262847304344,\n",
       " 0.061581917107105255,\n",
       " 0.03389262408018112,\n",
       " 0.15798847377300262,\n",
       " 0.026009371504187584,\n",
       " 0.16385407745838165,\n",
       " 0.020687062293291092,\n",
       " 0.05370892956852913,\n",
       " 0.0752251148223877,\n",
       " 0.02535119280219078,\n",
       " 0.09283509105443954,\n",
       " 0.11133090406656265,\n",
       " 0.279390424489975,\n",
       " 0.2952296733856201,\n",
       " 0.04719153046607971,\n",
       " 0.07683603465557098,\n",
       " 0.01686077006161213,\n",
       " 0.042883098125457764,\n",
       " 0.05328868329524994,\n",
       " 0.061622973531484604,\n",
       " 0.14397311210632324,\n",
       " 0.027609461918473244,\n",
       " 0.171150341629982,\n",
       " 0.05969575420022011,\n",
       " 0.049352869391441345,\n",
       " 0.05300608277320862,\n",
       " 0.14693069458007812,\n",
       " 0.09964627027511597,\n",
       " 0.030363766476511955,\n",
       " 0.13760095834732056,\n",
       " 0.046596039086580276,\n",
       " 0.08731965720653534,\n",
       " 0.1858803927898407,\n",
       " 0.14246101677417755,\n",
       " 0.0537654310464859,\n",
       " 0.04694240540266037,\n",
       " 0.11190560460090637,\n",
       " 0.24505305290222168,\n",
       " 0.05454671382904053,\n",
       " 0.06491394340991974,\n",
       " 0.11138030886650085,\n",
       " 0.12318546324968338,\n",
       " 0.09663009643554688,\n",
       " 0.04359445348381996,\n",
       " 0.04405181109905243,\n",
       " 0.11224709451198578,\n",
       " 0.3961303234100342,\n",
       " 0.030253104865550995,\n",
       " 0.021014399826526642,\n",
       " 0.1890917420387268,\n",
       " 0.0591377392411232,\n",
       " 0.0813579186797142,\n",
       " 0.08254972100257874,\n",
       " 0.13198262453079224,\n",
       " 0.06992466002702713,\n",
       " 0.16563238203525543,\n",
       " 0.10183387994766235,\n",
       " 0.023282082751393318,\n",
       " 0.050276532769203186,\n",
       " 0.020932622253894806,\n",
       " 0.07135206460952759,\n",
       " 0.1471756547689438,\n",
       " 0.09406208992004395,\n",
       " 0.07012883573770523,\n",
       " 0.23217692971229553,\n",
       " 0.07100233435630798,\n",
       " 0.05566611886024475,\n",
       " 0.008490162901580334,\n",
       " 0.008012539707124233,\n",
       " 0.2935008108615875,\n",
       " 0.0845811739563942,\n",
       " 0.06637066602706909,\n",
       " 0.13246959447860718,\n",
       " 0.054256174713373184,\n",
       " 0.0903579592704773,\n",
       " 0.062339842319488525,\n",
       " 0.22501514852046967,\n",
       " 0.05001574382185936,\n",
       " 0.07899101078510284,\n",
       " 0.11146313697099686,\n",
       " 0.09162652492523193,\n",
       " 0.10536342114210129,\n",
       " 0.028542736545205116,\n",
       " 0.25037482380867004,\n",
       " 0.16334299743175507,\n",
       " 0.16334503889083862,\n",
       " 0.09275482594966888,\n",
       " 0.008994120173156261,\n",
       " 0.1316579431295395,\n",
       " 0.07707048207521439,\n",
       " 0.10901638120412827,\n",
       " 0.04078023135662079,\n",
       " 0.04538576677441597,\n",
       " 0.12670335173606873,\n",
       " 0.07508447021245956,\n",
       " 0.1403714120388031,\n",
       " 0.09768759459257126,\n",
       " 0.04128202423453331,\n",
       " 0.02081984467804432,\n",
       " 0.06243928521871567,\n",
       " 0.07075673341751099,\n",
       " 0.06943622976541519,\n",
       " 0.2536415457725525,\n",
       " 0.31923434138298035,\n",
       " 0.05441170558333397,\n",
       " 0.08795405924320221,\n",
       " 0.07556996494531631,\n",
       " 0.0892876535654068,\n",
       " 0.0383770577609539,\n",
       " 0.04153621941804886,\n",
       " 0.15242047607898712,\n",
       " 0.08304298669099808,\n",
       " 0.203957200050354,\n",
       " 0.12131582945585251,\n",
       " 0.06945092976093292,\n",
       " 0.12513034045696259,\n",
       " 0.10506654530763626,\n",
       " 0.20419129729270935,\n",
       " 0.02164795994758606,\n",
       " 0.15871800482273102,\n",
       " 0.13918592035770416,\n",
       " 0.04677936062216759,\n",
       " 0.06712355464696884,\n",
       " 0.06151553615927696,\n",
       " 0.08414021134376526,\n",
       " 0.03915565460920334,\n",
       " 0.023739391937851906,\n",
       " 0.16478504240512848,\n",
       " 0.042232125997543335,\n",
       " 0.10809759795665741,\n",
       " 0.09521761536598206,\n",
       " 0.12187455594539642,\n",
       " 0.05617713928222656,\n",
       " 0.13274918496608734,\n",
       " 0.08187710493803024,\n",
       " 0.10589270293712616,\n",
       " 0.18192532658576965,\n",
       " 0.06832247972488403,\n",
       " 0.19997195899486542,\n",
       " 0.13144125044345856,\n",
       " 0.07398388534784317,\n",
       " 0.11191017925739288,\n",
       " 0.032414019107818604,\n",
       " 0.15287569165229797,\n",
       " 0.04306824505329132,\n",
       " 0.06346229463815689,\n",
       " 0.13582634925842285,\n",
       " 0.07678472995758057,\n",
       " 0.11844035238027573,\n",
       " 0.18896251916885376,\n",
       " 0.09779061377048492,\n",
       " 0.03234449028968811,\n",
       " 0.07590661942958832,\n",
       " 0.06962618976831436,\n",
       " 0.08994314074516296,\n",
       " 0.09813518077135086,\n",
       " 0.03768179565668106,\n",
       " 0.06621196866035461,\n",
       " 0.015658171847462654,\n",
       " 0.07853006571531296,\n",
       " 0.05512227118015289,\n",
       " 0.08043693006038666,\n",
       " 0.07231910526752472,\n",
       " 0.06882364302873611,\n",
       " 0.2200998216867447,\n",
       " 0.0928114578127861,\n",
       " 0.2205156534910202,\n",
       " 0.09296813607215881,\n",
       " 0.10052381455898285,\n",
       " 0.027437014505267143,\n",
       " 0.03167689964175224,\n",
       " 0.025220753625035286,\n",
       " 0.1142740324139595,\n",
       " 0.10904700309038162,\n",
       " 0.0705978199839592,\n",
       " 0.14441953599452972,\n",
       " 0.07360362261533737,\n",
       " 0.023280221968889236,\n",
       " 0.09620511531829834,\n",
       " 0.04552927985787392,\n",
       " 0.032856933772563934,\n",
       " 0.0630001574754715,\n",
       " 0.09496483206748962,\n",
       " 0.042497508227825165,\n",
       " 0.12497276812791824,\n",
       " 0.04814225807785988,\n",
       " 0.05818026885390282,\n",
       " 0.0336640365421772,\n",
       " 0.13815727829933167,\n",
       " 0.04183800145983696,\n",
       " 0.06711670011281967,\n",
       " 0.11225645989179611,\n",
       " 0.09231571853160858,\n",
       " 0.08358697593212128,\n",
       " 0.057855911552906036,\n",
       " 0.07430277019739151,\n",
       " 0.26530706882476807,\n",
       " 0.057364556938409805,\n",
       " 0.0933854877948761,\n",
       " 0.03595637157559395,\n",
       " 0.11509346216917038,\n",
       " 0.0980416014790535,\n",
       " 0.08360490202903748,\n",
       " 0.06342905759811401,\n",
       " 0.13364224135875702,\n",
       " 0.039932213723659515,\n",
       " 0.05119483917951584,\n",
       " 0.12305320799350739,\n",
       " 0.014625314623117447,\n",
       " 0.04278631508350372,\n",
       " 0.03305647522211075,\n",
       " 0.054039567708969116,\n",
       " 0.05550483986735344,\n",
       " 0.13114166259765625,\n",
       " 0.1185433492064476,\n",
       " 0.07557018101215363,\n",
       " 0.22152432799339294,\n",
       " 0.03498143330216408,\n",
       " 0.12889982759952545,\n",
       " 0.018076278269290924,\n",
       " 0.06954334676265717,\n",
       " 0.03686748445034027,\n",
       " 0.05289271101355553,\n",
       " 0.11557992547750473,\n",
       " 0.04811207950115204,\n",
       " 0.18370966613292694,\n",
       " 0.07551494985818863,\n",
       " 0.042957138270139694,\n",
       " 0.01590263843536377,\n",
       " 0.13318273425102234,\n",
       " 0.08176116645336151,\n",
       " 0.09347011893987656,\n",
       " 0.04045306146144867,\n",
       " 0.16770125925540924,\n",
       " 0.06026773899793625,\n",
       " 0.03563300147652626,\n",
       " 0.20185181498527527,\n",
       " 0.03301461413502693,\n",
       " 0.12281712144613266,\n",
       " 0.050227090716362,\n",
       " 0.11378724873065948,\n",
       " 0.031848810613155365,\n",
       " 0.07716292887926102,\n",
       " 0.08292227238416672,\n",
       " 0.15032263100147247,\n",
       " 0.053614143282175064,\n",
       " 0.10157521814107895,\n",
       " 0.16306984424591064,\n",
       " 0.09964662790298462,\n",
       " 0.10973756015300751,\n",
       " 0.05322393774986267,\n",
       " 0.10665617138147354,\n",
       " 0.030651548877358437,\n",
       " 0.05299699306488037,\n",
       " 0.045415930449962616,\n",
       " 0.016236258670687675,\n",
       " 0.12876375019550323,\n",
       " 0.0940094068646431,\n",
       " 0.10408986359834671,\n",
       " 0.25885987281799316,\n",
       " 0.07340870797634125,\n",
       " 0.021867653355002403,\n",
       " 0.05802227556705475,\n",
       " 0.07589992880821228,\n",
       " 0.21762961149215698,\n",
       " 0.061089158058166504,\n",
       " 0.06622162461280823,\n",
       " 0.01292533241212368,\n",
       " 0.14829884469509125,\n",
       " 0.1168012022972107,\n",
       " 0.09514012932777405,\n",
       " 0.024089433252811432,\n",
       " 0.13041052222251892,\n",
       " 0.04896917939186096,\n",
       " 0.04309740662574768,\n",
       " 0.06562201678752899,\n",
       " 0.005022438708692789,\n",
       " 0.03612695634365082,\n",
       " 0.03376166522502899,\n",
       " 0.010893975384533405,\n",
       " 0.04600602760910988,\n",
       " 0.21287165582180023,\n",
       " 0.03611551225185394,\n",
       " 0.04532182961702347,\n",
       " 0.13935351371765137,\n",
       " 0.08734994381666183,\n",
       " 0.08714967221021652,\n",
       " 0.08377733081579208,\n",
       " 0.22630278766155243,\n",
       " 0.2586965262889862,\n",
       " 0.0924927368760109,\n",
       " 0.12747390568256378,\n",
       " 0.06063181906938553,\n",
       " 0.018579533323645592,\n",
       " 0.09584599733352661,\n",
       " 0.09842398017644882,\n",
       " 0.04398621618747711,\n",
       " 0.05024081841111183,\n",
       " 0.09070998430252075,\n",
       " 0.03730199113488197,\n",
       " 0.026447167620062828,\n",
       " 0.22525350749492645,\n",
       " 0.06948783248662949,\n",
       " 0.09772403538227081,\n",
       " 0.16914209723472595,\n",
       " 0.0393427275121212,\n",
       " 0.20238162577152252,\n",
       " 0.0306861512362957,\n",
       " 0.09545588493347168,\n",
       " 0.09161997586488724,\n",
       " 0.1253974586725235,\n",
       " 0.0802849754691124,\n",
       " 0.07198027521371841,\n",
       " 0.012807069346308708,\n",
       " 0.04288241267204285,\n",
       " 0.04899292439222336,\n",
       " 0.0714508667588234,\n",
       " 0.07608914375305176,\n",
       " 0.03573376312851906,\n",
       " 0.25615808367729187,\n",
       " 0.20140965282917023,\n",
       " 0.09411703050136566,\n",
       " 0.11163534224033356,\n",
       " 0.2431756854057312,\n",
       " 0.24555103480815887,\n",
       " 0.060457684099674225,\n",
       " 0.06714945286512375,\n",
       " 0.018688110634684563,\n",
       " 0.05198518559336662,\n",
       " 0.012871136888861656,\n",
       " 0.12997497618198395,\n",
       " 0.014134190045297146,\n",
       " 0.09088422358036041,\n",
       " 0.11515451222658157,\n",
       " 0.0978560596704483,\n",
       " 0.03376494720578194,\n",
       " 0.044794682413339615,\n",
       " 0.17255404591560364,\n",
       " 0.15410232543945312,\n",
       " 0.07516495138406754,\n",
       " 0.04726523533463478,\n",
       " 0.06330419331789017,\n",
       " 0.16674892604351044,\n",
       " 0.03226048871874809,\n",
       " 0.04535392299294472,\n",
       " 0.05648817494511604,\n",
       " 0.18805687129497528,\n",
       " 0.27160680294036865,\n",
       " 0.19193436205387115,\n",
       " 0.17364680767059326,\n",
       " 0.05716389790177345,\n",
       " 0.13054229319095612,\n",
       " 0.05200941860675812,\n",
       " 0.18660861253738403,\n",
       " 0.029042284935712814,\n",
       " 0.0727192610502243,\n",
       " 0.14663194119930267,\n",
       " 0.06082936003804207,\n",
       " 0.17046359181404114,\n",
       " 0.10399514436721802,\n",
       " 0.1131640076637268,\n",
       " 0.11103137582540512,\n",
       " 0.05326096713542938,\n",
       " 0.056641507893800735,\n",
       " 0.05325327068567276,\n",
       " 0.13551625609397888,\n",
       " 0.1127418503165245,\n",
       " 0.1392957717180252,\n",
       " 0.06768380105495453,\n",
       " 0.10110001266002655,\n",
       " 0.03168931230902672,\n",
       " 0.13687536120414734,\n",
       " 0.029381610453128815,\n",
       " 0.11862770467996597,\n",
       " 0.0866096019744873,\n",
       " 0.12865391373634338,\n",
       " 0.3281475305557251,\n",
       " 0.09818018227815628,\n",
       " 0.1054491400718689,\n",
       " 0.04381546005606651,\n",
       " 0.043780043721199036,\n",
       " 0.0496792271733284,\n",
       " 0.20641174912452698,\n",
       " 0.15345653891563416,\n",
       " 0.034850671887397766,\n",
       " 0.21218661963939667,\n",
       " 0.033140409737825394,\n",
       " 0.09641949832439423,\n",
       " 0.03677559643983841,\n",
       " 0.1202482208609581,\n",
       " 0.12817636132240295,\n",
       " 0.10161016136407852,\n",
       " 0.02720695361495018,\n",
       " 0.0390201210975647,\n",
       " 0.06221611797809601,\n",
       " 0.08782334625720978,\n",
       " 0.08235030621290207,\n",
       " 0.11031638830900192,\n",
       " 0.14131885766983032,\n",
       " 0.1156022772192955,\n",
       " 0.053189467638731,\n",
       " 0.15818290412425995,\n",
       " 0.19193267822265625,\n",
       " 0.11891480535268784,\n",
       " 0.07251366972923279,\n",
       " 0.17921830713748932,\n",
       " 0.2243822067975998,\n",
       " 0.09964226186275482,\n",
       " 0.11691859364509583,\n",
       " 0.24647974967956543,\n",
       " 0.04404162988066673,\n",
       " 0.14108610153198242,\n",
       " 0.04831061139702797,\n",
       " 0.1768915206193924,\n",
       " 0.10738138109445572,\n",
       " 0.1266505867242813,\n",
       " 0.07270412147045135,\n",
       " 0.02812795527279377,\n",
       " 0.09977159649133682,\n",
       " 0.060259316116571426,\n",
       " 0.07674961537122726,\n",
       " 0.18355819582939148,\n",
       " 0.06293945759534836,\n",
       " 0.042125094681978226,\n",
       " 0.05337165296077728,\n",
       " 0.03762923181056976,\n",
       " 0.09460785984992981,\n",
       " 0.10824339091777802,\n",
       " 0.11189472675323486,\n",
       " 0.05901508405804634,\n",
       " 0.07866508513689041,\n",
       " 0.05576822906732559,\n",
       " 0.11767024546861649,\n",
       " 0.05719001218676567,\n",
       " 0.07281555980443954,\n",
       " 0.05814364552497864,\n",
       " 0.07513633370399475,\n",
       " 0.04520655423402786,\n",
       " 0.07493312656879425,\n",
       " 0.08622504770755768,\n",
       " 0.03955867886543274,\n",
       " 0.0928131714463234,\n",
       " 0.06379826366901398,\n",
       " 0.08733980357646942,\n",
       " 0.04998170584440231,\n",
       " 0.05782228335738182,\n",
       " 0.1483573615550995,\n",
       " 0.23055757582187653,\n",
       " 0.07177682220935822,\n",
       " 0.027102533727884293,\n",
       " 0.26249128580093384,\n",
       " 0.1225028783082962,\n",
       " 0.033815037459135056,\n",
       " 0.048291388899087906,\n",
       " 0.09646488726139069,\n",
       " 0.08084157109260559,\n",
       " 0.09253140538930893,\n",
       " 0.3895566165447235,\n",
       " 0.07194174081087112,\n",
       " 0.15090931951999664,\n",
       " 0.19995050132274628,\n",
       " 0.04470016062259674,\n",
       " 0.12735049426555634,\n",
       " 0.029851824045181274,\n",
       " 0.1146402433514595,\n",
       " 0.033513788133859634,\n",
       " 0.20995114743709564,\n",
       " 0.11780237406492233,\n",
       " 0.09052398055791855,\n",
       " 0.04529932513833046,\n",
       " 0.08412908017635345,\n",
       " 0.04172670096158981,\n",
       " 0.03420180454850197,\n",
       " 0.10917721688747406,\n",
       " 0.06489970535039902,\n",
       " 0.0655221939086914,\n",
       " 0.035776879638433456,\n",
       " 0.0743766501545906,\n",
       " 0.09588450193405151,\n",
       " 0.10042127221822739,\n",
       " 0.04083988443017006,\n",
       " 0.0466642752289772,\n",
       " 0.029202083125710487,\n",
       " 0.11693015694618225,\n",
       " 0.3403795659542084,\n",
       " 0.06229010969400406,\n",
       " 0.055945660918951035,\n",
       " 0.1325386017560959,\n",
       " 0.1805362105369568,\n",
       " 0.06264178454875946,\n",
       " ...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                Output Shape              Param #        \n",
      "=================================================================\n",
      "conv1                     torch.Size([512])         1024\n",
      "conv1.aggr_module         N/A                       0\n",
      "conv1.lin                 torch.Size([512, 1])      512\n",
      "bn1                       torch.Size([512])         1024\n",
      "dropout1                  N/A                       0\n",
      "conv2                     torch.Size([512])         262656\n",
      "conv2.aggr_module         N/A                       0\n",
      "conv2.lin                 torch.Size([512, 512])    262144\n",
      "bn2                       torch.Size([512])         1024\n",
      "dropout2                  N/A                       0\n",
      "prot_conv1                torch.Size([512, 1024, 3]) 1573376\n",
      "prot_conv2                torch.Size([256, 512, 3]) 393472\n",
      "fc1                       torch.Size([256, 3072])   786688\n",
      "fc2                       torch.Size([128, 256])    32896\n",
      "fc3                       torch.Size([64, 128])     8256\n",
      "out                       torch.Size([1, 64])       65\n",
      "=================================================================\n",
      "Total Trainable Parameters: 3323137\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Example function to count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Function to print the model summary\n",
    "def print_model_summary(model):\n",
    "    print(f\"{'Layer Name':<25} {'Output Shape':<25} {'Param #':<15}\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    model_modules = []\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and name != \"\":\n",
    "            params_list = list(module.parameters())\n",
    "            output_shape = params_list[0].size() if params_list else 'N/A'\n",
    "            param_num = count_parameters(module)\n",
    "            meta_info = f\"{name:<25} {str(output_shape):<25} {param_num}\"\n",
    "            print(meta_info)\n",
    "            total_params += param_num\n",
    "            model_modules.append(meta_info)\n",
    "\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "    return model_modules\n",
    "\n",
    "# for index, batch in enumerate(final_test):\n",
    "#     p_test       = x_test_p[index*batch_size: (index + 1) * batch_size]\n",
    "#     test_results = model(batch, p_test) \n",
    "#     break\n",
    "\n",
    "# Print the model summary now that the parameters are initialized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_main(index, vocab_tokens, batch_size = 100000):\n",
    "    ''' \n",
    "    This function handles all the function calls for loading\n",
    "    data in batches all the way to feeding it to the model for inference\n",
    "    and saving the predicted probabilites to CSV file for competition_submission\n",
    "    \n",
    "    Parameters:\n",
    "    - index:\n",
    "    - vocab_tokens:\n",
    "    - batch_size: \n",
    "    \n",
    "    Returns:\n",
    "    - final_test:\n",
    "    - proteins: \n",
    "    \n",
    "    '''\n",
    "    # Data Batch is a list containing matricies\n",
    "    data_batch            = load_embeddings(base_path = 'Batched Data/test_preprocessed_Batch', index = index)\n",
    "    final_test, proteins  = load_test_proteins(data_batch = data_batch,\n",
    "                                     batch_size = batch_size, \n",
    "                                     index = index, \n",
    "                                     vocab_tokens = vocab_tokens,\n",
    "                                     mode = 'Evaluate')\n",
    "#     final_test    = create_gcn_data(mode = 'Evaluate', final_index = batch_size)\n",
    "    \n",
    "    return final_test, proteins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, final_test, proteins, batch_size = 32):\n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - model:\n",
    "    - final_test:\n",
    "    - proteins:\n",
    "    - batch_size:\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities:\n",
    "    - raw_preds:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    test_preds       = []\n",
    "    raw_preds        = []\n",
    "\n",
    "    for index, batch in enumerate(final_test):\n",
    "        p_test       = proteins[index*batch_size: (index + 1) * batch_size]\n",
    "        test_results = model(batch, p_test)\n",
    "        raw_prob     = copy.copy(test_results)\n",
    "        \n",
    "        test_results = [1 if x > 0.5 else 0 for x in test_results]\n",
    "        test_preds.append(test_results)\n",
    "        raw_preds.append(raw_prob)\n",
    "\n",
    "    predictions      = []\n",
    "    for sub in test_preds:\n",
    "        for result in sub:\n",
    "            predictions.append(result)\n",
    "\n",
    "    probabilities    = []\n",
    "    for sub in raw_preds:\n",
    "        for result in sub:\n",
    "            probabilities.append(result)\n",
    "\n",
    "    \n",
    "    return probabilities, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                Output Shape              Param #        \n",
      "=================================================================\n",
      "conv1                     torch.Size([16])          32\n",
      "conv1.aggr_module         N/A                       0\n",
      "conv1.lin                 torch.Size([16, 1])       16\n",
      "bn1                       torch.Size([16])          32\n",
      "dropout1                  N/A                       0\n",
      "conv2                     torch.Size([16])          272\n",
      "conv2.aggr_module         N/A                       0\n",
      "conv2.lin                 torch.Size([16, 16])      256\n",
      "bn2                       torch.Size([16])          32\n",
      "dropout2                  N/A                       0\n",
      "p_fc1                     torch.Size([256, 12288])  3145984\n",
      "p_fc2                     torch.Size([64, 256])     16448\n",
      "fc1                       torch.Size([1, 80])       81\n",
      "=================================================================\n",
      "Total Trainable Parameters: 3163153\n",
      "Precision: 0.0015527950310559005\n",
      "Recall: 0.3333333333333333\n",
      "F1 Score: 0.0030911901081916537\n"
     ]
    }
   ],
   "source": [
    "def export_results(experiment_name, module_info, model_name, json_master = 'model_experiments.json', view_only = True):\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - experiment_name\n",
    "    - module_info\n",
    "    - model_name\n",
    "    - json_master\n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    '''\n",
    "    # Could Add better Comprehension Here\n",
    "    if not os.path.exists(model_name):\n",
    "        torch.save(model, model_name)\n",
    "    else:\n",
    "        warnings.warn(f'{model_name} already exists. Model is not being saved')\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(predictions, y_test)\n",
    "    recall    = recall_score(predictions, y_test)\n",
    "    f1        = f1_score(predictions, y_test)\n",
    "    accuracy  = accuracy_score(predictions, y_test)\n",
    "    mae       = mean_absolute_error(predictions, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    metrics_dictionary = {'Precision': precision,\n",
    "                         'Recall': recall,\n",
    "                         'F1 Score:': f1,\n",
    "                         'Accuracy': accuracy,\n",
    "                         'MAE': mae}\n",
    "    \n",
    "    \n",
    "    if not view_only:\n",
    "        summary_dict                  = json.load(open(json_master, 'r'))\n",
    "        if experiment_name not in summary_dict:\n",
    "            summary_dict[experiment_name] = {'Model Metadata': module_info,\n",
    "                                            'Results': metrics_dictionary}\n",
    "\n",
    "            json.dump(summary_dict, open(json_master, 'w'), indent=4)\n",
    "        else:\n",
    "            raise ValueError('Duplicate Experiment Name. Choose a different experiment name')\n",
    "\n",
    "    return\n",
    "\n",
    "## Get the current date\n",
    "current_date    = datetime.now()\n",
    "timestamp       = current_date.strftime(\"%d-%m-%Y\")  # You can change the format as needed\n",
    "\n",
    "model_modules    = None\n",
    "class_0_size    = 15000\n",
    "class_1_size    = 2750\n",
    "model_paths     = 'Trained_Models'\n",
    "# model_name      = f'{model_paths}/GCN_{class_0_size}_{class_1_size}_{timestamp}.pth'\n",
    "\n",
    "model_modules   = print_model_summary(model)\n",
    "model_name      = f'{model_paths}/GCN_{class_0_size}_{class_1_size}_{timestamp}.pth'\n",
    "\n",
    "experiment_name = f'{class_0_size} (No Bind) and {class_1_size} (Bind)'\n",
    "export_results(experiment_name, model_modules, model_name, view_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1674896it [01:50, 15193.16it/s]                             \n"
     ]
    }
   ],
   "source": [
    "def inference_csv(final_test, \n",
    "                  test_data,\n",
    "                  proteins,\n",
    "                  submission_name, \n",
    "                  model_name = 'GCN_15000_2576_08-05-2024.pth'):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    - final_test: DataLoader with graphs\n",
    "    - test_data: Raw CSV test file from Kaggle\n",
    "    - submission_name: Name\n",
    "    - model_name: \n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    '''\n",
    "    \n",
    "    protein_ids   = test_data['id']\n",
    "    model_path    = 'Trained_Models/' + model_name\n",
    "    model         = torch.load(model_path)\n",
    "    \n",
    "    probabilities, predictions = run_model(model, final_test, proteins, batch_size = 32)\n",
    "    \n",
    "    probabilities = [float(x.detach().numpy()) for x in probabilities]\n",
    "    probabilities = pd.DataFrame(probabilities)#.apply(lambda x: x.detach().numpy())\n",
    "    probabilities.index = protein_ids.index\n",
    "    submissions   = pd.concat([protein_ids, probabilities], axis = 1)\n",
    "    \n",
    "    submissions.columns = ['id', 'binds']\n",
    "    # Avoid overriding previous submissions\n",
    "#     submission_name     = submission_name + f'{index}'\n",
    "#     if os.path.exists(submission_name):\n",
    "#         raise ValueError(f'{submission_name}.csv already exists')\n",
    "#     else:\n",
    "#         submissions.to_csv(submission_name, index = False)\n",
    "        \n",
    "    return model, submissions\n",
    "\n",
    "\n",
    "# The size data was batched in pckl files DO NOT CHANGE\n",
    "batch_size                       = 100000\n",
    "batches                          = math.ceil(len(test_data) / batch_size)\n",
    "vocab_tokens                     = load_vocab_tokens(base_path = 'test_preprocessed')\n",
    "vocab_keys                       = list(vocab_tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m submission_df                    \u001b[38;5;241m=\u001b[39m   pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mbatches\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Need to fix the logic for this no no need to open full json file everytime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Implement the batching into all function calls\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     subset_keys                  \u001b[38;5;241m=\u001b[39m vocab_keys[index\u001b[38;5;241m*\u001b[39mbatch_size:(index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size]\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Step 3: Create a new dictionary with these keys\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batches' is not defined"
     ]
    }
   ],
   "source": [
    "submission_df                    =   pd.DataFrame()\n",
    "for index in range(batches):\n",
    "    # Need to fix the logic for this no no need to open full json file everytime\n",
    "    # Implement the batching into all function calls\n",
    "    \n",
    "    subset_keys                  = vocab_keys[index*batch_size:(index + 1)*batch_size]\n",
    "    # Step 3: Create a new dictionary with these keys\n",
    "    token_subset                 = {key: vocab_tokens[key] for key in subset_keys}\n",
    "    \n",
    "    final_test, proteins         = preprocess_main(batch_size   = batch_size, \n",
    "                                                   index        = index, \n",
    "                                                   vocab_tokens = token_subset,\n",
    "                                                  )\n",
    "\n",
    "    model, submissions           = inference_csv(final_test,\n",
    "                                   test_data[index*batch_size:(index + 1)*batch_size],\n",
    "                                   proteins,\n",
    "                                   submission_name = f'Model Submissions/submission_b{index}.csv',\n",
    "                                   model_name      = 'GCN_15000_2576_08-05-2024.pth')\n",
    "    \n",
    "       \n",
    "    \n",
    "    submission_df                = pd.concat([submission_df, submissions], axis = 0)\n",
    "    \n",
    "    # Delete Variables to deal with memory\n",
    "    del subset_keys\n",
    "    del token_subset\n",
    "    del final_test\n",
    "    del proteins\n",
    "    \n",
    "    # Force Garbage Collection for Memory\n",
    "    gc.collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  binds\n",
       "0        True   True\n",
       "1        True  False\n",
       "2        True  False\n",
       "3        True   True\n",
       "4        True   True\n",
       "...       ...    ...\n",
       "1674891  True  False\n",
       "1674892  True   True\n",
       "1674893  True  False\n",
       "1674894  True   True\n",
       "1674895  True   True\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_sub = pd.read_csv('submission.csv')\n",
    "\n",
    "equal = other_sub == submission_df\n",
    "equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binds\n",
       "True     1433295\n",
       "False     241601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal['binds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       1574896\n",
       "binds    1574896\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>protein_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC1CCC(CCN)CC1</td>\n",
       "      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC1CCC(CCN)CC1</td>\n",
       "      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                              buildingblock1_smiles  \\\n",
       "0        295246830    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "1        295246831    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "2        295246832    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "3        295246833    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "4        295246834    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "...            ...                                                ...   \n",
       "1674891  296921721  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674892  296921722  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674893  296921723  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674894  296921724  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674895  296921725  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "\n",
       "        buildingblock2_smiles   buildingblock3_smiles  \\\n",
       "0              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "1              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "2              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "3              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "4              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "...                       ...                     ...   \n",
       "1674891     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n",
       "1674892     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n",
       "1674893     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "1674894     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "1674895     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "\n",
       "                                           molecule_smiles protein_name  \n",
       "0        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...         BRD4  \n",
       "1        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          HSA  \n",
       "2        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          sEH  \n",
       "3        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...         BRD4  \n",
       "4        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...          HSA  \n",
       "...                                                    ...          ...  \n",
       "1674891  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...          HSA  \n",
       "1674892  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...          sEH  \n",
       "1674893  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...         BRD4  \n",
       "1674894  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...          HSA  \n",
       "1674895  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...          sEH  \n",
       "\n",
       "[1674896 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reset    = 0\n",
    "reset_index    = []\n",
    "for index in submission_df.index:\n",
    "    if index < index_reset:\n",
    "        reset_index.append(index_reset)\n",
    "        index_reset = 0\n",
    "    else:\n",
    "        index_reset = index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[199999,\n",
       " 299999,\n",
       " 399999,\n",
       " 499999,\n",
       " 599999,\n",
       " 699999,\n",
       " 799999,\n",
       " 899999,\n",
       " 999999,\n",
       " 1099999,\n",
       " 1199999,\n",
       " 1299999,\n",
       " 1399999,\n",
       " 1499999,\n",
       " 1599999,\n",
       " 1674895]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830.0</td>\n",
       "      <td>0.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831.0</td>\n",
       "      <td>0.913789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832.0</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833.0</td>\n",
       "      <td>0.812792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834.0</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74891</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74892</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.985396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74894</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.975440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74895</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3249792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     binds\n",
       "0      295246830.0  0.947388\n",
       "1      295246831.0  0.913789\n",
       "2      295246832.0  0.927355\n",
       "3      295246833.0  0.812792\n",
       "4      295246834.0  0.718755\n",
       "...            ...       ...\n",
       "74891          NaN  0.956296\n",
       "74892          NaN  0.963440\n",
       "74893          NaN  0.985396\n",
       "74894          NaN  0.975440\n",
       "74895          NaN  0.979522\n",
       "\n",
       "[3249792 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830.0</td>\n",
       "      <td>0.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831.0</td>\n",
       "      <td>0.913789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832.0</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833.0</td>\n",
       "      <td>0.812792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834.0</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721.0</td>\n",
       "      <td>0.956296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722.0</td>\n",
       "      <td>0.963440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723.0</td>\n",
       "      <td>0.985396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724.0</td>\n",
       "      <td>0.975440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725.0</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     binds\n",
       "0        295246830.0  0.947388\n",
       "1        295246831.0  0.913789\n",
       "2        295246832.0  0.927355\n",
       "3        295246833.0  0.812792\n",
       "4        295246834.0  0.718755\n",
       "...              ...       ...\n",
       "1674891  296921721.0  0.956296\n",
       "1674892  296921722.0  0.963440\n",
       "1674893  296921723.0  0.985396\n",
       "1674894  296921724.0  0.975440\n",
       "1674895  296921725.0  0.979522\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
