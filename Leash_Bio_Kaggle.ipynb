{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m474.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchsummary, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.0 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 torchsummary-1.5.1 transformers-4.41.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rdkit) (10.2.0)\n",
      "Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rdkit\n",
      "Successfully installed rdkit-2023.9.6\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting d2l\n",
      "  Downloading d2l-1.0.3-py3-none-any.whl.metadata (556 bytes)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (1.0.0)\n",
      "Collecting numpy==1.23.5 (from d2l)\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting matplotlib==3.7.2 (from d2l)\n",
      "  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (0.1.6)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from d2l) (2.31.0)\n",
      "Collecting pandas==2.0.3 (from d2l)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy==1.10.1 (from d2l)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: notebook in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.1.2)\n",
      "Requirement already satisfied: qtconsole in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (7.16.3)\n",
      "Requirement already satisfied: ipykernel in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (6.29.3)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter==1.0.0->d2l) (8.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (10.2.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib==3.7.2->d2l)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib==3.7.2->d2l) (2.9.0)\n",
      "Requirement already satisfied: traitlets in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib-inline==0.1.6->d2l) (5.14.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas==2.0.3->d2l) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests==2.31.0->d2l) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.22.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (1.6.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->d2l) (6.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.42)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.17.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.13.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (2.25.4)\n",
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (4.1.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->d2l) (4.2.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.5.3)\n",
      "Requirement already satisfied: overrides in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
      "Requirement already satisfied: websocket-client in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.7.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.2.4)\n",
      "Requirement already satisfied: tomli in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.9.24)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (4.21.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter==1.0.0->d2l) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter==1.0.0->d2l) (0.18.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.4)\n",
      "Requirement already satisfied: uri-template in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->d2l) (2.9.0.20240316)\n",
      "Downloading d2l-1.0.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, numpy, scipy, pandas, matplotlib, d2l\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.12.0\n",
      "    Uninstalling scipy-1.12.0:\n",
      "      Successfully uninstalled scipy-1.12.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autovizwidget 0.21.0 requires pandas<2.0.0,>=0.20.1, but you have pandas 2.0.3 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed d2l-1.0.3 matplotlib-3.7.2 numpy-1.23.5 pandas-2.0.3 pyparsing-3.0.9 scipy-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.1.3)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Using cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch-geometric) (5.9.8)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Installing collected packages: multidict, h5py, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 h5py-3.11.0 multidict-6.0.5 torch-geometric-2.5.3 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "--46iLsBsl_W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:616: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from   d2l import torch as d2l\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from   torchsummary import summary\n",
    "from   torchtext.data.utils import get_tokenizer\n",
    "from   torchtext.datasets import AG_NEWS\n",
    "from   transformers import AutoTokenizer\n",
    "from   torch.nn.utils.rnn import pad_sequence\n",
    "from   torch.utils.data import TensorDataset, DataLoader\n",
    "from   transformers import AutoModel, AutoTokenizer\n",
    "from   torch import nn\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import csv\n",
    "import h5py\n",
    "import ast\n",
    "import gc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
    "from datetime import datetime\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "from JunctionTree.main import Datapreprocess\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoyZdy6Emr2E"
   },
   "source": [
    "**First Protein Target from Competition Description**\n",
    "1. Epoxide Hydrolase 2\n",
    "2. High Blood pressure and diabetes target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-OBEQKDrmVyf"
   },
   "outputs": [],
   "source": [
    "EPHX2 = '''MTLRAAVFDLDGVLALPAVFGVLGRTEEALALPRGLLNDAFQKGGPEGATTRLMKGEI\n",
    "TLSQWIPLMEENCRKCSETAKVCLPKNFSIKEIFDKAISARKINRPMLQAALMLRKKGFTTA\n",
    "ILTNTWLDDRAERDGLAQLMCELKMHFDFLIESCQVGMVKPEPQIYKFLLDTLKASPSEVVF\n",
    "LDDIGANLKPARDLGMVTILVQDTDTALKELEKVTGIQLLNTPAPLPTSCNPSDMSHGYVTV\n",
    "KPRVRLHFVELGSGPAVCLCHGFPESWYSWRYQIPALAQAGYRVLAMDMKGYGESSAPPEIE\n",
    "EYCMEVLCKEMVTFLDKLGLSQAVFIGHDWGGMLVWYMALFYPERVRAVASLNTPFIPANPNM\n",
    "SPLESIKANPVFDYQLYFQEPGVAEAELEQNLSRTFKSLFRASDESVLSMHKVCEAGGLFVNS\n",
    "PEEPSLSRMVTEEEIQFYVQQFKKSGFRGPLNWYRNMERNWKWACKSLGRKILIPALMVTAEK\n",
    "DFVLVPQMSQHMEDWIPHLKRGHIEDCGHWTQMDKPTEVNQILIKWLDSDARNPPVVSKM'''\n",
    "\n",
    "EPHX2 = EPHX2[1:554] # Commerically Purchased Variant is amino 2-555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq4dugrKnU8e"
   },
   "source": [
    "**Second Protein Target from Competition Description**\n",
    "1. Bromo Domain 4\n",
    "2. Cancer disease progression (histone protein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sNBvC9xOnjt3"
   },
   "outputs": [],
   "source": [
    "BRD4 = '''MSAESGPGTRLRNLPVMGDGLETSQMSTTQAQAQPQPANAASTNPPPPETSNPNKPKRQTN\n",
    "QLQYLLRVVLKTLWKHQFAWPFQQPVDAVKLNLPDYYKIIKTPMDMGTIKKRLENNYYWNAQEC\n",
    "IQDFNTMFTNCYIYNKPGDDIVLMAEALEKLFLQKINELPTEETEIMIVQAKGRGRGRKETGTA\n",
    "KPGVSTVPNTTQASTPPQTQTPQPNPPPVQATPHPFPAVTPDLIVQTPVMTVVPPQPLQTPPPV\n",
    "PPQPQPPPAPAPQPVQSHPPIIAATPQPVKTKKGVKRKADTTTPTTIDPIHEPPSLPPEPKTTK\n",
    "LGQRRESSRPVKPPKKDVPDSQQHPAPEKSSKVSEQLKCCSGILKEMFAKKHAAYAWPFYKPVD\n",
    "VEALGLHDYCDIIKHPMDMSTIKSKLEAREYRDAQEFGADVRLMFSNCYKYNPPDHEVVAMARK\n",
    "LQDVFEMRFAKMPDEPEEPVVAVSSPAVPPPTKVVAPPSSSDSSSDSSSDSDSSTDDSEEERAQ\n",
    "RLAELQEQLKAVHEQLAALSQPQQNKPKKKEKDKKEKKKEKHKRKEEVEENKKSKAKEPPPKKT\n",
    "KKNNSSNSNVSKKEPAPMKSKPPPTYESEEEDKCKPMSYEEKRQLSLDINKLPGEKLGRVVHII\n",
    "QSREPSLKNSNPDEIEIDFETLKPSTLRELERYVTSCLRKKRKPQAEKVDVIAGSSKMKGFSSS\n",
    "ESESSSESSSSDSEDSETEMAPKSKKKGHPGREQKKHHHHHHQQMQQAPAPVPQQPPPPPQQPP\n",
    "PPPPPQQQQQPPPPPPPPSMPQQAAPAMKSSPPPFIATQVPVLEPQLPGSVFDPIGHFTQPILH\n",
    "LPQPELPPHLPQPPEHSTPPHLNQHAVVSPPALHNALPQQPSRPSNRAAALPPKPARPPAVSPA\n",
    "LTQTPLLPQPPMAQPPQVLLEDEEPPAPPLTSMQMQLYLQQLQKVQPPTPLLPSVKVQSQPPPP\n",
    "LPPPPHPSVQQQLQQQPPPPPPPQPQPPPQQQHQPPPRPVHLQPMQFSTHIQQPPPPQGQQPPH\n",
    "PPPGQQPPPPQPAKPQQVIQHHHSPRHHKSDPYSTGHLREAPSPLMIHSPQMSQFQSLTHQSPP\n",
    "QQNVQPKKQELRAASVVQPQPLVVVKEEKIHSPIIRSEPFSPSLRPEPPKHPESIKAPVHLPQR\n",
    "PEMKPVDVGRPVIRPPEQNAPPPGAPDKDKQKQEPKTPVAPKKDLKIKNMGSWASLVQKHPTTP\n",
    "SSTAKSSSDSFEQFRRAAREKEEREKALKAQAEHAEKEKERLRQERMRSREDEDALEQARRAHE\n",
    "EARRRQEQQQQQRQEQQQQQQQQAAAVAAAATPQAQSSQPQSMLDQQRELARKREQERRRREAM\n",
    "AATIDMNFQSDLLSIFEENLF'''\n",
    "\n",
    "\n",
    "BRD4   =   BRD4[3:459] # Positions 4-460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlGXTKTGo3Jy"
   },
   "source": [
    "**Third Protein From Competition Description**\n",
    "1. Serum Albumin\n",
    "2. The most common protein in the blood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dIcPq8XhohfA"
   },
   "outputs": [],
   "source": [
    "P02768 = ''' MKWVTFISLLFLFSSAYSRGVFRRDAHKSEVAHRFKDLGEENFKALVLIAFAQYLQQCPFE\n",
    "DHVKLVNEVTEFAKTCVADESAENCDKSLHTLFGDKLCTVATLRETYGEMADCCAKQEPERNECF\n",
    "LQHKDDNPNLPRLVRPEVDVMCTAFHDNEETFLKKYLYEIARRHPYFYAPELLFFAKRYKAAFTE\n",
    "CCQAADKAACLLPKLDELRDEGKASSAKQRLKCASLQKFGERAFKAWAVARLSQRFPKAEFAEVS\n",
    "KLVTDLTKVHTECCHGDLLECADDRADLAKYICENQDSISSKLKECCEKPLLEKSHCIAEVENDE\n",
    "MPADLPSLAADFVESKDVCKNYAEAKDVFLGMFLYEYARRHPDYSVVLLLRLAKTYETTLEKCCA\n",
    "AADPHECYAKVFDEFKPLVEEPQNLIKQNCELFEQLGEYKFQNALLVRYTKKVPQVSTPTLVEVS\n",
    "RNLGKVGSKCCKHPEAKRMPCAEDYLSVVLNQLCVLHEKTPVSDRVTKCCTESLVNRRPCFSALE\n",
    "VDETYVPKEFNAETFTFHADICTLSEKERQIKKQTALVELVKHKPKATKEQLKAVMDDFAAFVEK\n",
    "CCKADDKETCFAEEGKKLVAASQAALGL'''\n",
    "\n",
    "\n",
    "P02768 = P02768[24:608] # Positions 25-609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left off here 05May2024**\n",
    "1. Make the embedding dim shapes the same so that they can easily be batched in training\n",
    "2. Otherwise training is going to be a pain in the ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Yr0hEwTSsAdT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41cd72c55294497ac94424c947f0c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07336767e3a04c71a831c9b5dade32a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4dea70139b44c584297d3c179552ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ad90fac08549ab84ce90ea2cdf0ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdd6771ace248aea43600ab4f766c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations from Kaggle\n",
    "\n",
    "def create_protein_embeddings():\n",
    "    proteins           = {'sEH': EPHX2, 'BRD4': BRD4, 'HSA': P02768}\n",
    "\n",
    "    embedded_sequences = {}\n",
    "\n",
    "    model_name         = \"Rostlab/prot_bert\"  # Example, check the actual model repository\n",
    "    tokenizer          = AutoTokenizer.from_pretrained(model_name)\n",
    "    model              = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    #Maximum Embedding dim 1 for tensor\n",
    "    embedding_max      = 12\n",
    "    for name, sequence in proteins.items():\n",
    "        inputs         = tokenizer(sequence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs       = model(**inputs)\n",
    "            embeddings    = outputs.last_hidden_state\n",
    "            pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Flatten into 1D vector for Nueral network Linear Layers\n",
    "        embeddings        = embeddings.squeeze(0)\n",
    "        embeddings        = torch.concatenate([embeddings[i] for i in range(embeddings.size(0))])\n",
    "\n",
    "        embedded_sequences[name] = embeddings\n",
    "\n",
    "    longest_padding       = max([len(i) for i in list(embedded_sequences.values())])\n",
    "    embedded_sequences    = {name: F.pad(tensor, (0, longest_padding - len(tensor)),\n",
    "                                               mode = 'constant', value = 0) for name, tensor\n",
    "                                                in embedded_sequences.items()}\n",
    "    \n",
    "    return embedded_sequences\n",
    "\n",
    "embedded_sequences = create_protein_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQC94Bs_M_NZ"
   },
   "source": [
    "**Download Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PJzQFTECNBTy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session                 import s3_input, Session\n",
    "import io\n",
    "\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMboHhnhNNGp"
   },
   "source": [
    "**Retrieve Files from s3**\n",
    "1. Fun OSError may have to expand the local s3 space to work in this notebook\n",
    "2. SSH key Passphrase and GITHUB Secret Key: Y5d7fp32!@\n",
    "3. SSH Key GITHUB: SHA256:KFfeCtQipma5pu6OqX3BMrtckwIjZqqg6HvtqN7fdiI alexxaeljimenez@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_in_batches(file_path, batch_size):\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    for i in range(0, num_row_groups, batch_size):\n",
    "        yield parquet_file.read_row_groups(range(i, min(i + batch_size, num_row_groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zVJ7PakpNTnx"
   },
   "outputs": [],
   "source": [
    "# Retrieve Files From S3 Bucket\n",
    "def load_s3_files(bucket_name = 'leashbio-kaggle',\n",
    "                  key         = 'leash-BELKA.zip',\n",
    "                  batch_data   = True):\n",
    "    \n",
    "    session     = boto3.Session()\n",
    "    s3          = session.client('s3')\n",
    "    obj         = s3.get_object(Bucket=bucket_name, Key=key)    \n",
    "    data        = obj['Body'].read()\n",
    "\n",
    "    stream      = obj['Body']\n",
    "    # Load the data into a bytes buffer\n",
    "    data_stream = io.BytesIO(data)\n",
    "    \n",
    "    \n",
    "    chunk_size  = 10 * 1024 * 1024\n",
    "    max_read    = 5 * 1024 * 1024 * 1024  # 5 GB\n",
    "    buffer      = io.BytesIO()\n",
    "    \n",
    "    dataframes  = {}\n",
    "    # The 'data_stream' is our zip file loaded into memory\n",
    "    with ZipFile(data_stream, 'r') as zip_ref:\n",
    "        # List all files contained in the zip\n",
    "        list_of_files = zip_ref.namelist()\n",
    "        print(\"Files in zip:\", list_of_files)\n",
    "        \n",
    "        total_read = 0\n",
    "        # Optionally, process each file within the zip\n",
    "        for file_name in list_of_files:\n",
    "            # Open the file\n",
    "            \n",
    "            if file_name.endswith('.parquet'):\n",
    "                with zip_ref.open(file_name) as file:\n",
    "                     # Read the stream in chunks until we reach approximately 5 GB\n",
    "                    print(file_name)\n",
    "                    # If you need to process a parquet file, load it into pandas (example)\n",
    "                    if batch_data:\n",
    "                        batch_size = 100  # Adjust the batch size as needed\n",
    "\n",
    "                        # Process each batch\n",
    "                        for batch in read_parquet_in_batches(file_path, batch_size):\n",
    "                            df = batch.to_pandas()\n",
    "                            # Process your DataFrame here\n",
    "                            \n",
    "                        \n",
    "                    else:\n",
    "                        df    = pd.read_parquet(file)\n",
    "                        dataframes[file_name] = df\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "            else:\n",
    "                pass\n",
    "                    # For other file types, read the contents directly\n",
    "#                     content = file.read()\n",
    "#                     print(f\"Contents of {file_name}:\", content[:100])  # Show first 100 characters\n",
    "\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# s3.meta.client.upload_file('leash-BELKA.zip', bucket_name, 'leash-BELKA.zip')\n",
    "\n",
    "# Load with m5.12xlarge instance to load all the data into memory\n",
    "# dataframes = load_s3_files()\n",
    "\n",
    "def open_pickle_files(class_0_size = 1500):\n",
    "    ''' \n",
    "    Load the full test and partial train data\n",
    "    '''\n",
    "    test_data             = pickle.load(open('test_df.pckl', 'rb'))\n",
    "    train_truncated       = pickle.load(open('train_df_truncated.pckl', 'rb'))\n",
    "    \n",
    "    train_bind            = train_truncated[train_truncated['binds']== 1]\n",
    "    train_no              = train_truncated[train_truncated['binds'] == 0][:class_0_size]\n",
    "    \n",
    "    train_data            = pd.concat([train_bind, train_no])\n",
    "    train_data            = train_data.reset_index()\n",
    "    \n",
    "    train_data            = train_data.drop(columns = ['index'])\n",
    "    \n",
    "    return test_data, train_truncated\n",
    "\n",
    "# class_0_size = 1500\n",
    "\n",
    "# class_1_size          = 2576\n",
    "test_data, train_data = open_pickle_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update this next**\n",
    "1. Still need to process 1,500,000 to end of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_smiles(output_name = 'test_processed_batched', batch_size = 100000, start_index = 0):\n",
    "    ''' \n",
    "    Preprocess smiles in batches and save to files\n",
    "    \n",
    "    Parameters:\n",
    "    - output_name:\n",
    "    - batch_size: \n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \n",
    "    '''\n",
    "    smiles_group = {}\n",
    "    batches      = math.ceil(len(test_data) / batch_size)\n",
    "    train_smiles = list(test_data['molecule_smiles'])\n",
    "    index        = start_index\n",
    "    index_json   = start_index * batch_size\n",
    "    \n",
    "#     print(start_index)\n",
    "    for batch in range(start_index, batches):\n",
    "        print(f'Executing Batch {batch + 1}')\n",
    "        \n",
    "        if batch <= batches-1:\n",
    "            train_batch = train_smiles[batch*batch_size:(batch + 1)*batch_size]   \n",
    "        else:\n",
    "            train_batch = train_smiles[batch*batch_size:]\n",
    "\n",
    "        train_batch     = Datapreprocess(train_batch)\n",
    "        train_adjacency, graph_tokens  = train_batch.get_processed_data()\n",
    "        \n",
    "        # JSON File are being dynamically updated now\n",
    "#         token_dictionary = preprocess.token_dictionary\n",
    "\n",
    "        \n",
    "        pickle_file      = output_name + f'_Batch_{batch}.pckl'\n",
    "        \n",
    "        pickle_file      = pickle.dump(train_adjacency, open(pickle_file, 'wb'))\n",
    "#         if os.path.exists(output_name + '.h5'):\n",
    "#             mode = 'a'\n",
    "#         else:\n",
    "#             mode = 'w'\n",
    "            \n",
    "# #         pickle.dump(train_adjacency, open(pickle_file, 'wb'))\n",
    "#        # Write to HDF5 file with gzip compression\n",
    "#         index        = 0\n",
    "#         with h5py.File(f'{output_name}.h5', mode) as hdf:\n",
    "#             hdf.create_dataset(f\"Test_Preprocessed_B{batch}\", (len(train_adjacency),), compression='gzip')\n",
    "            \n",
    "#             for array in train_adjacency:\n",
    "#                 # Store each array with gzip compression\n",
    "#                 index += 1\n",
    "               \n",
    "#             print('Exited H5 Writing')\n",
    "            \n",
    "            \n",
    "#         dt      = h5py.vlen_dtype(np.dtype('float64'))  # Vlen dtype for variable-length arrays of floats\n",
    "#         dataset = hdf.create_dataset('var_length_arrays', (len(arrays),), dtype=dt)\n",
    "\n",
    "#         # Store each array in the dataset\n",
    "#         for i, arr in enumerate(arrays):\n",
    "#             dataset[i] = arr\n",
    "            \n",
    "#         json_file        = output_name + '.json' \n",
    "#         if os.path.exists(json_file):\n",
    "#             mode = 'a'\n",
    "#         else:\n",
    "#             mode = 'w'\n",
    "            \n",
    "# #         # Write Vocab Tokens to Json File to be loaded later\n",
    "#         with open(json_file, mode, buffering=1) as file:\n",
    "            \n",
    "#             for key, value in graph_tokens.items():\n",
    "#                 # Serialize and write each key-value pair as a JSON string per line\n",
    "                \n",
    "#                 json_str = json.dumps({index_json: value})\n",
    "#                 file.write(json_str + \"\\n\")\n",
    "#                 index_json += 1\n",
    "#             print('Exited Json Writing')\n",
    "               \n",
    "    return \n",
    "    \n",
    "\n",
    "    \n",
    "# batch_size                = 100000\n",
    "# create_smiles(output_name = 'test_preprocessed', batch_size = batch_size, start_index = 16)\n",
    "        \n",
    "# for \n",
    "# test_data['molecule_smiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opening H5py File Intermediate Stopping INdexes**\n",
    "1. 1455892\n",
    "2. 1499407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    # Get the memory usage statistics\n",
    "    memory           = psutil.virtual_memory()\n",
    "    gb_conversion    = 1024 ** 3\n",
    "    total_memory     = memory.total / gb_conversion\n",
    "    used_memory      = memory.used / gb_conversion\n",
    "    available_memory = memory.available / gb_conversion\n",
    "    \n",
    "#     print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "#     print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "    print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab_tokens(base_path, file_length = 1500000):\n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - base_path: \n",
    "    - final_index:\n",
    "    \n",
    "    Returns:\n",
    "    - vocab_tokens: \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    vocab_tokens  = {}\n",
    "    base_path     = base_path + '.txt'\n",
    "    \n",
    "    with open(base_path, 'r') as txt:\n",
    "        for index, line in tqdm(enumerate(txt), total = file_length):\n",
    "            dict_line           = ast.literal_eval(line.strip())\n",
    "            vocab_tokens.update(dict_line)  \n",
    "#             if index == (final_index - 1):\n",
    "#                 break\n",
    "    \n",
    "    \n",
    "    return vocab_tokens\n",
    "\n",
    "# vocab_tokens = load_vocab_tokens(base_path = 'test_preprocessed', final_index = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(base_path, index = None, mode = 'batch'):\n",
    "    \n",
    "    '''\n",
    "    Opening the prepocessed files with memory statistics\n",
    "    to understand the memory usage needed\n",
    "    \n",
    "    Parameters\n",
    "    - base_path:\n",
    "    - batches:\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - data_batches; \n",
    "    \n",
    "    '''\n",
    "    global data_batches\n",
    "    \n",
    "    data_batches                = {}\n",
    "    \n",
    "    if mode == 'all':\n",
    "        for index in tqdm(range(batches), total = batches):\n",
    "            get_memory_usage()\n",
    "            t1                  = time.time()\n",
    "            batch_path          = base_path + f'_{index}.pckl'\n",
    "            train_batch         = pickle.load(open(batch_path, 'rb'))\n",
    "            data_batches[index] = train_batch\n",
    "            \n",
    "            t2                  = time.time()\n",
    "        \n",
    "        print('Time Elapsed:', t2 - t1)\n",
    "        \n",
    "    elif mode == 'batch' and index is not None:\n",
    "            get_memory_usage()\n",
    "            batch_path          = base_path + f'_{index}.pckl'\n",
    "            data_batches        = pickle.load(open(batch_path, 'rb'))\n",
    "        \n",
    "    return data_batches\n",
    "\n",
    "# data_batches = load_embeddings(base_path = 'Batched Data/test_preprocessed_Batch', index = 0)\n",
    "# data_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess                     = Datapreprocess(list(train_data['molecule_smiles']))\n",
    "# train_adjacency, graph_tokens  = preprocess.get_processed_data()\n",
    "\n",
    "# token_dictionary               = preprocess.token_dictionary\n",
    "# train_adjacency  = torch.tensor(train_adjacency, dtype = torch.float32)\n",
    "# train_adjacency  = train_adjacency.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Step**\n",
    "1. Pass an embedded version of the protein sequence into a parellel nueral network\n",
    "2. Combine the outputs of whatever network we decide before the full connected layer \n",
    "3. Stack a full connected layer on top of the concatenated out put and train neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_split(train_adjacency, graph_tokens, targets, test_size = 0.2, validation_split = 0.5):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    graph_tokens   = list(graph_tokens.values())\n",
    "    train_features = (train_adjacency, graph_tokens)\n",
    "    \n",
    "    x_train_adj, x_test_adj, y_train, y_test = train_test_split(train_adjacency, targets,\n",
    "                                       test_size = test_size, random_state = 42, stratify = targets)\n",
    "    \n",
    "    x_train_g, x_test_g, _, _          = train_test_split(graph_tokens, targets,\n",
    "                                       test_size = test_size, random_state = 42, stratify = targets)\n",
    "    \n",
    "#     x_train = np.concatenate([x_train_adj, x_train_g], dim = 1)\n",
    "#     x_test  = np.concatenate([])\n",
    "    \n",
    "    return x_train_adj, x_test_adj, x_train_g, x_test_g, y_train, y_test\n",
    "\n",
    "# x_train_adj, x_test_adj, x_train_g, x_test_g, y_train, y_test = train_split(train_adjacency, graph_tokens, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Embedding\n",
    "from torch_geometric.data   import Data\n",
    "from torch_geometric.loader import DataLoader  # Correct import\n",
    "\n",
    "\n",
    "def create_gcn_data(train_adjacency, graph_tokens, batch_size = None, batch_index = None, targets = None, mode = 'Evaluate'):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    - train_adjacency:\n",
    "    - graph_tokens:\n",
    "    - targets:\n",
    "    \n",
    "    Returns\n",
    "    - final_data:\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    graphs         = []\n",
    "    feature_size   = 0\n",
    "    if mode == 'Evaluate':\n",
    "        for index in range(len(train_adjacency)):\n",
    "\n",
    "            adj_matrix = train_adjacency[index]\n",
    "            row, col   = np.where(adj_matrix > 0)\n",
    "\n",
    "            # Bond information should flow both ways in graph\n",
    "            bi_message = [[row, col], [col, row]]\n",
    "            edge_index = torch.tensor(bi_message, dtype=torch.long)\n",
    "\n",
    "\n",
    "            # Expand dim so that dim =1 can be set in the model definition\n",
    "\n",
    "            # Vocab tokens were exported as 1-1,500,000\n",
    "            str_index  = str(index + (batch_size * batch_index))\n",
    "            x_features = torch.tensor(graph_tokens[str_index], dtype = torch.long)\n",
    "            x_features = x_features.unsqueeze(1)\n",
    "\n",
    "            if targets is not None:\n",
    "                y_data     = targets[index]\n",
    "                graph      = Data(x=x_features, edge_index=edge_index, y = y_data)\n",
    "            else:\n",
    "                graph      = Data(x=x_features, edge_index=edge_index)\n",
    "\n",
    "            graphs.append(graph)\n",
    "\n",
    "    elif mode == 'Build':\n",
    "        for index in range(len(train_adjacency)):\n",
    "\n",
    "            adj_matrix   = train_adjacency[index]\n",
    "            row, col     = np.where(adj_matrix > 0)\n",
    "            # Bond information should flow both ways in graph\n",
    "            bi_message   = [[row, col], [col, row]]\n",
    "            edge_index   = torch.tensor(bi_message, dtype=torch.long)\n",
    "\n",
    "            # Expand dim so that dim = 1 can be set in the model definition\n",
    "            x_features   = torch.tensor(graph_tokens[index], dtype = torch.long)\n",
    "            x_features   = x_features.unsqueeze(1)\n",
    " \n",
    "            \n",
    "            if targets is not None:\n",
    "                y_data   = targets[index]\n",
    "                graph    = Data(x=x_features, edge_index=edge_index, y = y_data)\n",
    "            \n",
    "            else:\n",
    "                graph    = Data(x=x_features, edge_index=edge_index)\n",
    "           \n",
    "            graphs.append(graph)\n",
    "\n",
    "#     print(feature_size)\n",
    "    final_data = DataLoader(graphs, batch_size = 32)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "\n",
    "def load_test_proteins(data_batch, batch_size, index, vocab_tokens, mode = 'Build', test = False, data = None):\n",
    "\n",
    "    if mode == 'Build':\n",
    "        proteins    = data['protein_name']#[:batch_size]\n",
    "        proteins    = list(proteins.apply(lambda x: embedded_sequences[x]))\n",
    "        proteins    = torch.concatenate([p.unsqueeze(0) for p in proteins], axis = 0)\n",
    "        targets     = torch.tensor(data['binds'], dtype = torch.long)\n",
    "        \n",
    "        if not test:\n",
    "            final_data  = create_gcn_data(data_batch, vocab_tokens, targets = targets, mode = mode)\n",
    "        else:\n",
    "            final_data  = create_gcn_data(data_batch, vocab_tokens, targets = None, mode = mode)\n",
    "\n",
    "    elif mode == 'Evaluate':\n",
    "        proteins      = test_data['protein_name'][index*batch_size:(index + 1)*batch_size]\n",
    "        proteins      = list(proteins.apply(lambda x: embedded_sequences[x]))\n",
    "        proteins      = torch.concatenate([p.unsqueeze(0) for p in proteins], axis = 0)\n",
    "        final_data    = create_gcn_data(data_batch, vocab_tokens, batch_size, index, mode = mode)\n",
    "    \n",
    "    return final_data, proteins\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left off here Need to re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv, GCNConv, global_mean_pool\n",
    "from torch.nn import LazyLinear, Transformer\n",
    "\n",
    "# Example GCN model definition\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.dim_p1       = 512\n",
    "        self.num_layers   = 4\n",
    "        self.concat_dim   = self.dim_p1 + (hidden_dim * 2)\n",
    "        self.conv1        = GCNConv(input_dim, hidden_dim)\n",
    "        self.hidden_dim   = hidden_dim\n",
    "        self.conv2        = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fc2          = LazyLinear(1)\n",
    "        self.fc1          = LazyLinear(32)\n",
    "        self.p_fc1        = LazyLinear(self.dim_p1)\n",
    "        self.tf1          = Transformer(d_model = self.concat_dim)  \n",
    "        self.p_fc2        = LazyLinear(64)\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, data, protein_data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x            = x.float()\n",
    "        x            = F.relu(self.conv1(x, edge_index))\n",
    "        x            = F.relu(self.conv2(x, edge_index))\n",
    "#         x            = self.GAT1(x, edge_index)\n",
    "        x            = global_mean_pool(x, batch)\n",
    "        \n",
    "        protein_data = self.p_fc1(protein_data)\n",
    "        protein_data = self.p_fc2(protein_data)\n",
    "        \n",
    "        x            = torch.concatenate([x, protein_data], axis = 1)\n",
    "#         x            = self.tf1(x, )\n",
    "        x            = self.fc1(x)\n",
    "        \n",
    "        x            = F.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):\n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, \n",
    "                 max_epochs,\n",
    "                 bypass_prepare = False,\n",
    "                 num_gpus=0, \n",
    "                 gradient_clip_val=0, \n",
    "                 batch_size = 32):\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.bypass_prepare     = bypass_prepare\n",
    "        self.max_epochs         = max_epochs\n",
    "        self.batch_size         = 32\n",
    "        # assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, train_data, val_data):\n",
    "        train_dataloader  = DataLoader(train_data, batch_size = self.batch_size)\n",
    "        if val_data is not None:\n",
    "            val_dataloader    = DataLoader(val_data, batch_size = self.batch_size)\n",
    "        else:\n",
    "            val_dataloader    = None\n",
    "            \n",
    "        self.num_train_batches = len(train_dataloader)\n",
    "        self.num_val_batches   = (len(val_dataloader)\n",
    "                                if val_dataloader is not None else 0)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "#     def prepare_model(self, model, optimizer, loss_fn):\n",
    "#         # model.trainer = self\n",
    "#         # model.board.xlim = [0, self.max_epochs]\n",
    "#         self.model     = model\n",
    "#         self.optimizer = optimizer\n",
    "#         self.loss_fn   = loss_fn\n",
    "\n",
    "    def fit(self, model, train_data, other_data = None, val_data = None, optimizer = None, loss_fn = None):\n",
    "        self.model           = model\n",
    "        self.optimizer       = optimizer\n",
    "        self.loss_fn         = loss_fn\n",
    "        \n",
    "        # Bass other data (Placeholder for now 05May2024)\n",
    "        if other_data is not None:\n",
    "            self.supplement_data = other_data\n",
    "   \n",
    "        if not self.bypass_prepare:\n",
    "            self.train_dataloader, self.val_dataloader = self.prepare_data(train_data, val_data)\n",
    "            loader_type      = None\n",
    "            \n",
    "        else:\n",
    "            self.train_dataloader, self.val_dataloader = train_data, val_data\n",
    "            loader_type  = 'Graph'\n",
    "        \n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx   = 0\n",
    "        for self.epoch in tqdm(range(self.max_epochs), total = self.max_epochs):\n",
    "            self.fit_epoch(loader_type = loader_type, p_data = self.supplement_data)\n",
    "            print(f'Epoch {self.epoch + 1} LOSS: {self.loss.detach().numpy()}')\n",
    "            \n",
    "    def fit_epoch(self, loader_type = 'Graph', p_data = None):\n",
    "        \n",
    "        if loader_type == 'Graph':\n",
    "            for index, data in enumerate(self.train_dataloader):\n",
    "                targets     = data.y.unsqueeze(1).float()\n",
    "                p_batch     = p_data[index*self.batch_size:(index + 1)*self.batch_size]\n",
    "                predictions = self.model.forward(data, p_batch)\n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                self.loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                \n",
    "        if loader_type != 'Graph':\n",
    "            for features, targets in self.train_dataloader:\n",
    "                predictions = self.model.forward(features)\n",
    "                   \n",
    "                self.loss   = self.loss_fn(predictions, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                self.loss.backward()\n",
    "                \n",
    "            \n",
    "                self.optimizer.step()\n",
    "                    \n",
    "                # Left off here trying to fix the generalizatin of trainer 05May2024\n",
    "                if self.val_dataloader is not None:\n",
    "                  with torch.no_grad():\n",
    "                    for val_features, val_targets in self.val_dataloader:\n",
    "                      val_predictions = model.forward(val_features)\n",
    "                      val_loss        = self.loss_fn(val_predictions, val_targets)\n",
    "\n",
    "                      print(f'Epoch {self.epoch} w/ Loss: ', val_loss)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left Off here... The dimension of each dataBatch are not agreeing with batch_size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17576/17576 [01:34<00:00, 186.07it/s]\n",
      "100%|██████████| 17576/17576 [00:21<00:00, 809.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17576/17576 [00:13<00:00, 1300.14it/s]\n",
      "100%|██████████| 17576/17576 [00:00<00:00, 186126.88it/s]\n",
      "/tmp/ipykernel_7648/527698293.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1696595231861/work/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  edge_index   = torch.tensor(bi_message, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_build(train_data, batch_size = 100000, batch_loading = True):\n",
    "    \n",
    "    ''' \n",
    "    Preprocessing build funtion for batch preprocessing, loading,\n",
    "    and evaluating of test_data\n",
    "    \n",
    "    Parameters:\n",
    "    - \n",
    "    - \n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    - \n",
    "    \n",
    "    '''\n",
    "    index  = 0 \n",
    "    batches = math.ceil(len(train_data) / batch_size)\n",
    "    \n",
    "    if batch_loading:\n",
    "        for batch in range(batches):\n",
    "\n",
    "            batch_start  = index*batch_size\n",
    "            batch_end    = (index + 1)*batch_size\n",
    "            batch_subset = train_data[batch_start: batch_end]['']\n",
    "\n",
    "            train_batch                    = Datapreprocess(batch_subset)\n",
    "            train_adjacency, graph_tokens  = train_batch.get_processed_data()\n",
    "            final_test, proteins           = load_test_proteins(data_batch = data_batch,\n",
    "                                             batch_size = batch_size, \n",
    "                                             index = index, \n",
    "                                             vocab_tokens = vocab_tokens,\n",
    "                                             mode = 'Build')\n",
    "            \n",
    "            break\n",
    "#         data_batches    = load_embeddings(base_path = 'Batched Data/test_preprocessed_Batch', index = index)\n",
    "            \n",
    "    else:\n",
    "        train_bind                       = train_data[train_data['binds'] == 1].dropna()\n",
    "        train_no_bind                    = train_data[train_data['binds'] == 0].dropna()[:batch_size] \n",
    "        train_input                      = pd.concat([train_bind, train_no_bind], axis = 0)\n",
    "        train_input                      = train_input.reset_index()\n",
    "        train_input                      = train_input.drop(columns = ['index'])\n",
    "        \n",
    "        targets                          = torch.tensor(train_input['binds'], dtype = torch.long)\n",
    "        train_batch                      = Datapreprocess(train_input['molecule_smiles'])\n",
    "        data_batch, vocab_tokens         = train_batch.get_processed_data()\n",
    "        \n",
    "        # Split Train and Test Data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data_batch, targets, stratify = targets, random_state = 42)\n",
    "        train_input, test_input, _, _    = train_test_split(train_input, targets, stratify = targets, random_state = 42)\n",
    "        train_token, test_token, _, _    = train_test_split(vocab_tokens, targets, stratify = targets, random_state = 42)\n",
    "        \n",
    "        train_input                      = train_input.reset_index()\n",
    "        train_input                      = train_input.drop(columns = ['index'])\n",
    "        test_input                       = test_input.reset_index()\n",
    "        test_input                       = test_input.drop(columns = ['index'])\n",
    "        \n",
    "        # Load Final Model Inputs\n",
    "        final_train, x_train_p           = load_test_proteins(data_batch = x_train,\n",
    "                                                         batch_size     = batch_size, \n",
    "                                                         index          = index, \n",
    "                                                         vocab_tokens   = train_token,\n",
    "                                                         mode           = 'Build',\n",
    "                                                         data           = train_input)\n",
    "\n",
    "        final_test, x_test_p             = load_test_proteins(data_batch = x_test,\n",
    "                                                         batch_size     = batch_size, \n",
    "                                                         index          = index, \n",
    "                                                         vocab_tokens   = test_token,\n",
    "                                                         mode           = 'Build',\n",
    "                                                         test           = True,\n",
    "                                                         data           = test_input)\n",
    "\n",
    "\n",
    "    return final_train, final_test, x_train_p, x_test_p \n",
    "\n",
    "final_train, final_test, x_train_p, x_test_p = preprocess_build(train_data, batch_size = 15000, batch_loading = False)\n",
    "# final_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "  5%|▌         | 1/20 [00:15<04:47, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 LOSS: 1.138365387916565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:29<04:28, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 LOSS: 1.130529761314392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:43<03:59, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 LOSS: 1.1229360103607178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:56<03:44, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 LOSS: 1.1191946268081665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:10<03:27, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 LOSS: 1.1177345514297485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:24<03:14, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 LOSS: 1.1180777549743652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:39<03:04, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 LOSS: 1.1206549406051636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:42<03:09, 14.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m trainer             \u001b[38;5;241m=\u001b[39m Trainer(max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, bypass_prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[1;32m     28\u001b[0m optimizer           \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 56\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_data, other_data, val_data, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_batch_idx   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs), total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloader_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupplement_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LOSS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 65\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self, loader_type, p_data)\u001b[0m\n\u001b[1;32m     63\u001b[0m targets     \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     64\u001b[0m p_batch     \u001b[38;5;241m=\u001b[39m p_data[index\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:(index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m---> 65\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(predictions, targets)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[44], line 28\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, data, protein_data)\u001b[0m\n\u001b[1;32m     25\u001b[0m         x, edge_index, batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch\n\u001b[1;32m     27\u001b[0m         x            \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 28\u001b[0m         x            \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m         x            \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#         x            = self.GAT1(x, edge_index)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch_geometric/utils/loop.py:651\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    647\u001b[0m     is_undirected \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mis_undirected\n\u001b[1;32m    649\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[0;32m--> 651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[1;32m    652\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n\u001b[1;32m    654\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index, loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_jit_internal.py:1109\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m-> 1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m              return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def train_model(model, final_train, x_train_p, loss_function):\n",
    "#     batch_size          = 32\n",
    "#     # train_dataset, test_dataset, val_dataset = split_data(padded_sequences, targets)\n",
    "\n",
    "#     # train_dataset = TensorDataset(train_adjacency, targets)\n",
    "#     trainer             = Trainer(max_epochs = 20, bypass_prepare = True, batch_size = batch_size)\n",
    "#     optimizer           = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "#     trainer.fit(model, final_train, x_train_p,  None, optimizer, loss_function)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "class1              = len(train_data[train_data['binds'] == 1].dropna())\n",
    "class0              = len(train_data[train_data['binds'] == 0].dropna()[:15000])\n",
    "weight              = class0 / class1\n",
    "\n",
    "loss_function       = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(weight))\n",
    "# loss_function = nn.BCELoss()\n",
    "\n",
    "model               = Model(input_dim = 1, hidden_dim = 128)\n",
    "# model         = train_model(model, final_train, x_train_p, loss_function)\n",
    "\n",
    "\n",
    "batch_size          = 32\n",
    "# train_dataset, test_dataset, val_dataset = split_data(padded_sequences, targets)\n",
    "\n",
    "# train_dataset = TensorDataset(train_adjacency, targets)\n",
    "trainer             = Trainer(max_epochs = 20, bypass_prepare = True, batch_size = batch_size)\n",
    "optimizer           = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "trainer.fit(model, final_train, x_train_p,  None, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Example function to count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Function to print the model summary\n",
    "def print_model_summary(model):\n",
    "    print(f\"{'Layer Name':<25} {'Output Shape':<25} {'Param #':<15}\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    model_modules = []\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Module) and name != \"\":\n",
    "            params_list = list(module.parameters())\n",
    "            output_shape = params_list[0].size() if params_list else 'N/A'\n",
    "            param_num = count_parameters(module)\n",
    "            meta_info = f\"{name:<25} {str(output_shape):<25} {param_num}\"\n",
    "            print(meta_info)\n",
    "            total_params += param_num\n",
    "            model_modules.append(meta_info)\n",
    "\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"Total Trainable Parameters: {total_params}\")\n",
    "    return model_modules\n",
    "\n",
    "# for index, batch in enumerate(final_test):\n",
    "#     p_test       = x_test_p[index*batch_size: (index + 1) * batch_size]\n",
    "#     test_results = model(batch, p_test) \n",
    "#     break\n",
    "\n",
    "# Print the model summary now that the parameters are initialized\n",
    "# model_modules = print_model_summary(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_main(index, vocab_tokens, batch_size = 100000):\n",
    "    ''' \n",
    "    This function handles all the function calls for loading\n",
    "    data in batches all the way to feeding it to the model for inference\n",
    "    and saving the predicted probabilites to CSV file for competition_submission\n",
    "    \n",
    "    Parameters:\n",
    "    - index:\n",
    "    - vocab_tokens:\n",
    "    - batch_size: \n",
    "    \n",
    "    Returns:\n",
    "    - final_test:\n",
    "    - proteins: \n",
    "    \n",
    "    '''\n",
    "    # Data Batch is a list containing matricies\n",
    "    data_batch            = load_embeddings(base_path = 'Batched Data/test_preprocessed_Batch', index = index)\n",
    "    final_test, proteins  = load_test_proteins(data_batch = data_batch,\n",
    "                                     batch_size = batch_size, \n",
    "                                     index = index, \n",
    "                                     vocab_tokens = vocab_tokens,\n",
    "                                     mode = 'Evaluate')\n",
    "#     final_test    = create_gcn_data(mode = 'Evaluate', final_index = batch_size)\n",
    "    \n",
    "    return final_test, proteins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, final_test, proteins, batch_size = 32):\n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - model:\n",
    "    - final_test:\n",
    "    - proteins:\n",
    "    - batch_size:\n",
    "    \n",
    "    Returns:\n",
    "    - probabilities:\n",
    "    - raw_preds:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    test_preds       = []\n",
    "    raw_preds        = []\n",
    "\n",
    "    for index, batch in enumerate(final_test):\n",
    "        p_test       = proteins[index*batch_size: (index + 1) * batch_size]\n",
    "        test_results = model(batch, p_test)\n",
    "        raw_prob     = copy.copy(test_results)\n",
    "        \n",
    "        test_results = [1 if x > 0.5 else 0 for x in test_results]\n",
    "        test_preds.append(test_results)\n",
    "        raw_preds.append(raw_prob)\n",
    "\n",
    "    predictions      = []\n",
    "    for sub in test_preds:\n",
    "        for result in sub:\n",
    "            predictions.append(result)\n",
    "\n",
    "    probabilities    = []\n",
    "    for sub in raw_preds:\n",
    "        for result in sub:\n",
    "            probabilities.append(result)\n",
    "\n",
    "    \n",
    "    return probabilities, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preds       = []\n",
    "# raw_preds        = []\n",
    "# for index, batch in enumerate(final_test):\n",
    "#     p_test       = x_test_p[index*batch_size: (index + 1) * batch_size]\n",
    "#     test_results = model(batch, p_test)\n",
    "#     raw_prob     = copy.copy(test_results)\n",
    "#     test_results = [1 if x > 0.5 else 0 for x in test_results]\n",
    "#     test_preds.append(test_results)\n",
    "#     raw_preds.append(raw_prob)\n",
    "    \n",
    "# predictions      = []\n",
    "# for sub in test_preds:\n",
    "#     for result in sub:\n",
    "#         predictions.append(result)\n",
    "\n",
    "# probabilities    = []\n",
    "# for sub in raw_preds:\n",
    "#     for result in sub:\n",
    "#         probabilities.append(float(result))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(experiment_name, module_info, model_name, json_master = 'model_experiments.json', view_only = True):\n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    Parameters:\n",
    "    - experiment_name\n",
    "    - module_info\n",
    "    - model_name\n",
    "    - json_master\n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    \n",
    "    '''\n",
    "    # Could Add better Comprehension Here\n",
    "    if not os.path.exists(model_name):\n",
    "        torch.save(model, model_name)\n",
    "    else:\n",
    "        warnings.warn(f'{model_name} already exists. Model is not being saved')\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(predictions, y_test)\n",
    "    recall    = recall_score(predictions, y_test)\n",
    "    f1        = f1_score(predictions, y_test)\n",
    "    accuracy  = accuracy_score(predictions, y_test)\n",
    "    mae       = mean_absolute_error(predictions, y_test)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    metrics_dictionary = {'Precision': precision,\n",
    "                         'Recall': recall,\n",
    "                         'F1 Score:': f1,\n",
    "                         'Accuracy': accuracy,\n",
    "                         'MAE': mae}\n",
    "    \n",
    "    \n",
    "    if not view_only:\n",
    "        summary_dict                  = json.load(open(json_master, 'r'))\n",
    "        if experiment_name not in summary_dict:\n",
    "            summary_dict[experiment_name] = {'Model Metadata': module_info,\n",
    "                                            'Results': metrics_dictionary}\n",
    "\n",
    "            json.dump(summary_dict, open(json_master, 'w'), indent=4)\n",
    "        else:\n",
    "            raise ValueError('Duplicate Experiment Name. Choose a different experiment name')\n",
    "\n",
    "    return\n",
    "\n",
    "## Get the current date\n",
    "# current_date    = datetime.now()\n",
    "# timestamp       = current_date.strftime(\"%d-%m-%Y\")  # You can change the format as needed\n",
    "\n",
    "# model_paths     = 'Trained_Models'\n",
    "# model_name      = f'{model_paths}/GCN_{class_0_size}_{class_1_size}_{timestamp}.pth'\n",
    "\n",
    "# experiment_name = f'{class_0_size} (No Bind) and {class_1_size} (Bind)'\n",
    "# export_results(experiment_name, model_modules, model_name, view_only = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1674896it [01:50, 15193.16it/s]                             \n"
     ]
    }
   ],
   "source": [
    "def inference_csv(final_test, \n",
    "                  test_data,\n",
    "                  proteins,\n",
    "                  submission_name, \n",
    "                  model_name = 'GCN_15000_2576_08-05-2024.pth'):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    - final_test: DataLoader with graphs\n",
    "    - test_data: Raw CSV test file from Kaggle\n",
    "    - submission_name: Name\n",
    "    - model_name: \n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    '''\n",
    "    \n",
    "    protein_ids   = test_data['id']\n",
    "    model_path    = 'Trained_Models/' + model_name\n",
    "    model         = torch.load(model_path)\n",
    "    \n",
    "    probabilities, predictions = run_model(model, final_test, proteins, batch_size = 32)\n",
    "    \n",
    "    probabilities = [float(x.detach().numpy()) for x in probabilities]\n",
    "    probabilities = pd.DataFrame(probabilities)#.apply(lambda x: x.detach().numpy())\n",
    "    probabilities.index = protein_ids.index\n",
    "    submissions   = pd.concat([protein_ids, probabilities], axis = 1)\n",
    "    \n",
    "    submissions.columns = ['id', 'binds']\n",
    "    # Avoid overriding previous submissions\n",
    "#     submission_name     = submission_name + f'{index}'\n",
    "#     if os.path.exists(submission_name):\n",
    "#         raise ValueError(f'{submission_name}.csv already exists')\n",
    "#     else:\n",
    "#         submissions.to_csv(submission_name, index = False)\n",
    "        \n",
    "    return model, submissions\n",
    "\n",
    "\n",
    "# The size data was batched in pckl files DO NOT CHANGE\n",
    "batch_size                       = 100000\n",
    "batches                          = math.ceil(len(test_data) / batch_size)\n",
    "vocab_tokens                     = load_vocab_tokens(base_path = 'test_preprocessed')\n",
    "vocab_keys                       = list(vocab_tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 27.51 GB\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'GCN' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m token_subset                 \u001b[38;5;241m=\u001b[39m {key: vocab_tokens[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m subset_keys}\n\u001b[1;32m     10\u001b[0m final_test, proteins         \u001b[38;5;241m=\u001b[39m preprocess_main(batch_size   \u001b[38;5;241m=\u001b[39m batch_size, \n\u001b[1;32m     11\u001b[0m                                                index        \u001b[38;5;241m=\u001b[39m index, \n\u001b[1;32m     12\u001b[0m                                                vocab_tokens \u001b[38;5;241m=\u001b[39m token_subset,\n\u001b[1;32m     13\u001b[0m                                               )\n\u001b[0;32m---> 15\u001b[0m model, submissions           \u001b[38;5;241m=\u001b[39m \u001b[43minference_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mproteins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                               \u001b[49m\u001b[43msubmission_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModel Submissions/submission_b\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mindex\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGCN_15000_2576_08-05-2024.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m submission_df                \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([submission_df, submissions], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Delete Variables to deal with memory\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 22\u001b[0m, in \u001b[0;36minference_csv\u001b[0;34m(final_test, test_data, proteins, submission_name, model_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m protein_ids   \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m model_path    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrained_Models/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model_name\n\u001b[0;32m---> 22\u001b[0m model         \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m probabilities, predictions \u001b[38;5;241m=\u001b[39m run_model(model, final_test, proteins, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     26\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m probabilities]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/serialization.py:1415\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'GCN' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "submission_df                    =   pd.DataFrame()\n",
    "for index in range(batches):\n",
    "    # Need to fix the logic for this no no need to open full json file everytime\n",
    "    # Implement the batching into all function calls\n",
    "    \n",
    "    subset_keys                  = vocab_keys[index*batch_size:(index + 1)*batch_size]\n",
    "    # Step 3: Create a new dictionary with these keys\n",
    "    token_subset                 = {key: vocab_tokens[key] for key in subset_keys}\n",
    "    \n",
    "    final_test, proteins         = preprocess_main(batch_size   = batch_size, \n",
    "                                                   index        = index, \n",
    "                                                   vocab_tokens = token_subset,\n",
    "                                                  )\n",
    "\n",
    "    model, submissions           = inference_csv(final_test,\n",
    "                                   test_data[index*batch_size:(index + 1)*batch_size],\n",
    "                                   proteins,\n",
    "                                   submission_name = f'Model Submissions/submission_b{index}.csv',\n",
    "                                   model_name      = 'GCN_15000_2576_08-05-2024.pth')\n",
    "    \n",
    "       \n",
    "    \n",
    "    submission_df                = pd.concat([submission_df, submissions], axis = 0)\n",
    "    \n",
    "    # Delete Variables to deal with memory\n",
    "    del subset_keys\n",
    "    del token_subset\n",
    "    del final_test\n",
    "    del proteins\n",
    "    \n",
    "    # Force Garbage Collection for Memory\n",
    "    gc.collect()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>0.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>0.913789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>0.812792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721</td>\n",
       "      <td>0.956296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722</td>\n",
       "      <td>0.963440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723</td>\n",
       "      <td>0.985396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724</td>\n",
       "      <td>0.975440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     binds\n",
       "0        295246830  0.947388\n",
       "1        295246831  0.913789\n",
       "2        295246832  0.927355\n",
       "3        295246833  0.812792\n",
       "4        295246834  0.718755\n",
       "...            ...       ...\n",
       "1674891  296921721  0.956296\n",
       "1674892  296921722  0.963440\n",
       "1674893  296921723  0.985396\n",
       "1674894  296921724  0.975440\n",
       "1674895  296921725  0.979522\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  binds\n",
       "0        True   True\n",
       "1        True  False\n",
       "2        True  False\n",
       "3        True   True\n",
       "4        True   True\n",
       "...       ...    ...\n",
       "1674891  True  False\n",
       "1674892  True   True\n",
       "1674893  True  False\n",
       "1674894  True   True\n",
       "1674895  True   True\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_sub = pd.read_csv('submission.csv')\n",
    "\n",
    "equal = other_sub == submission_df\n",
    "equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "binds\n",
       "True     1433295\n",
       "False     241601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal['binds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       1574896\n",
       "binds    1574896\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>protein_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC1CCC(CCN)CC1</td>\n",
       "      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>COC1CCC(CCN)CC1</td>\n",
       "      <td>COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...</td>\n",
       "      <td>Nc1noc2ccc(F)cc12</td>\n",
       "      <td>NCc1cccs1</td>\n",
       "      <td>[N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                              buildingblock1_smiles  \\\n",
       "0        295246830    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "1        295246831    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "2        295246832    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "3        295246833    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "4        295246834    C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "...            ...                                                ...   \n",
       "1674891  296921721  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674892  296921722  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674893  296921723  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674894  296921724  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "1674895  296921725  [N-]=[N+]=NCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc...   \n",
       "\n",
       "        buildingblock2_smiles   buildingblock3_smiles  \\\n",
       "0              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "1              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "2              C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "3              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "4              C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "...                       ...                     ...   \n",
       "1674891     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n",
       "1674892     Nc1noc2ccc(F)cc12         COC1CCC(CCN)CC1   \n",
       "1674893     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "1674894     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "1674895     Nc1noc2ccc(F)cc12               NCc1cccs1   \n",
       "\n",
       "                                           molecule_smiles protein_name  \n",
       "0        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...         BRD4  \n",
       "1        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          HSA  \n",
       "2        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          sEH  \n",
       "3        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...         BRD4  \n",
       "4        C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...          HSA  \n",
       "...                                                    ...          ...  \n",
       "1674891  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...          HSA  \n",
       "1674892  COC1CCC(CCNc2nc(Nc3noc4ccc(F)cc34)nc(N[C@@H](C...          sEH  \n",
       "1674893  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...         BRD4  \n",
       "1674894  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...          HSA  \n",
       "1674895  [N-]=[N+]=NCCC[C@H](Nc1nc(NCc2cccs2)nc(Nc2noc3...          sEH  \n",
       "\n",
       "[1674896 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reset    = 0\n",
    "reset_index    = []\n",
    "for index in submission_df.index:\n",
    "    if index < index_reset:\n",
    "        reset_index.append(index_reset)\n",
    "        index_reset = 0\n",
    "    else:\n",
    "        index_reset = index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[199999,\n",
       " 299999,\n",
       " 399999,\n",
       " 499999,\n",
       " 599999,\n",
       " 699999,\n",
       " 799999,\n",
       " 899999,\n",
       " 999999,\n",
       " 1099999,\n",
       " 1199999,\n",
       " 1299999,\n",
       " 1399999,\n",
       " 1499999,\n",
       " 1599999,\n",
       " 1674895]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830.0</td>\n",
       "      <td>0.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831.0</td>\n",
       "      <td>0.913789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832.0</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833.0</td>\n",
       "      <td>0.812792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834.0</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74891</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.956296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74892</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.963440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74893</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.985396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74894</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.975440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74895</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3249792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id     binds\n",
       "0      295246830.0  0.947388\n",
       "1      295246831.0  0.913789\n",
       "2      295246832.0  0.927355\n",
       "3      295246833.0  0.812792\n",
       "4      295246834.0  0.718755\n",
       "...            ...       ...\n",
       "74891          NaN  0.956296\n",
       "74892          NaN  0.963440\n",
       "74893          NaN  0.985396\n",
       "74894          NaN  0.975440\n",
       "74895          NaN  0.979522\n",
       "\n",
       "[3249792 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830.0</td>\n",
       "      <td>0.947388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831.0</td>\n",
       "      <td>0.913789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832.0</td>\n",
       "      <td>0.927355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833.0</td>\n",
       "      <td>0.812792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834.0</td>\n",
       "      <td>0.718755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674891</th>\n",
       "      <td>296921721.0</td>\n",
       "      <td>0.956296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674892</th>\n",
       "      <td>296921722.0</td>\n",
       "      <td>0.963440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674893</th>\n",
       "      <td>296921723.0</td>\n",
       "      <td>0.985396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674894</th>\n",
       "      <td>296921724.0</td>\n",
       "      <td>0.975440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674895</th>\n",
       "      <td>296921725.0</td>\n",
       "      <td>0.979522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1674896 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     binds\n",
       "0        295246830.0  0.947388\n",
       "1        295246831.0  0.913789\n",
       "2        295246832.0  0.927355\n",
       "3        295246833.0  0.812792\n",
       "4        295246834.0  0.718755\n",
       "...              ...       ...\n",
       "1674891  296921721.0  0.956296\n",
       "1674892  296921722.0  0.963440\n",
       "1674893  296921723.0  0.985396\n",
       "1674894  296921724.0  0.975440\n",
       "1674895  296921725.0  0.979522\n",
       "\n",
       "[1674896 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
